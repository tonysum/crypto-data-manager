from db import engine
from sqlalchemy import text  # pyright: ignore[reportMissingImports]
import pandas as pd  # pyright: ignore[reportMissingImports]
from typing import Union, Dict, List, Optional
from datetime import datetime, timedelta
import logging

from binance_client import in_exchange_trading_symbols, kline_candlestick_data, kline2df

# è·å–å¸å®‰äº¤æ˜“æ‰€æ‰€æœ‰åˆçº¦äº¤æ˜“å¯¹
IN_EXCHANGE_SYMBOLS = in_exchange_trading_symbols()

# è·å–æœ¬åœ°å…¨éƒ¨äº¤æ˜“å¯¹æ•°æ®
def get_local_symbols(interval: str = "1d"):
    """è·å–æœ¬åœ°æ•°æ®åº“ä¸­æŒ‡å®šæ—¶é—´é—´éš”çš„äº¤æ˜“å¯¹åˆ—è¡¨"""
    try:
        # è¡¨åæ ¼å¼: K{interval}{symbol}, ä¾‹å¦‚: K1dBTCUSDT
        prefix = f'K{interval}'
        # ä½¿ç”¨ ILIKE è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
        stmt = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name ILIKE :prefix
        """
        with engine.connect() as conn:
            result = conn.execute(text(stmt), {"prefix": f"{prefix}%"})
            table_names = result.fetchall()
        
        local_symbols = []
        for name_row in table_names:
            name = name_row[0]
            # æ‰¾åˆ°å‰ç¼€çš„ä½ç½®ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            if name.lower().startswith(prefix.lower()):
                symbol = name[len(prefix):]
                local_symbols.append(symbol.upper())
        
        return list(set(local_symbols))  # å»é‡å¹¶è¿”å›
    except Exception as e:
        logging.warning(f"æ— æ³•è¿æ¥åˆ°æ•°æ®åº“è·å–äº¤æ˜“å¯¹åˆ—è¡¨: {e}")
        logging.warning("å°†ä½¿ç”¨ç©ºåˆ—è¡¨ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        return []

# å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶è¿æ¥å¤±è´¥
try:
    LOCAL_SYMBOLS = get_local_symbols(interval="1d")  # é»˜è®¤ä½¿ç”¨æ—¥çº¿æ•°æ®
except Exception as e:
    logging.warning(f"åˆå§‹åŒ–æœ¬åœ°äº¤æ˜“å¯¹åˆ—è¡¨å¤±è´¥: {e}")
    LOCAL_SYMBOLS = []

#æ¯”è¾ƒæœ¬åœ°å’Œäº¤æ˜“æ‰€äº¤æ˜“å¯¹ï¼Œæ‰¾å‡ºç¼ºå¤±çš„äº¤æ˜“å¯¹
def find_missing_symbols():
    
    if not IN_EXCHANGE_SYMBOLS:
        return []
    
    missing_symbols = [
        symbol for symbol in IN_EXCHANGE_SYMBOLS
        if symbol not in LOCAL_SYMBOLS
    ]
    return missing_symbols

MISSING_SYMBOLS = find_missing_symbols()
# print(f"Missing symbols: {MISSING_SYMBOLS}")  # æ³¨é‡Šæ‰ï¼Œé¿å…æ¯æ¬¡å¯¼å…¥æ—¶éƒ½æ‰“å°

def get_local_kline_data(symbol: str, interval: str = "1d") -> pd.DataFrame:
    """è·å–æœ¬åœ°æ•°æ®åº“ä¸­æŒ‡å®šäº¤æ˜“å¯¹çš„Kçº¿æ•°æ®"""
    # æ¸…æ´—è¾“å…¥
    symbol = symbol.strip().upper()
    interval = interval.strip()
    table_name = f'K{interval}{symbol}'
    
    # PostgreSQL è¡¨åéœ€è¦ç”¨å¼•å·åŒ…è£¹ï¼ˆä¿æŒå¤§å°å†™ï¼‰
    safe_table_name = f'"{table_name}"'
    stmt = f'SELECT * FROM {safe_table_name} ORDER BY open_time ASC'
    try:
        with engine.connect() as conn:
            result = conn.execute(text(stmt))
            data = result.fetchall()
            columns = result.keys()
        df = pd.DataFrame(data, columns=columns)
        logging.debug(f"æˆåŠŸä»è¡¨ {table_name} è·å– {len(df)} æ¡æ•°æ®")
        return df
    except Exception as e:
        # å¦‚æœè¡¨ä¸å­˜åœ¨æˆ–å…¶ä»–æ•°æ®åº“é”™è¯¯ï¼Œè¿”å›ç©ºDataFrame
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…å¤„ç†ç©ºæ•°æ®çš„æƒ…å†µ
        logging.warning(f"è·å–æœ¬åœ°Kçº¿æ•°æ®å¤±è´¥ï¼ˆè¡¨ {table_name} å¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")
        # å°è¯•æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆæ”¹è¿›ï¼šæåº¦å®½æ¾æŸ¥æ‰¾ï¼‰
        try:
            with engine.connect() as conn:
                result = conn.execute(
                    text("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND (
                            table_name = :table_name 
                            OR table_name = LOWER(:table_name)
                            OR table_name = UPPER(:table_name)
                            OR LOWER(table_name) = LOWER(:table_name)
                        )
                        LIMIT 1
                    """),
                    {"table_name": table_name}
                )
                actual_table_name_row = result.fetchone()
                
                if not actual_table_name_row:
                    # ILIKE å°è¯•å›é€€
                    result_fallback = conn.execute(
                        text("SELECT table_name FROM information_schema.tables WHERE table_name ILIKE :table_name LIMIT 1"),
                        {"table_name": table_name}
                    )
                    actual_table_name_row = result_fallback.fetchone()

                if actual_table_name_row:
                    actual_name = actual_table_name_row[0]
                    logging.info(f"å‘ç°è¡¨åå¤§å°å†™ä¸åŒ¹é…æˆ–æ‹¼å†™ç›¸è¿‘: æŸ¥è¯¢çš„æ˜¯ {table_name}ï¼Œå®é™…è¡¨åæ˜¯ {actual_name}")
                    # ä½¿ç”¨å®é™…è¡¨åé‡è¯•
                    safe_actual_name = f'"{actual_name}"'
                    stmt_retry = f'SELECT * FROM {safe_actual_name} ORDER BY open_time ASC'
                    result_retry = conn.execute(text(stmt_retry))
                    data_retry = result_retry.fetchall()
                    columns_retry = result_retry.keys()
                    df_retry = pd.DataFrame(data_retry, columns=columns_retry)
                    logging.info(f"ä½¿ç”¨å®é™…è¡¨å {actual_name} æˆåŠŸè·å– {len(df_retry)} æ¡æ•°æ®")
                    return df_retry
        except Exception as e2:
            logging.debug(f"æ£€æŸ¥è¡¨åæ—¶å‡ºé”™: {e2}")
        return pd.DataFrame()


def get_kline_data_for_date(symbol: str, date: str) -> Optional[pd.Series]:
    """
    è·å–æŒ‡å®šäº¤æ˜“å¯¹åœ¨æŒ‡å®šæ—¥æœŸçš„Kçº¿æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        date: æ—¥æœŸå­—ç¬¦ä¸² 'YYYY-MM-DD'
    
    Returns:
        SeriesåŒ…å«è¯¥æ—¥æœŸçš„Kçº¿æ•°æ®ï¼Œæˆ–None
    """
    try:
        df = get_local_kline_data(symbol)
        if df.empty:
            return None
        
        # å°†trade_dateè½¬æ¢ä¸ºæ—¥æœŸå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒï¼ˆå¤„ç†å¤šç§æ—¥æœŸæ ¼å¼ï¼‰
        if df['trade_date'].dtype == 'object':
            # å­—ç¬¦ä¸²æ ¼å¼ï¼Œæå–æ—¥æœŸéƒ¨åˆ†
            df['trade_date_str'] = df['trade_date'].str[:10]
        else:
            # datetimeæ ¼å¼
            df['trade_date_str'] = pd.to_datetime(df['trade_date']).dt.strftime('%Y-%m-%d')
        
        date_data = df[df['trade_date_str'] == date]
        if date_data.empty:
            return None
        
        return date_data.iloc[0]
    except Exception as e:
        logging.error(f"è·å– {symbol} åœ¨ {date} çš„Kçº¿æ•°æ®å¤±è´¥: {e}")
        return None



def get_24h_quote_volume(symbol: str, entry_datetime: str) -> float:
    """
    è·å–å»ºä»“æ—¶åˆ»å¾€å‰24å°æ—¶çš„æˆäº¤é¢ï¼ˆquote_volumeï¼‰
    
    ç”¨äºåˆ¤æ–­ä¸»åŠ›æ˜¯å¦å·²ç»å‡ºè´§ï¼š
    - é«˜æ¶¨å¹… + ä½æˆäº¤é¢(<3äº¿)ï¼šä¸»åŠ›è¿˜æ²¡å‡ºå®Œè´§ï¼Œç»§ç»­æ‹‰ç›˜é£é™©é«˜
    - é«˜æ¶¨å¹… + é«˜æˆäº¤é¢(>=3äº¿)ï¼šFOMOå……åˆ†ï¼Œä¸»åŠ›å¯ä»¥å‡ºè´§ï¼Œåšç©ºæ›´å®‰å…¨
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        entry_datetime: å»ºä»“æ—¶é—´ï¼ˆæ ¼å¼ï¼š'YYYY-MM-DD HH:MM:SS' æˆ– 'YYYY-MM-DD'ï¼‰
    
    Returns:
        24å°æ—¶æˆäº¤é¢ï¼ˆUSDTï¼‰ï¼Œå¤±è´¥è¿”å›-1
    """
    table_name = f'K1h{symbol}'
    try:
        # è§£æå»ºä»“æ—¶é—´
        if ' ' in entry_datetime:
            entry_dt = datetime.strptime(entry_datetime, '%Y-%m-%d %H:%M:%S')
        else:
            entry_dt = datetime.strptime(entry_datetime, '%Y-%m-%d')
        
        # è®¡ç®—24å°æ—¶å‰çš„æ—¶é—´
        start_dt = entry_dt - timedelta(hours=24)
        
        # æŸ¥è¯¢24å°æ—¶å†…çš„æˆäº¤é¢æ€»å’Œ
        query = f'''
            SELECT SUM(quote_volume) as total_volume
            FROM {table_name}
            WHERE trade_date >= "{start_dt.strftime('%Y-%m-%d %H:%M:%S')}"
            AND trade_date < "{entry_dt.strftime('%Y-%m-%d %H:%M:%S')}"
        '''
        
        with engine.connect() as conn:
            result = conn.execute(text(query))
            row = result.fetchone()
            if row and row[0]:
                return float(row[0])
            return -1
    except Exception as e:
        logging.warning(f"è·å– {symbol} 24å°æ—¶æˆäº¤é¢å¤±è´¥: {e}")
        return -1


def get_top_gainer_by_date(date: str) -> Optional[tuple]:
    """
    è·å–æŒ‡å®šæ—¥æœŸæ¶¨å¹…ç¬¬ä¸€çš„äº¤æ˜“å¯¹
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ 'YYYY-MM-DD'
    
    Returns:
        Tuple[symbol, pct_chg] æˆ– None
    """
    from typing import Tuple
    symbols = get_local_symbols()
    top_gainer = None
    max_pct_chg = float('-inf')
    
    for symbol in symbols:
        try:
            df = get_local_kline_data(symbol)
            if df.empty:
                continue
            
            # å°†trade_dateè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒï¼ˆå¤„ç†å¤šç§æ—¥æœŸæ ¼å¼ï¼‰
            if df['trade_date'].dtype == 'object':
                # å­—ç¬¦ä¸²æ ¼å¼ï¼Œæå–æ—¥æœŸéƒ¨åˆ†
                df['trade_date_str'] = df['trade_date'].str[:10]
            else:
                # datetimeæ ¼å¼
                df['trade_date_str'] = pd.to_datetime(df['trade_date']).dt.strftime('%Y-%m-%d')
            
            # æŸ¥æ‰¾æŒ‡å®šæ—¥æœŸçš„æ•°æ®
            date_data = df[df['trade_date_str'] == date]
            if date_data.empty:
                continue
            
            row = date_data.iloc[0]
            pct_chg = row['pct_chg']
            
            # å¦‚æœpct_chgæ˜¯NaNï¼Œå°è¯•ä½¿ç”¨æ”¶ç›˜ä»·å’Œå¼€ç›˜ä»·è®¡ç®—æ¶¨å¹…
            if pd.isna(pct_chg):
                # æŸ¥æ‰¾å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
                date_dt = datetime.strptime(date, '%Y-%m-%d')
                prev_date = (date_dt - timedelta(days=1)).strftime('%Y-%m-%d')
                prev_data = df[df['trade_date_str'] == prev_date]
                
                if not prev_data.empty and not pd.isna(prev_data.iloc[0]['close']):
                    prev_close = prev_data.iloc[0]['close']
                    current_close = row['close']
                    if not pd.isna(current_close) and prev_close > 0:
                        # è®¡ç®—æ¶¨å¹…
                        pct_chg = (current_close - prev_close) / prev_close * 100
                    else:
                        continue
                else:
                    continue
            
            if pct_chg > max_pct_chg:
                max_pct_chg = pct_chg
                top_gainer = symbol
        except Exception as e:
            logging.debug(f"è·å– {symbol} åœ¨ {date} çš„æ•°æ®å¤±è´¥: {e}")
            continue
    
    if top_gainer:
        return (top_gainer, max_pct_chg)
    return None


def get_all_top_gainers(start_date: str, end_date: str) -> pd.DataFrame:
    """
    è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ‰€æœ‰æ¶¨å¹…ç¬¬ä¸€çš„äº¤æ˜“å¯¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ 'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸ 'YYYY-MM-DD'
    
    Returns:
        DataFrameåŒ…å«æ—¥æœŸã€äº¤æ˜“å¯¹ã€æ¶¨å¹…
    """
    symbols = get_local_symbols()
    all_data = []
    
    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰äº¤æ˜“å¯¹çš„æ•°æ®
    logging.info(f"æ­£åœ¨è¯»å– {len(symbols)} ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®...")
    for symbol in symbols:
        try:
            df = get_local_kline_data(symbol)
            if df.empty:
                continue
            
            # æ ‡å‡†åŒ–trade_dateæ ¼å¼
            if df['trade_date'].dtype == 'object':
                df['trade_date_str'] = df['trade_date'].str[:10]
            else:
                df['trade_date_str'] = pd.to_datetime(df['trade_date']).dt.strftime('%Y-%m-%d')
            
            # ç­›é€‰æ—¥æœŸèŒƒå›´
            date_mask = (df['trade_date_str'] >= start_date) & (df['trade_date_str'] <= end_date)
            df_filtered = df[date_mask].copy()
            
            if df_filtered.empty:
                continue
            
            # æ·»åŠ symbolåˆ—
            df_filtered['symbol'] = symbol
            
            # å¤„ç†NaNçš„pct_chg
            for idx, row in df_filtered.iterrows():
                if pd.isna(row['pct_chg']):
                    # å°è¯•è®¡ç®—æ¶¨å¹…
                    date_str = row['trade_date_str']
                    date_dt = datetime.strptime(date_str, '%Y-%m-%d')
                    prev_date = (date_dt - timedelta(days=1)).strftime('%Y-%m-%d')
                    prev_data = df[df['trade_date_str'] == prev_date]
                    
                    if not prev_data.empty and not pd.isna(prev_data.iloc[0]['close']):
                        prev_close = prev_data.iloc[0]['close']
                        current_close = row['close']
                        if not pd.isna(current_close) and prev_close > 0:
                            df_filtered.at[idx, 'pct_chg'] = (current_close - prev_close) / prev_close * 100
            
            # åªä¿ç•™éœ€è¦çš„åˆ—
            df_filtered = df_filtered[['trade_date_str', 'symbol', 'pct_chg']].copy()
            all_data.append(df_filtered)
        except Exception as e:
            logging.debug(f"è¯»å– {symbol} æ•°æ®å¤±è´¥: {e}")
            continue
    
    if not all_data:
        logging.warning("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®")
        return pd.DataFrame(columns=['date', 'symbol', 'pct_chg'])
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    logging.info("æ­£åœ¨åˆå¹¶æ•°æ®å¹¶è®¡ç®—æ¶¨å¹…ç¬¬ä¸€...")
    combined_df = pd.concat(all_data, ignore_index=True)
    
    # è¿‡æ»¤æ‰pct_chgä¸ºNaNçš„è¡Œ
    combined_df = combined_df[combined_df['pct_chg'].notna()]
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œä½¿ç”¨nlargestæ‰¾å‡ºæ¯å¤©æ¶¨å¹…æœ€å¤§çš„äº¤æ˜“å¯¹
    top_gainers = (
        combined_df.groupby('trade_date_str', group_keys=False)
        .apply(lambda x: x.nlargest(1, 'pct_chg'))
        .reset_index(drop=True)
    )
    
    # é‡å‘½ååˆ—
    top_gainers = top_gainers.rename(columns={'trade_date_str': 'date'})
    
    # æŒ‰æ—¥æœŸæ’åº
    top_gainers = top_gainers.sort_values('date').reset_index(drop=True)
    
    # è®°å½•æ—¥å¿—
    for _, row in top_gainers.iterrows():
        logging.info(f"{row['date']}: æ¶¨å¹…ç¬¬ä¸€ {row['symbol']}, æ¶¨å¹… {row['pct_chg']:.2f}%")
    
    return top_gainers[['date', 'symbol', 'pct_chg']]


def delete_all_tables(confirm: bool = False) -> int:
    """
    åˆ é™¤æ•°æ®åº“ä¸­æ‰€æœ‰çš„è¡¨
    
    Args:
        confirm: æ˜¯å¦ç¡®è®¤åˆ é™¤ï¼Œé»˜è®¤Falseï¼ˆéœ€è¦æ˜¾å¼è®¾ç½®ä¸ºTrueæ‰ä¼šæ‰§è¡Œï¼‰
    
    Returns:
        åˆ é™¤çš„è¡¨æ•°é‡
    """
    if not confirm:
        print("è­¦å‘Šï¼šåˆ é™¤æ‰€æœ‰è¡¨éœ€è¦è®¾ç½® confirm=True å‚æ•°")
        return 0
    
    with engine.connect() as conn:
        # è·å–æ‰€æœ‰è¡¨å
        stmt = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """
        result = conn.execute(text(stmt))
        table_names = [row[0] for row in result.fetchall()]
        
        if not table_names:
            print("æ•°æ®åº“ä¸­æ²¡æœ‰è¡¨")
            return 0
        
        # åˆ é™¤æ‰€æœ‰è¡¨
        deleted_count = 0
        for table_name in table_names:
            try:
                # ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨å¼•å·åŒ…è£¹è¡¨åï¼Œå¤„ç†å¤§å°å†™
                conn.execute(text(f'DROP TABLE IF EXISTS "{table_name}";'))
                print(f"å·²åˆ é™¤è¡¨: {table_name}")
                deleted_count += 1
            except Exception as e:
                print(f"åˆ é™¤è¡¨ {table_name} å¤±è´¥: {e}")
        
        conn.commit()
        print(f"å…±åˆ é™¤ {deleted_count} ä¸ªè¡¨")
        return deleted_count


def delete_table(table_name: str) -> bool:
    """
    é€šè¿‡è¡¨åç›´æ¥åˆ é™¤æ•°æ®åº“ä¸­çš„æŸä¸ªè¡¨ï¼ˆæ”¯æŒä¸åŒºåˆ†å¤§å°å†™å’Œå‰åç©ºæ ¼ï¼‰
    """
    if not table_name:
        return False
        
    table_name = table_name.strip()
    with engine.connect() as conn:
        try:
            # ğŸ”§ æ”¹è¿›ï¼šæå…¶å®½æ¾çš„æŸ¥æ‰¾æ–¹å¼
            stmt = """
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND (
                    table_name = :table_name 
                    OR table_name = LOWER(:table_name)
                    OR table_name = UPPER(:table_name)
                    OR LOWER(table_name) = LOWER(:table_name)
                )
                LIMIT 1
            """
            result = conn.execute(text(stmt), {"table_name": table_name})
            actual_row = result.fetchone()
            
            if not actual_row:
                # æœ€åçš„å°è¯•ï¼šå…¨åº“ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
                stmt_fallback = """
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_name ILIKE :table_name
                    LIMIT 1
                """
                result = conn.execute(text(stmt_fallback), {"table_name": table_name})
                actual_row = result.fetchone()

            if not actual_row:
                print(f"æœªæ‰¾åˆ°åŒ¹é…çš„è¡¨: {table_name}")
                return False
                
            actual_table_name = actual_row[0]
            conn.execute(text(f'DROP TABLE IF EXISTS "{actual_table_name}";'))
            conn.commit()
            print(f"å·²æˆåŠŸåˆ é™¤è¡¨: {actual_table_name} (åŸè¯·æ±‚: {table_name})")
            return True
        except Exception as e:
            print(f"åˆ é™¤è¡¨ {table_name} å¤±è´¥: {e}")
            return False


def delete_kline_data(
    symbol: str,
    interval: str,
    start_time: Optional[str] = None,
    end_time: Optional[str] = None,
    verbose: bool = True
) -> Dict:
    """
    åˆ é™¤æŒ‡å®šäº¤æ˜“å¯¹å’Œintervalçš„Kçº¿æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼Œä¾‹å¦‚ 'BTCUSDT'
        interval: Kçº¿é—´éš”ï¼Œä¾‹å¦‚ '1d', '1h', '4h'
        start_time: å¼€å§‹æ—¶é—´ï¼ˆæ ¼å¼: 'YYYY-MM-DD' æˆ– 'YYYY-MM-DD HH:MM:SS'ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ é™¤å…¨éƒ¨
        end_time: ç»“æŸæ—¶é—´ï¼ˆæ ¼å¼: 'YYYY-MM-DD' æˆ– 'YYYY-MM-DD HH:MM:SS'ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ é™¤å…¨éƒ¨
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        Dict: åˆ é™¤ç»“æœç»Ÿè®¡
    """
    from datetime import datetime
    from sqlalchemy import text
    
    # æ¸…æ´—è¾“å…¥æ•°æ®
    symbol = symbol.strip().upper()
    interval = interval.strip()
    table_name = f'K{interval}{symbol}'
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆæ”¹è¿›ï¼šæåº¦å®½æ¾çš„ä¸åŒºåˆ†å¤§å°å†™æŸ¥æ‰¾ï¼‰
    with engine.connect() as conn:
        stmt = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND (
                table_name = :table_name 
                OR table_name = LOWER(:table_name)
                OR table_name = UPPER(:table_name)
                OR LOWER(table_name) = LOWER(:table_name)
            )
            LIMIT 1
        """
        result = conn.execute(text(stmt), {"table_name": table_name})
        table_row = result.fetchone()
        
        # æœ€åçš„å°è¯•ï¼šä¸åŒºåˆ†å¤§å°å†™çš„ ILIKE åŒ¹é…
        if not table_row:
            stmt_fallback = """
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_name ILIKE :table_name
                LIMIT 1
            """
            result = conn.execute(text(stmt_fallback), {"table_name": table_name})
            table_row = result.fetchone()
        
        if not table_row:
            if verbose:
                print(f"è¡¨ {table_name} ä¸å­˜åœ¨ (å¤„ç†ååç§°)")
            return {
                'success': False,
                'message': f'è¡¨ {table_name} ä¸å­˜åœ¨',
                'deleted_count': 0
            }
        
        # ä½¿ç”¨æ•°æ®åº“ä¸­å®é™…çš„è¡¨åè¿›è¡Œåç»­æ“ä½œ
        actual_table_name = table_row[0]
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œåˆ é™¤æ•´ä¸ªè¡¨
        if start_time is None and end_time is None:
            conn.execute(text(f'DROP TABLE IF EXISTS "{actual_table_name}";'))
            conn.commit()
            if verbose:
                print(f"å·²ä»æ•°æ®åº“å½»åº•åˆ é™¤è¡¨: {actual_table_name}")
            return {
                'success': True,
                'message': f'å·²ä»æ•°æ®åº“å½»åº•åˆ é™¤è¡¨: {actual_table_name}',
                'deleted_count': -1  # -1 è¡¨ç¤ºåˆ é™¤æ•´ä¸ªè¡¨
            }
        
        # åˆ é™¤æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        # å…ˆè·å–åˆ é™¤å‰çš„è®°å½•æ•°
        count_stmt = f'SELECT COUNT(*) FROM "{actual_table_name}"'
        count_result = conn.execute(text(count_stmt))
        before_count = count_result.fetchone()[0]
        
        # æ„å»ºWHEREæ¡ä»¶
        conditions = []
        if start_time:
            try:
                # å°è¯•è§£æå®Œæ•´æ—¶é—´æ ¼å¼
                start_dt = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')
                start_str = start_dt.strftime('%Y-%m-%d %H:%M:%S')
            except ValueError:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ—¥æœŸæ ¼å¼
                try:
                    start_dt = datetime.strptime(start_time, '%Y-%m-%d')
                    start_str = start_dt.strftime('%Y-%m-%d')
                except ValueError:
                    if verbose:
                        print(f"æ— æ•ˆçš„å¼€å§‹æ—¶é—´æ ¼å¼: {start_time}")
                    return {
                        'success': False,
                        'message': f'æ— æ•ˆçš„å¼€å§‹æ—¶é—´æ ¼å¼: {start_time}',
                        'deleted_count': 0
                    }
            conditions.append(f"trade_date >= '{start_str}'")
        
        if end_time:
            try:
                # å°è¯•è§£æå®Œæ•´æ—¶é—´æ ¼å¼
                end_dt = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')
                end_str = end_dt.strftime('%Y-%m-%d %H:%M:%S')
            except ValueError:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ—¥æœŸæ ¼å¼
                try:
                    end_dt = datetime.strptime(end_time, '%Y-%m-%d')
                    # å¯¹äºæ—¥æœŸæ ¼å¼ï¼Œéœ€è¦åŒ…å«å½“å¤©çš„æ‰€æœ‰æ—¶é—´
                    end_str = end_dt.strftime('%Y-%m-%d 23:59:59')
                except ValueError:
                    if verbose:
                        print(f"æ— æ•ˆçš„ç»“æŸæ—¶é—´æ ¼å¼: {end_time}")
                    return {
                        'success': False,
                        'message': f'æ— æ•ˆçš„ç»“æŸæ—¶é—´æ ¼å¼: {end_time}',
                        'deleted_count': 0
                    }
            conditions.append(f"trade_date <= '{end_str}'")
        
        where_clause = " AND ".join(conditions)
        delete_stmt = f'DELETE FROM "{actual_table_name}" WHERE {where_clause}'
        
        try:
            conn.execute(text(delete_stmt))
            conn.commit()
            
            # è·å–åˆ é™¤åçš„è®°å½•æ•°
            count_result = conn.execute(text(count_stmt))
            after_count = count_result.fetchone()[0]
            deleted_count = before_count - after_count
            
            if verbose:
                print(f"å·²ä»è¡¨ {actual_table_name} åˆ é™¤ {deleted_count} æ¡è®°å½•")
            
            return {
                'success': True,
                'message': f'å·²ä»è¡¨ {actual_table_name} åˆ é™¤ {deleted_count} æ¡è®°å½•',
                'deleted_count': deleted_count,
                'before_count': before_count,
                'after_count': after_count
            }
        except Exception as e:
            conn.rollback()
            if verbose:
                print(f"åˆ é™¤æ•°æ®å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'åˆ é™¤å¤±è´¥: {str(e)}',
                'deleted_count': 0
            }


def check_data_integrity(
    symbol: Optional[str] = None,
    interval: str = "1d",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    check_duplicates: bool = True,
    check_missing_dates: bool = True,
    check_data_quality: bool = True,
    verbose: bool = True
) -> Dict:
    """
    æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼Œå¦‚æœä¸ºNoneåˆ™æ£€æŸ¥æ‰€æœ‰äº¤æ˜“å¯¹
        interval: Kçº¿é—´éš”ï¼Œé»˜è®¤"1d"
        start_date: å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼: YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼: YYYY-MM-DD
        check_duplicates: æ˜¯å¦æ£€æŸ¥é‡å¤æ•°æ®ï¼Œé»˜è®¤True
        check_missing_dates: æ˜¯å¦æ£€æŸ¥ç¼ºå¤±æ—¥æœŸï¼Œé»˜è®¤True
        check_data_quality: æ˜¯å¦æ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆç©ºå€¼ã€å¼‚å¸¸å€¼ç­‰ï¼‰ï¼Œé»˜è®¤True
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼Œé»˜è®¤True
    
    Returns:
        Dict: åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
    """
    results = {
        'total_symbols': 0,
        'checked_symbols': 0,
        'symbols_with_issues': [],
        'summary': {
            'duplicates': 0,
            'missing_dates': 0,
            'data_quality_issues': 0,
            'empty_tables': 0
        },
        'details': {}
    }
    
    # è·å–è¦æ£€æŸ¥çš„äº¤æ˜“å¯¹åˆ—è¡¨
    if symbol:
        symbols_to_check = [symbol]
    else:
        symbols_to_check = get_local_symbols(interval=interval)
    
    results['total_symbols'] = len(symbols_to_check)
    
    if verbose:
        print(f"å¼€å§‹æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        print(f"æ—¶é—´é—´éš”: {interval}")
        print(f"å¾…æ£€æŸ¥äº¤æ˜“å¯¹æ•°é‡: {len(symbols_to_check)}")
        if start_date:
            print(f"å¼€å§‹æ—¥æœŸ: {start_date}")
        if end_date:
            print(f"ç»“æŸæ—¥æœŸ: {end_date}")
        print("-" * 60)
    
    for symbol in symbols_to_check:
        symbol_results = {
            'symbol': symbol,
            'table_name': f'K{interval}{symbol}',
            'record_count': 0,
            'date_range': None,
            'issues': [],
            'duplicate_count': 0,
            'missing_dates': [],
            'data_quality_issues': []
        }
        
        try:
            # è·å–æ•°æ®
            df = get_local_kline_data(symbol, interval=interval)
            
            if df.empty:
                symbol_results['issues'].append('è¡¨ä¸ºç©º')
                results['summary']['empty_tables'] += 1
                results['details'][symbol] = symbol_results
                if verbose:
                    print(f"âš ï¸  {symbol}: è¡¨ä¸ºç©º")
                continue
            
            symbol_results['record_count'] = len(df)
            
            # å¤„ç†æ—¥æœŸæ ¼å¼
            if df['trade_date'].dtype == 'object':
                df['trade_date_dt'] = pd.to_datetime(df['trade_date'].str[:10])
            else:
                df['trade_date_dt'] = pd.to_datetime(df['trade_date'])
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date_dt').reset_index(drop=True)
            
            # æ—¥æœŸèŒƒå›´
            min_date = df['trade_date_dt'].min()
            max_date = df['trade_date_dt'].max()
            symbol_results['date_range'] = {
                'start': min_date.strftime('%Y-%m-%d'),
                'end': max_date.strftime('%Y-%m-%d'),
                'days': (max_date - min_date).days + 1
            }
            
            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œè¿›è¡Œè¿‡æ»¤
            if start_date:
                start_dt = pd.to_datetime(start_date)
                df = df[df['trade_date_dt'] >= start_dt]
            if end_date:
                end_dt = pd.to_datetime(end_date)
                df = df[df['trade_date_dt'] <= end_dt]
            
            # 1. æ£€æŸ¥é‡å¤æ•°æ®
            if check_duplicates:
                duplicate_mask = df.duplicated(subset=['trade_date'], keep=False)
                duplicate_count = duplicate_mask.sum()
                if duplicate_count > 0:
                    symbol_results['duplicate_count'] = duplicate_count
                    symbol_results['issues'].append(f'å‘ç° {duplicate_count} æ¡é‡å¤æ•°æ®')
                    results['summary']['duplicates'] += duplicate_count
                    if verbose:
                        print(f"âš ï¸  {symbol}: å‘ç° {duplicate_count} æ¡é‡å¤æ•°æ®")
            
            # 2. æ£€æŸ¥ç¼ºå¤±æ—¥æœŸ
            if check_missing_dates and len(df) > 1:
                # ç¡®å®šæ£€æŸ¥çš„èµ·å§‹æ—¥æœŸ
                # å¦‚æœç”¨æˆ·æä¾›äº†start_dateï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ—¥æœŸ
                # å¦åˆ™ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„æœ€æ—©æ—¥æœŸä½œä¸ºèµ·å§‹æ—¥æœŸï¼ˆå› ä¸ºæ•°æ®å¯èƒ½ä¸æ˜¯ä»äº¤æ˜“æ‰€å¼€å§‹å°±æœ‰çš„ï¼‰
                if start_date:
                    check_start_date = pd.to_datetime(start_date)
                else:
                    # ä½¿ç”¨æ•°æ®ä¸­çš„æœ€æ—©æ—¥æœŸä½œä¸ºèµ·å§‹æ—¥æœŸ
                    check_start_date = df['trade_date_dt'].min()
                    if verbose:
                        logging.debug(f"{symbol} æœªæŒ‡å®šå¼€å§‹æ—¥æœŸï¼Œä½¿ç”¨æ•°æ®æœ€æ—©æ—¥æœŸ: {check_start_date.strftime('%Y-%m-%d')}")
                
                # ç¡®å®šæ£€æŸ¥çš„ç»“æŸæ—¥æœŸ
                if end_date:
                    check_end_date = pd.to_datetime(end_date)
                else:
                    # ä½¿ç”¨æ•°æ®ä¸­çš„æœ€æ™šæ—¥æœŸä½œä¸ºç»“æŸæ—¥æœŸ
                    check_end_date = df['trade_date_dt'].max()
                
                # è½¬æ¢intervalä¸ºpandasé¢‘ç‡
                freq_map = {
                    '1d': 'D',
                    '1h': 'h',  # ä½¿ç”¨å°å†™ 'h' æ›¿ä»£å·²å¼ƒç”¨çš„ 'H'
                    '4h': '4h',  # ä½¿ç”¨å°å†™ 'h' æ›¿ä»£å·²å¼ƒç”¨çš„ 'H'
                    '1m': '1min',
                    '5m': '5min',
                    '15m': '15min',
                    '30m': '30min'
                }
                freq = freq_map.get(interval, 'D')
                
                # ç”ŸæˆæœŸæœ›çš„æ—¥æœŸåºåˆ—ï¼ˆä»æ£€æŸ¥èµ·å§‹æ—¥æœŸåˆ°ç»“æŸæ—¥æœŸï¼‰
                if freq != 'D':
                    date_range = pd.date_range(
                        start=check_start_date,
                        end=check_end_date,
                        freq=freq
                    )
                else:
                    date_range = pd.date_range(
                        start=check_start_date,
                        end=check_end_date,
                        freq='D'
                    )
                
                # è·å–å®é™…å­˜åœ¨çš„æ—¥æœŸ
                existing_dates = set(df['trade_date_dt'].dt.date)
                
                # åªæ£€æŸ¥åœ¨æ£€æŸ¥èŒƒå›´å†…çš„æ—¥æœŸ
                check_date_range = pd.date_range(start=check_start_date, end=check_end_date, freq=freq)
                check_date_set = set(check_date_range.date)
                
                # æ‰¾å‡ºåœ¨æ£€æŸ¥èŒƒå›´å†…ä½†ä¸å­˜åœ¨çš„æ•°æ®
                missing_dates = sorted(check_date_set - existing_dates)
                
                if missing_dates:
                    symbol_results['missing_dates'] = [d.strftime('%Y-%m-%d') for d in missing_dates[:10]]  # åªä¿å­˜å‰10ä¸ª
                    missing_count = len(missing_dates)
                    symbol_results['issues'].append(f'ç¼ºå¤± {missing_count} ä¸ªæ—¥æœŸ')
                    results['summary']['missing_dates'] += missing_count
                    if verbose:
                        print(f"âš ï¸  {symbol}: ç¼ºå¤± {missing_count} ä¸ªæ—¥æœŸï¼ˆæ˜¾ç¤ºå‰10ä¸ª: {symbol_results['missing_dates']}ï¼‰")
            
            # 3. æ£€æŸ¥æ•°æ®è´¨é‡
            if check_data_quality:
                quality_issues = []
                
                # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦æœ‰ç©ºå€¼
                required_fields = ['open', 'high', 'low', 'close', 'volume']
                for field in required_fields:
                    null_count = df[field].isna().sum()
                    if null_count > 0:
                        quality_issues.append(f'{field} å­—æ®µæœ‰ {null_count} ä¸ªç©ºå€¼')
                
                # æ£€æŸ¥ä»·æ ¼æ•°æ®çš„åˆç†æ€§
                invalid_price_mask = (
                    (df['high'] < df['low']) |
                    (df['open'] > df['high']) |
                    (df['open'] < df['low']) |
                    (df['close'] > df['high']) |
                    (df['close'] < df['low'])
                )
                invalid_price_count = invalid_price_mask.sum()
                if invalid_price_count > 0:
                    # è·å–å…·ä½“çš„é—®é¢˜æ•°æ®
                    invalid_rows = df[invalid_price_mask].copy()
                    # åªä¿ç•™å‰20æ¡é—®é¢˜æ•°æ®ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                    invalid_rows_display = invalid_rows.head(20)
                    
                    invalid_data_list = []
                    for idx, row in invalid_rows_display.iterrows():
                        issues = []
                        if row['high'] < row['low']:
                            issues.append(f"high({row['high']}) < low({row['low']})")
                        if row['open'] > row['high']:
                            issues.append(f"open({row['open']}) > high({row['high']})")
                        if row['open'] < row['low']:
                            issues.append(f"open({row['open']}) < low({row['low']})")
                        if row['close'] > row['high']:
                            issues.append(f"close({row['close']}) > high({row['high']})")
                        if row['close'] < row['low']:
                            issues.append(f"close({row['close']}) < low({row['low']})")
                        
                        trade_date = row.get('trade_date', 'N/A')
                        if isinstance(trade_date, pd.Timestamp):
                            trade_date = trade_date.strftime('%Y-%m-%d %H:%M:%S')
                        elif pd.isna(trade_date):
                            trade_date = 'N/A'
                        
                        invalid_data_list.append({
                            'trade_date': str(trade_date),
                            'open': float(row['open']) if pd.notna(row['open']) else None,
                            'high': float(row['high']) if pd.notna(row['high']) else None,
                            'low': float(row['low']) if pd.notna(row['low']) else None,
                            'close': float(row['close']) if pd.notna(row['close']) else None,
                            'issues': issues
                        })
                    
                    quality_issues.append(f'å‘ç° {invalid_price_count} æ¡ä»·æ ¼æ•°æ®ä¸åˆç†ï¼ˆhigh < low æˆ– open/close è¶…å‡ºèŒƒå›´ï¼‰')
                    # å°†å…·ä½“çš„é—®é¢˜æ•°æ®æ·»åŠ åˆ°symbol_resultsä¸­
                    if 'invalid_price_data' not in symbol_results:
                        symbol_results['invalid_price_data'] = []
                    symbol_results['invalid_price_data'].extend(invalid_data_list)
                    
                    if verbose:
                        print(f"âš ï¸  {symbol}: å‘ç° {invalid_price_count} æ¡ä»·æ ¼æ•°æ®ä¸åˆç†")
                        for data in invalid_data_list[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
                            print(f"   æ—¥æœŸ: {data['trade_date']}, open={data['open']}, high={data['high']}, low={data['low']}, close={data['close']}, é—®é¢˜: {', '.join(data['issues'])}")
                        if invalid_price_count > 5:
                            print(f"   ... è¿˜æœ‰ {invalid_price_count - 5} æ¡é—®é¢˜æ•°æ®æœªæ˜¾ç¤º")
                
                # æ£€æŸ¥ä»·æ ¼æ˜¯å¦ä¸º0æˆ–è´Ÿæ•°
                price_fields = ['open', 'high', 'low', 'close']
                for field in price_fields:
                    invalid_count = (df[field] <= 0).sum()
                    if invalid_count > 0:
                        quality_issues.append(f'{field} å­—æ®µæœ‰ {invalid_count} ä¸ªæ— æ•ˆå€¼ï¼ˆ<=0ï¼‰')
                
                # æ£€æŸ¥æˆäº¤é‡æ˜¯å¦ä¸ºè´Ÿæ•°
                if 'volume' in df.columns:
                    invalid_volume_count = (df['volume'] < 0).sum()
                    if invalid_volume_count > 0:
                        quality_issues.append(f'volume å­—æ®µæœ‰ {invalid_volume_count} ä¸ªè´Ÿæ•°')
                
                if quality_issues:
                    symbol_results['data_quality_issues'] = quality_issues
                    symbol_results['issues'].extend(quality_issues)
                    results['summary']['data_quality_issues'] += len(quality_issues)
                    if verbose:
                        for issue in quality_issues:
                            print(f"âš ï¸  {symbol}: {issue}")
            
            # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œæ ‡è®°ä¸ºé€šè¿‡
            if not symbol_results['issues']:
                results['checked_symbols'] += 1
                if verbose:
                    print(f"âœ… {symbol}: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼ˆ{symbol_results['record_count']} æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´: {symbol_results['date_range']['start']} è‡³ {symbol_results['date_range']['end']}ï¼‰")
            else:
                results['symbols_with_issues'].append(symbol)
                results['checked_symbols'] += 1
            
            results['details'][symbol] = symbol_results
            
        except Exception as e:
            symbol_results['issues'].append(f'æ£€æŸ¥å¤±è´¥: {str(e)}')
            results['symbols_with_issues'].append(symbol)
            results['details'][symbol] = symbol_results
            if verbose:
                print(f"âŒ {symbol}: æ£€æŸ¥å¤±è´¥ - {str(e)}")
    
    # è¾“å‡ºæ€»ç»“
    if verbose:
        print("-" * 60)
        print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æ€»ç»“:")
        print(f"æ€»äº¤æ˜“å¯¹æ•°: {results['total_symbols']}")
        print(f"å·²æ£€æŸ¥: {results['checked_symbols']}")
        print(f"æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹: {len(results['symbols_with_issues'])}")
        print(f"ç©ºè¡¨æ•°é‡: {results['summary']['empty_tables']}")
        print(f"é‡å¤æ•°æ®æ€»æ•°: {results['summary']['duplicates']}")
        print(f"ç¼ºå¤±æ—¥æœŸæ€»æ•°: {results['summary']['missing_dates']}")
        print(f"æ•°æ®è´¨é‡é—®é¢˜æ€»æ•°: {results['summary']['data_quality_issues']}")
        
        if results['symbols_with_issues']:
            print(f"\næœ‰é—®é¢˜çš„äº¤æ˜“å¯¹åˆ—è¡¨: {', '.join(results['symbols_with_issues'][:20])}")
            if len(results['symbols_with_issues']) > 20:
                print(f"... è¿˜æœ‰ {len(results['symbols_with_issues']) - 20} ä¸ªäº¤æ˜“å¯¹")
    
    return results


def generate_integrity_report(
    check_results: Dict,
    interval: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    check_duplicates: bool = True,
    check_missing_dates: bool = True,
    check_data_quality: bool = True,
    output_format: str = "text",
    output_file: Optional[str] = None
) -> str:
    """
    ç”Ÿæˆæ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š
    
    Args:
        check_results: check_data_integrity() è¿”å›çš„æ£€æŸ¥ç»“æœ
        interval: Kçº¿é—´éš”
        start_date: æ£€æŸ¥çš„å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        end_date: æ£€æŸ¥çš„ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
        check_duplicates: æ˜¯å¦æ£€æŸ¥äº†é‡å¤æ•°æ®
        check_missing_dates: æ˜¯å¦æ£€æŸ¥äº†ç¼ºå¤±æ—¥æœŸ
        check_data_quality: æ˜¯å¦æ£€æŸ¥äº†æ•°æ®è´¨é‡
        output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰: "text", "json", "html", "markdown"
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜åˆ°æ–‡ä»¶
    
    Returns:
        str: æŠ¥å‘Šå†…å®¹
    """
    from datetime import datetime
    
    report_lines = []
    
    # æŠ¥å‘Šå¤´éƒ¨
    report_lines.append("=" * 80)
    report_lines.append("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Kçº¿é—´éš”: {interval}")
    if start_date:
        report_lines.append(f"å¼€å§‹æ—¥æœŸ: {start_date}")
    if end_date:
        report_lines.append(f"ç»“æŸæ—¥æœŸ: {end_date}")
    report_lines.append("")
    
    # æ£€æŸ¥é…ç½®
    report_lines.append("æ£€æŸ¥é…ç½®:")
    report_lines.append(f"  - æ£€æŸ¥é‡å¤æ•°æ®: {'æ˜¯' if check_duplicates else 'å¦'}")
    report_lines.append(f"  - æ£€æŸ¥ç¼ºå¤±æ—¥æœŸ: {'æ˜¯' if check_missing_dates else 'å¦'}")
    report_lines.append(f"  - æ£€æŸ¥æ•°æ®è´¨é‡: {'æ˜¯' if check_data_quality else 'å¦'}")
    report_lines.append("")
    
    # æ€»ä½“ç»Ÿè®¡
    report_lines.append("=" * 80)
    report_lines.append("æ€»ä½“ç»Ÿè®¡")
    report_lines.append("=" * 80)
    report_lines.append(f"æ€»äº¤æ˜“å¯¹æ•°: {check_results['total_symbols']}")
    report_lines.append(f"å·²æ£€æŸ¥äº¤æ˜“å¯¹æ•°: {check_results['checked_symbols']}")
    report_lines.append(f"æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹æ•°: {len(check_results['symbols_with_issues'])}")
    report_lines.append(f"æ•°æ®å®Œæ•´æ€§: {((check_results['checked_symbols'] - len(check_results['symbols_with_issues'])) / check_results['checked_symbols'] * 100):.2f}%" if check_results['checked_symbols'] > 0 else "N/A")
    report_lines.append("")
    
    # é—®é¢˜åˆ†ç±»ç»Ÿè®¡
    report_lines.append("é—®é¢˜åˆ†ç±»ç»Ÿè®¡:")
    report_lines.append(f"  - ç©ºè¡¨æ•°é‡: {check_results['summary']['empty_tables']}")
    report_lines.append(f"  - é‡å¤æ•°æ®æ€»æ•°: {check_results['summary']['duplicates']}")
    report_lines.append(f"  - ç¼ºå¤±æ—¥æœŸæ€»æ•°: {check_results['summary']['missing_dates']}")
    report_lines.append(f"  - æ•°æ®è´¨é‡é—®é¢˜æ€»æ•°: {check_results['summary']['data_quality_issues']}")
    report_lines.append("")
    
    # æ•°æ®è´¨é‡è¯„åˆ†
    total_issues = (
        check_results['summary']['duplicates'] +
        check_results['summary']['missing_dates'] +
        check_results['summary']['data_quality_issues']
    )
    total_records = sum(details.get('record_count', 0) for details in check_results['details'].values())
    
    # åˆå§‹åŒ–è´¨é‡è¯„åˆ†å˜é‡
    quality_score = None
    quality_level = None
    
    if total_records > 0:
        issue_rate = (total_issues / total_records) * 100 if total_records > 0 else 0
        quality_score = max(0, 100 - issue_rate * 10)  # æ¯ä¸ªé—®é¢˜æ‰£10åˆ†ï¼Œæœ€ä½0åˆ†
        report_lines.append("æ•°æ®è´¨é‡è¯„åˆ†:")
        report_lines.append(f"  - æ€»è®°å½•æ•°: {total_records:,}")
        report_lines.append(f"  - é—®é¢˜æ€»æ•°: {total_issues}")
        report_lines.append(f"  - é—®é¢˜ç‡: {issue_rate:.4f}%")
        report_lines.append(f"  - è´¨é‡è¯„åˆ†: {quality_score:.2f}/100")
        
        # è´¨é‡ç­‰çº§
        if quality_score >= 95:
            quality_level = "ä¼˜ç§€"
        elif quality_score >= 85:
            quality_level = "è‰¯å¥½"
        elif quality_score >= 70:
            quality_level = "ä¸€èˆ¬"
        elif quality_score >= 60:
            quality_level = "è¾ƒå·®"
        else:
            quality_level = "å¾ˆå·®"
        report_lines.append(f"  - è´¨é‡ç­‰çº§: {quality_level}")
        report_lines.append("")
    
    # æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹è¯¦æƒ…
    if check_results['symbols_with_issues']:
        report_lines.append("=" * 80)
        report_lines.append("æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹è¯¦æƒ…")
        report_lines.append("=" * 80)
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç±»
        empty_tables = []
        duplicate_issues = []
        missing_date_issues = []
        quality_issues = []
        
        for symbol in check_results['symbols_with_issues']:
            details = check_results['details'][symbol]
            if details['record_count'] == 0:
                empty_tables.append(symbol)
            if details['duplicate_count'] > 0:
                duplicate_issues.append((symbol, details))
            if details['missing_dates']:
                missing_date_issues.append((symbol, details))
            if details['data_quality_issues']:
                quality_issues.append((symbol, details))
        
        # ç©ºè¡¨
        if empty_tables:
            report_lines.append(f"\nç©ºè¡¨äº¤æ˜“å¯¹ ({len(empty_tables)} ä¸ª):")
            for symbol in empty_tables[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                report_lines.append(f"  - {symbol}")
            if len(empty_tables) > 20:
                report_lines.append(f"  ... è¿˜æœ‰ {len(empty_tables) - 20} ä¸ªç©ºè¡¨")
        
        # é‡å¤æ•°æ®
        if duplicate_issues:
            report_lines.append(f"\næœ‰é‡å¤æ•°æ®çš„äº¤æ˜“å¯¹ ({len(duplicate_issues)} ä¸ª):")
            for symbol, details in duplicate_issues[:10]:
                report_lines.append(f"  - {symbol}: {details['duplicate_count']} æ¡é‡å¤æ•°æ®")
            if len(duplicate_issues) > 10:
                report_lines.append(f"  ... è¿˜æœ‰ {len(duplicate_issues) - 10} ä¸ªäº¤æ˜“å¯¹æœ‰é‡å¤æ•°æ®")
        
        # ç¼ºå¤±æ—¥æœŸ
        if missing_date_issues:
            report_lines.append(f"\næœ‰ç¼ºå¤±æ—¥æœŸçš„äº¤æ˜“å¯¹ ({len(missing_date_issues)} ä¸ª):")
            for symbol, details in missing_date_issues[:10]:
                missing_count = len(details['missing_dates'])
                date_range = details.get('date_range', {})
                if date_range:
                    report_lines.append(
                        f"  - {symbol}: ç¼ºå¤± {missing_count} ä¸ªæ—¥æœŸ "
                        f"(æ•°æ®èŒƒå›´: {date_range['start']} è‡³ {date_range['end']}, "
                        f"å…± {date_range['days']} å¤©, å®é™…æœ‰ {details['record_count']} æ¡è®°å½•)"
                    )
                    if details['missing_dates']:
                        missing_dates_str = ', '.join(details['missing_dates'][:5])
                        if missing_count > 5:
                            missing_dates_str += f" ... (è¿˜æœ‰ {missing_count - 5} ä¸ª)"
                        report_lines.append(f"    ç¼ºå¤±æ—¥æœŸç¤ºä¾‹: {missing_dates_str}")
                else:
                    report_lines.append(f"  - {symbol}: ç¼ºå¤± {missing_count} ä¸ªæ—¥æœŸ")
            if len(missing_date_issues) > 10:
                report_lines.append(f"  ... è¿˜æœ‰ {len(missing_date_issues) - 10} ä¸ªäº¤æ˜“å¯¹æœ‰ç¼ºå¤±æ—¥æœŸ")
        
        # æ•°æ®è´¨é‡é—®é¢˜
        if quality_issues:
            report_lines.append(f"\næœ‰æ•°æ®è´¨é‡é—®é¢˜çš„äº¤æ˜“å¯¹ ({len(quality_issues)} ä¸ª):")
            for symbol, details in quality_issues[:10]:
                report_lines.append(f"  - {symbol}:")
                for issue in details['data_quality_issues']:
                    report_lines.append(f"    * {issue}")
                
                # å¦‚æœæœ‰ä»·æ ¼æ•°æ®ä¸åˆç†çš„é—®é¢˜ï¼Œæ˜¾ç¤ºå…·ä½“çš„é—®é¢˜æ•°æ®
                if 'invalid_price_data' in details and details['invalid_price_data']:
                    invalid_data_list = details['invalid_price_data']
                    report_lines.append(f"    ä»·æ ¼æ•°æ®ä¸åˆç†è¯¦æƒ… (å…± {len(invalid_data_list)} æ¡):")
                    # æ˜¾ç¤ºæ‰€æœ‰é—®é¢˜æ•°æ®ï¼ˆæŠ¥å‘Šä¸­åº”è¯¥åŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
                    for idx, data in enumerate(invalid_data_list, 1):
                        report_lines.append(f"      [{idx}] æ—¥æœŸ: {data['trade_date']}")
                        report_lines.append(f"          open={data['open']}, high={data['high']}, low={data['low']}, close={data['close']}")
                        report_lines.append(f"          é—®é¢˜: {', '.join(data['issues'])}")
            if len(quality_issues) > 10:
                report_lines.append(f"  ... è¿˜æœ‰ {len(quality_issues) - 10} ä¸ªäº¤æ˜“å¯¹æœ‰æ•°æ®è´¨é‡é—®é¢˜")
    
    # æ­£å¸¸äº¤æ˜“å¯¹ç»Ÿè®¡
    normal_symbols = [
        symbol for symbol, details in check_results['details'].items()
        if symbol not in check_results['symbols_with_issues']
    ]
    if normal_symbols:
        report_lines.append("\n" + "=" * 80)
        report_lines.append("æ•°æ®æ­£å¸¸çš„äº¤æ˜“å¯¹")
        report_lines.append("=" * 80)
        report_lines.append(f"æ­£å¸¸äº¤æ˜“å¯¹æ•°: {len(normal_symbols)}")
        if len(normal_symbols) <= 20:
            for symbol in normal_symbols:
                details = check_results['details'][symbol]
                date_range = details.get('date_range', {})
                if date_range:
                    report_lines.append(
                        f"  - {symbol}: {details['record_count']} æ¡è®°å½• "
                        f"({date_range['start']} è‡³ {date_range['end']})"
                    )
                else:
                    report_lines.append(f"  - {symbol}: {details['record_count']} æ¡è®°å½•")
        else:
            report_lines.append(f"  (å‰20ä¸ª)")
            for symbol in normal_symbols[:20]:
                details = check_results['details'][symbol]
                date_range = details.get('date_range', {})
                if date_range:
                    report_lines.append(
                        f"  - {symbol}: {details['record_count']} æ¡è®°å½• "
                        f"({date_range['start']} è‡³ {date_range['end']})"
                    )
                else:
                    report_lines.append(f"  - {symbol}: {details['record_count']} æ¡è®°å½•")
            report_lines.append(f"  ... è¿˜æœ‰ {len(normal_symbols) - 20} ä¸ªæ­£å¸¸äº¤æ˜“å¯¹")
    
    # å»ºè®®å’Œä¿®å¤æ–¹æ¡ˆ
    report_lines.append("\n" + "=" * 80)
    report_lines.append("å»ºè®®å’Œä¿®å¤æ–¹æ¡ˆ")
    report_lines.append("=" * 80)
    
    if check_results['summary']['empty_tables'] > 0:
        report_lines.append(f"\n1. å‘ç° {check_results['summary']['empty_tables']} ä¸ªç©ºè¡¨:")
        report_lines.append("   å»ºè®®: ä½¿ç”¨æ•°æ®ä¸‹è½½åŠŸèƒ½ä¸‹è½½è¿™äº›äº¤æ˜“å¯¹çš„æ•°æ®")
        report_lines.append(f"   å‘½ä»¤: python download_klines.py --interval {interval} --missing-only")
    
    if check_results['summary']['missing_dates'] > 0:
        report_lines.append(f"\n2. å‘ç° {check_results['summary']['missing_dates']} ä¸ªç¼ºå¤±æ—¥æœŸ:")
        report_lines.append("   å»ºè®®: ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ•°æ®åŠŸèƒ½è¡¥å……ç¼ºå¤±çš„æ—¥æœŸ")
        report_lines.append("   æ–¹æ³•: åœ¨å‰ç«¯ç‚¹å‡»'è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ•°æ®'æŒ‰é’®ï¼Œæˆ–ä½¿ç”¨å‘½ä»¤è¡Œ:")
        report_lines.append(f"         python data.py --interval {interval} --auto-download")
    
    if check_results['summary']['duplicates'] > 0:
        report_lines.append(f"\n3. å‘ç° {check_results['summary']['duplicates']} æ¡é‡å¤æ•°æ®:")
        report_lines.append("   å»ºè®®: æ¸…ç†é‡å¤æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨æ•°æ®åº“å·¥å…·åˆ é™¤é‡å¤è®°å½•")
        report_lines.append("   æ³¨æ„: é‡å¤æ•°æ®å¯èƒ½å½±å“åˆ†æç»“æœçš„å‡†ç¡®æ€§")
    
    if check_results['summary']['data_quality_issues'] > 0:
        report_lines.append(f"\n4. å‘ç° {check_results['summary']['data_quality_issues']} ä¸ªæ•°æ®è´¨é‡é—®é¢˜:")
        report_lines.append("   å»ºè®®: æ£€æŸ¥æ•°æ®æ¥æºï¼Œå¯èƒ½éœ€è¦é‡æ–°ä¸‹è½½æœ‰é—®é¢˜çš„æ•°æ®")
        report_lines.append("   æ³¨æ„: æ•°æ®è´¨é‡é—®é¢˜å¯èƒ½å¯¼è‡´å›æµ‹å’Œåˆ†æç»“æœä¸å‡†ç¡®")
    
    if total_issues == 0:
        report_lines.append("\nâœ“ æ­å–œï¼æ‰€æœ‰æ•°æ®æ£€æŸ¥é€šè¿‡ï¼Œæ•°æ®å®Œæ•´æ€§è‰¯å¥½ã€‚")
    
    # æŠ¥å‘Šå°¾éƒ¨
    report_lines.append("\n" + "=" * 80)
    report_lines.append("æŠ¥å‘Šç»“æŸ")
    report_lines.append("=" * 80)
    
    report_content = "\n".join(report_lines)
    
    # æ ¹æ®æ ¼å¼è¾“å‡º
    if output_format == "json":
        import json
        report_dict = {
            "report_time": datetime.now().isoformat(),
            "config": {
                "interval": interval,
                "start_date": start_date,
                "end_date": end_date,
                "check_duplicates": check_duplicates,
                "check_missing_dates": check_missing_dates,
                "check_data_quality": check_data_quality
            },
            "summary": check_results['summary'],
            "statistics": {
                "total_symbols": check_results['total_symbols'],
                "checked_symbols": check_results['checked_symbols'],
                "symbols_with_issues": len(check_results['symbols_with_issues']),
                "quality_score": quality_score if total_records > 0 else None,
                "quality_level": quality_level if total_records > 0 else None
            },
            "details": check_results['details'],
            "text_report": report_content
        }
        report_content = json.dumps(report_dict, ensure_ascii=False, indent=2)
    
    elif output_format == "html":
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        h1 {{ color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }}
        h2 {{ color: #555; margin-top: 30px; }}
        .stat {{ background: #f9f9f9; padding: 15px; margin: 10px 0; border-left: 4px solid #2196F3; }}
        .issue {{ background: #fff3cd; padding: 10px; margin: 5px 0; border-left: 4px solid #ffc107; }}
        .success {{ background: #d4edda; padding: 10px; margin: 5px 0; border-left: 4px solid #28a745; }}
        .error {{ background: #f8d7da; padding: 10px; margin: 5px 0; border-left: 4px solid #dc3545; }}
        pre {{ background: #f4f4f4; padding: 10px; border-radius: 4px; overflow-x: auto; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #4CAF50; color: white; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>Kçº¿é—´éš”:</strong> {interval}</p>
        {f"<p><strong>å¼€å§‹æ—¥æœŸ:</strong> {start_date}</p>" if start_date else ""}
        {f"<p><strong>ç»“æŸæ—¥æœŸ:</strong> {end_date}</p>" if end_date else ""}
        
        <h2>æ€»ä½“ç»Ÿè®¡</h2>
        <div class="stat">
            <p><strong>æ€»äº¤æ˜“å¯¹æ•°:</strong> {check_results['total_symbols']}</p>
            <p><strong>å·²æ£€æŸ¥äº¤æ˜“å¯¹æ•°:</strong> {check_results['checked_symbols']}</p>
            <p><strong>æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹æ•°:</strong> {len(check_results['symbols_with_issues'])}</p>
        </div>
        
        <h2>é—®é¢˜åˆ†ç±»</h2>
        <div class="issue">
            <p><strong>ç©ºè¡¨æ•°é‡:</strong> {check_results['summary']['empty_tables']}</p>
            <p><strong>é‡å¤æ•°æ®æ€»æ•°:</strong> {check_results['summary']['duplicates']}</p>
            <p><strong>ç¼ºå¤±æ—¥æœŸæ€»æ•°:</strong> {check_results['summary']['missing_dates']}</p>
            <p><strong>æ•°æ®è´¨é‡é—®é¢˜æ€»æ•°:</strong> {check_results['summary']['data_quality_issues']}</p>
        </div>
        
        <h2>è¯¦ç»†ç»“æœ</h2>
        <pre>{report_content.replace('<', '&lt;').replace('>', '&gt;')}</pre>
    </div>
</body>
</html>
        """
        report_content = html_content
    
    elif output_format == "markdown":
        md_content = f"""# æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Kçº¿é—´éš”:** {interval}  
{f"**å¼€å§‹æ—¥æœŸ:** {start_date}  " if start_date else ""}
{f"**ç»“æŸæ—¥æœŸ:** {end_date}  " if end_date else ""}

## æ£€æŸ¥é…ç½®

- æ£€æŸ¥é‡å¤æ•°æ®: {'æ˜¯' if check_duplicates else 'å¦'}
- æ£€æŸ¥ç¼ºå¤±æ—¥æœŸ: {'æ˜¯' if check_missing_dates else 'å¦'}
- æ£€æŸ¥æ•°æ®è´¨é‡: {'æ˜¯' if check_data_quality else 'å¦'}

## æ€»ä½“ç»Ÿè®¡

| é¡¹ç›® | æ•°é‡ |
|------|------|
| æ€»äº¤æ˜“å¯¹æ•° | {check_results['total_symbols']} |
| å·²æ£€æŸ¥äº¤æ˜“å¯¹æ•° | {check_results['checked_symbols']} |
| æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹æ•° | {len(check_results['symbols_with_issues'])} |

## é—®é¢˜åˆ†ç±»ç»Ÿè®¡

| é—®é¢˜ç±»å‹ | æ•°é‡ |
|----------|------|
| ç©ºè¡¨æ•°é‡ | {check_results['summary']['empty_tables']} |
| é‡å¤æ•°æ®æ€»æ•° | {check_results['summary']['duplicates']} |
| ç¼ºå¤±æ—¥æœŸæ€»æ•° | {check_results['summary']['missing_dates']} |
| æ•°æ®è´¨é‡é—®é¢˜æ€»æ•° | {check_results['summary']['data_quality_issues']} |

## è¯¦ç»†ç»“æœ

{report_content.replace('=', '#').replace('  -', '-')}
"""
        report_content = md_content
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    return report_content


def generate_download_script_from_check(
    check_results: Dict,
    interval: str,
    output_file: Optional[str] = None,
    auto_execute: bool = False
) -> str:
    """
    æ ¹æ®æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ç»“æœç”Ÿæˆä¸‹è½½è„šæœ¬
    
    Args:
        check_results: check_data_integrity() è¿”å›çš„æ£€æŸ¥ç»“æœ
        interval: Kçº¿é—´éš”
        output_file: è¾“å‡ºè„šæœ¬æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åªè¿”å›è„šæœ¬å†…å®¹
        auto_execute: æ˜¯å¦è‡ªåŠ¨æ‰§è¡Œä¸‹è½½ï¼ˆé»˜è®¤Falseï¼‰
    
    Returns:
        str: ç”Ÿæˆçš„ä¸‹è½½è„šæœ¬å†…å®¹
    """
    from datetime import datetime, timedelta
    
    script_lines = [
        "#!/bin/bash",
        f"# æ ¹æ®æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ç»“æœè‡ªåŠ¨ç”Ÿæˆçš„ä¸‹è½½è„šæœ¬",
        f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"# Kçº¿é—´éš”: {interval}",
        "",
        "# ä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹å’Œç¼ºå¤±æ—¥æœŸçš„æ•°æ®",
        ""
    ]
    
    # æ”¶é›†éœ€è¦ä¸‹è½½çš„äº¤æ˜“å¯¹å’Œæ—¥æœŸèŒƒå›´
    symbols_to_download = []
    empty_tables = []
    symbols_with_missing_dates = {}
    
    for symbol, details in check_results['details'].items():
        if details['record_count'] == 0:
            # ç©ºè¡¨ï¼Œéœ€è¦å®Œæ•´ä¸‹è½½
            empty_tables.append(symbol)
        elif details['missing_dates']:
            # æœ‰ç¼ºå¤±æ—¥æœŸ
            symbols_with_missing_dates[symbol] = {
                'missing_dates': details['missing_dates'],
                'date_range': details['date_range']
            }
    
    # ç”Ÿæˆä¸‹è½½å‘½ä»¤
    if empty_tables:
        script_lines.append("# ä¸‹è½½ç©ºè¡¨çš„æ•°æ®")
        for symbol in empty_tables:
            details = check_results['details'][symbol]
            if details.get('date_range'):
                start_date = details['date_range']['start']
                end_date = details['date_range']['end']
            else:
                # é»˜è®¤ä¸‹è½½æœ€è¿‘1å¹´çš„æ•°æ®
                end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
                start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')
            
            script_lines.append(
                f"python download_klines.py --interval {interval} --symbols {symbol} "
                f"--start-time {start_date} --end-time {end_date}"
            )
        script_lines.append("")
    
    if symbols_with_missing_dates:
        script_lines.append("# ä¸‹è½½ç¼ºå¤±æ—¥æœŸçš„æ•°æ®")
        for symbol, info in symbols_with_missing_dates.items():
            missing_dates = info['missing_dates']
            if missing_dates:
                # è®¡ç®—ç¼ºå¤±æ—¥æœŸçš„èŒƒå›´
                missing_dates_sorted = sorted(missing_dates)
                start_date = missing_dates_sorted[0]
                end_date = missing_dates_sorted[-1]
                
                script_lines.append(
                    f"python download_klines.py --interval {interval} --symbols {symbol} "
                    f"--start-time {start_date} --end-time {end_date}"
                )
        script_lines.append("")
    
    # å¦‚æœæœ‰ç¼ºå¤±çš„äº¤æ˜“å¯¹ï¼ˆåœ¨äº¤æ˜“æ‰€ä½†ä¸åœ¨æœ¬åœ°ï¼‰
    if check_results['summary']['empty_tables'] > 0:
        script_lines.append("# ä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
        script_lines.append(f"python download_klines.py --interval {interval} --missing-only")
        script_lines.append("")
    
    script_content = "\n".join(script_lines)
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œå†™å…¥æ–‡ä»¶
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        import os
        os.chmod(output_file, 0o755)  # æ·»åŠ æ‰§è¡Œæƒé™
        print(f"ä¸‹è½½è„šæœ¬å·²ä¿å­˜åˆ°: {output_file}")
    
    # å¦‚æœå¯ç”¨è‡ªåŠ¨æ‰§è¡Œ
    if auto_execute:
        from download_klines import download_kline_data, download_all_symbols
        from datetime import datetime as dt
        
        print("å¼€å§‹è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ•°æ®...")
        
        # ä¸‹è½½ç©ºè¡¨
        for symbol in empty_tables:
            details = check_results['details'][symbol]
            if details['date_range']:
                start_date = dt.strptime(details['date_range']['start'], '%Y-%m-%d')
                end_date = dt.strptime(details['date_range']['end'], '%Y-%m-%d')
            else:
                end_date = dt.now() - timedelta(days=1)
                start_date = end_date - timedelta(days=365)
            
            print(f"ä¸‹è½½ {symbol} çš„æ•°æ®...")
            download_kline_data(
                symbol=symbol,
                interval=interval,
                start_time=start_date,
                end_time=end_date,
                update_existing=True  # å¼ºåˆ¶æ›´æ–°ï¼Œç¡®ä¿ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
            )
        
        # ä¸‹è½½ç¼ºå¤±æ—¥æœŸ
        for symbol, info in symbols_with_missing_dates.items():
            missing_dates = info['missing_dates']
            if missing_dates:
                missing_dates_sorted = sorted(missing_dates)
                start_date = dt.strptime(missing_dates_sorted[0], '%Y-%m-%d')
                end_date = dt.strptime(missing_dates_sorted[-1], '%Y-%m-%d')
                
                print(f"ä¸‹è½½ {symbol} ç¼ºå¤±æ—¥æœŸçš„æ•°æ® ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})...")
                download_kline_data(
                    symbol=symbol,
                    interval=interval,
                    start_time=start_date,
                    end_time=end_date,
                    update_existing=True  # å¼ºåˆ¶æ›´æ–°ï¼Œç¡®ä¿ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
                )
        
        print("è‡ªåŠ¨ä¸‹è½½å®Œæˆï¼")
    
    return script_content


def download_missing_data_from_check(
    check_results: Dict,
    interval: str,
    verbose: bool = True
) -> Dict:
    """
    æ ¹æ®æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ç»“æœç›´æ¥ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
    
    Args:
        check_results: check_data_integrity() è¿”å›çš„æ£€æŸ¥ç»“æœ
        interval: Kçº¿é—´éš”
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        Dict: ä¸‹è½½ç»“æœç»Ÿè®¡
    """
    from download_klines import download_kline_data
    from datetime import datetime, timedelta
    
    download_stats = {
        'empty_tables_downloaded': 0,
        'missing_dates_downloaded': 0,
        'failed': [],
        'success': []
    }
    
    # ä¸‹è½½ç©ºè¡¨
    for symbol, details in check_results['details'].items():
        if details['record_count'] == 0:
            try:
                if details['date_range']:
                    start_date = datetime.strptime(details['date_range']['start'], '%Y-%m-%d')
                    end_date = datetime.strptime(details['date_range']['end'], '%Y-%m-%d')
                else:
                    # é»˜è®¤ä¸‹è½½æœ€è¿‘1å¹´çš„æ•°æ®
                    end_date = datetime.now() - timedelta(days=1)
                    start_date = end_date - timedelta(days=365)
                
                if verbose:
                    print(f"ä¸‹è½½ç©ºè¡¨ {symbol} çš„æ•°æ® ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})...")
                
                success = download_kline_data(
                    symbol=symbol,
                    interval=interval,
                    start_time=start_date,
                    end_time=end_date,
                    update_existing=True  # å¼ºåˆ¶æ›´æ–°ï¼Œç¡®ä¿ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
                )
                
                if success:
                    download_stats['empty_tables_downloaded'] += 1
                    download_stats['success'].append(symbol)
                else:
                    download_stats['failed'].append(symbol)
            except Exception as e:
                if verbose:
                    print(f"ä¸‹è½½ {symbol} å¤±è´¥: {e}")
                download_stats['failed'].append(symbol)
    
    # ä¸‹è½½ç¼ºå¤±æ—¥æœŸ
    for symbol, details in check_results['details'].items():
        if details['missing_dates']:
            try:
                missing_dates = sorted(details['missing_dates'])
                start_date = datetime.strptime(missing_dates[0], '%Y-%m-%d')
                end_date = datetime.strptime(missing_dates[-1], '%Y-%m-%d')
                
                if verbose:
                    print(f"ä¸‹è½½ {symbol} ç¼ºå¤±æ—¥æœŸçš„æ•°æ® ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})...")
                
                success = download_kline_data(
                    symbol=symbol,
                    interval=interval,
                    start_time=start_date,
                    end_time=end_date,
                    update_existing=True  # å¼ºåˆ¶æ›´æ–°ï¼Œç¡®ä¿ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
                )
                
                if success:
                    download_stats['missing_dates_downloaded'] += 1
                    if symbol not in download_stats['success']:
                        download_stats['success'].append(symbol)
                    # ç­‰å¾…æ•°æ®åº“å†™å…¥å®Œæˆ
                    import time
                    time.sleep(0.5)
                else:
                    if symbol not in download_stats['failed']:
                        download_stats['failed'].append(symbol)
            except Exception as e:
                if verbose:
                    print(f"ä¸‹è½½ {symbol} ç¼ºå¤±æ—¥æœŸå¤±è´¥: {e}")
                if symbol not in download_stats['failed']:
                    download_stats['failed'].append(symbol)
    
    if verbose:
        print("\nä¸‹è½½ç»Ÿè®¡:")
        print(f"ç©ºè¡¨ä¸‹è½½: {download_stats['empty_tables_downloaded']}")
        print(f"ç¼ºå¤±æ—¥æœŸä¸‹è½½: {download_stats['missing_dates_downloaded']}")
        print(f"æˆåŠŸ: {len(download_stats['success'])}")
        print(f"å¤±è´¥: {len(download_stats['failed'])}")
        if download_stats['failed']:
            print(f"å¤±è´¥çš„äº¤æ˜“å¯¹: {', '.join(download_stats['failed'])}")
    
    return download_stats


def recheck_problematic_symbols(
    check_results: Dict,
    interval: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    verbose: bool = True,
    output_file: Optional[str] = None
) -> Dict:
    """
    å¤æ£€æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹ï¼Œå¯¹æ¯”äº¤æ˜“æ‰€APIæ•°æ®å’Œæœ¬åœ°æ•°æ®
    
    Args:
        check_results: check_data_integrityè¿”å›çš„æ£€æŸ¥ç»“æœ
        interval: Kçº¿é—´éš”
        start_date: å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼: YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼: YYYY-MM-DD
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼Œé»˜è®¤True
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæŒ‡å®šåˆ™ç”ŸæˆTXTæŠ¥å‘Šæ–‡ä»¶
    
    Returns:
        Dict: åŒ…å«å¤æ£€ç»“æœçš„å­—å…¸
    """
    import time
    from datetime import datetime as dt
    
    recheck_results = {
        'total_rechecked': 0,
        'exchange_api_issues': [],  # äº¤æ˜“æ‰€APIæ•°æ®æœ‰é—®é¢˜
        'local_data_issues': [],     # æœ¬åœ°æ•°æ®æœ‰é—®é¢˜
        'both_issues': [],           # ä¸¤è¾¹éƒ½æœ‰é—®é¢˜
        'fixed_by_redownload': [],   # é‡æ–°ä¸‹è½½åä¿®å¤
        'details': {}
    }
    
    problematic_symbols = check_results.get('symbols_with_issues', [])
    if not problematic_symbols:
        if verbose:
            print("æ²¡æœ‰éœ€è¦å¤æ£€çš„äº¤æ˜“å¯¹")
        return recheck_results
    
    recheck_results['total_rechecked'] = len(problematic_symbols)
    
    if verbose:
        print(f"\nå¼€å§‹å¤æ£€ {len(problematic_symbols)} ä¸ªæœ‰é—®é¢˜çš„äº¤æ˜“å¯¹...")
        print("=" * 80)
    
    # è½¬æ¢æ—¥æœŸä¸ºdatetimeå¯¹è±¡
    start_dt = None
    end_dt = None
    if start_date:
        start_dt = dt.strptime(start_date, '%Y-%m-%d')
    if end_date:
        end_dt = dt.strptime(end_date, '%Y-%m-%d')
    
    for idx, symbol in enumerate(problematic_symbols, 1):
        # ä»check_resultsçš„detailsä¸­è·å–è¯¥äº¤æ˜“å¯¹çš„è¯¦ç»†ä¿¡æ¯
        symbol_details = check_results.get('details', {}).get(symbol, {})
        issues = symbol_details.get('issues', [])
        
        if verbose:
            print(f"\n[{idx}/{len(problematic_symbols)}] å¤æ£€ {symbol}...")
            if issues:
                print(f"  é—®é¢˜: {', '.join(issues)}")
        
        symbol_detail = {
            'symbol': symbol,
            'issues': issues,
            'local_data': {},
            'exchange_data': {},
            'comparison': {},
            'conclusion': None
        }
        
        try:
            # 1. è·å–æœ¬åœ°æ•°æ®
            local_df = pd.DataFrame()  # åˆå§‹åŒ–ä¸ºç©ºDataFrame
            try:
                local_df = get_local_kline_data(symbol, interval=interval)
            except Exception as e:
                # è¡¨å¯èƒ½ä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯
                if verbose:
                    print(f"  è­¦å‘Š: è·å–æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}")
                symbol_detail['local_data'] = {
                    'record_count': 0,
                    'error': f'è·å–æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}'
                }
            
            if not local_df.empty:
                # ç¡®ä¿æœ‰ trade_date_dt åˆ—ï¼ˆç”¨äºæ—¥æœŸè¿‡æ»¤ï¼‰
                if 'trade_date_dt' not in local_df.columns:
                    if 'trade_date' in local_df.columns:
                        local_df['trade_date_dt'] = pd.to_datetime(local_df['trade_date'])
                    else:
                        # å¦‚æœæ²¡æœ‰ trade_date åˆ—ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®
                        local_df['trade_date_dt'] = pd.NaT
                
                # ç¡®å®šè¦æ£€æŸ¥çš„æ—¥æœŸèŒƒå›´
                if start_dt and end_dt:
                    mask = (local_df['trade_date_dt'] >= start_dt) & (local_df['trade_date_dt'] <= end_dt)
                    local_df_filtered = local_df[mask].copy()
                elif start_dt:
                    mask = local_df['trade_date_dt'] >= start_dt
                    local_df_filtered = local_df[mask].copy()
                elif end_dt:
                    mask = local_df['trade_date_dt'] <= end_dt
                    local_df_filtered = local_df[mask].copy()
                else:
                    local_df_filtered = local_df.copy()
                
                # æ˜¾ç¤ºå®é™…æ€»è®°å½•æ•°ï¼ˆä¸è¿‡æ»¤ï¼‰
                total_record_count = len(local_df)
                
                # è®¡ç®—æ—¥æœŸèŒƒå›´
                date_start = None
                date_end = None
                if 'trade_date_dt' in local_df.columns and not local_df['trade_date_dt'].isna().all():
                    valid_dates = local_df['trade_date_dt'].dropna()
                    if not valid_dates.empty:
                        date_start = valid_dates.min().strftime('%Y-%m-%d')
                        date_end = valid_dates.max().strftime('%Y-%m-%d')
                
                symbol_detail['local_data'] = {
                    'record_count': total_record_count,  # æ˜¾ç¤ºæ€»è®°å½•æ•°
                    'date_range': {
                        'start': date_start,
                        'end': date_end
                    },
                    'duplicates': local_df.duplicated(subset=['trade_date']).sum() if 'trade_date' in local_df.columns else 0,
                    'null_counts': {
                        'open': local_df['open'].isna().sum() if 'open' in local_df.columns else 0,
                        'high': local_df['high'].isna().sum() if 'high' in local_df.columns else 0,
                        'low': local_df['low'].isna().sum() if 'low' in local_df.columns else 0,
                        'close': local_df['close'].isna().sum() if 'close' in local_df.columns else 0,
                        'volume': local_df['volume'].isna().sum() if 'volume' in local_df.columns else 0
                    },
                    'invalid_prices': (((local_df['open'] <= 0) if 'open' in local_df.columns else pd.Series([False])) | 
                                      ((local_df['high'] <= 0) if 'high' in local_df.columns else pd.Series([False])) | 
                                      ((local_df['low'] <= 0) if 'low' in local_df.columns else pd.Series([False])) | 
                                      ((local_df['close'] <= 0) if 'close' in local_df.columns else pd.Series([False]))).sum(),
                    'invalid_volumes': (local_df['volume'] < 0).sum() if 'volume' in local_df.columns else 0
                }
            else:
                symbol_detail['local_data'] = {
                    'record_count': 0,
                    'error': 'æœ¬åœ°æ•°æ®ä¸ºç©º'
                }
            
            # 2. ä»äº¤æ˜“æ‰€APIè·å–æ•°æ®
            time.sleep(0.2)  # é¿å…APIé™æµ
            
            # è®¡ç®—æ—¶é—´æˆ³å’Œæ—¥æœŸèŒƒå›´
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
            if start_dt:
                actual_start_dt = start_dt
            elif not local_df.empty and 'trade_date_dt' in local_df.columns:
                # ä½¿ç”¨æœ¬åœ°æ•°æ®çš„æœ€æ—©æ—¥æœŸ
                valid_dates = local_df['trade_date_dt'].dropna()
                if not valid_dates.empty:
                    actual_start_dt = valid_dates.min().to_pydatetime()
                else:
                    # é»˜è®¤ä»2020å¹´å¼€å§‹
                    actual_start_dt = dt(2020, 1, 1)
            else:
                # é»˜è®¤ä»2020å¹´å¼€å§‹
                actual_start_dt = dt(2020, 1, 1)
            
            if end_dt:
                actual_end_dt = end_dt
            elif not local_df.empty and 'trade_date_dt' in local_df.columns:
                # ä½¿ç”¨æœ¬åœ°æ•°æ®çš„æœ€æ™šæ—¥æœŸï¼Œæˆ–è€…å½“å‰æ—¶é—´
                valid_dates = local_df['trade_date_dt'].dropna()
                if not valid_dates.empty:
                    # ä½¿ç”¨æœ¬åœ°æ•°æ®æœ€æ™šæ—¥æœŸåŠ 1å¤©ï¼Œç¡®ä¿èƒ½è·å–åˆ°æœ€æ–°æ•°æ®
                    actual_end_dt = valid_dates.max().to_pydatetime() + timedelta(days=1)
                else:
                    actual_end_dt = dt.now()
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                actual_end_dt = dt.now()
            
            # è®¡ç®—æ—¶é—´æˆ³
            start_timestamp = int(actual_start_dt.timestamp() * 1000)
            end_timestamp = int(actual_end_dt.timestamp() * 1000)
            
            # è®¡ç®—æ•°æ®æ¡æ•°ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ®µä¸‹è½½
            def calculate_interval_seconds(interval: str) -> int:
                """è®¡ç®—Kçº¿é—´éš”å¯¹åº”çš„ç§’æ•°"""
                interval_map = {
                    '1m': 60, '3m': 180, '5m': 300, '15m': 900, '30m': 1800,
                    '1h': 3600, '2h': 7200, '4h': 14400, '6h': 21600, '8h': 28800,
                    '12h': 43200, '1d': 86400, '3d': 259200, '1w': 604800, '1M': 2592000
                }
                return interval_map.get(interval, 86400)
            
            def calculate_data_count(start_time: datetime, end_time: datetime, interval: str) -> int:
                """è®¡ç®—æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®æ¡æ•°"""
                if not start_time or not end_time:
                    return 0
                interval_seconds = calculate_interval_seconds(interval)
                total_seconds = int((end_time - start_time).total_seconds())
                count = total_seconds // interval_seconds + 1
                return count
            
            def split_time_range(start_time: datetime, end_time: datetime, interval: str, max_count: int = 1500) -> List[tuple]:
                """å°†æ—¶é—´èŒƒå›´åˆ†å‰²æˆå¤šä¸ªæ®µï¼Œæ¯æ®µä¸è¶…è¿‡max_countæ¡æ•°æ®"""
                if not start_time or not end_time:
                    return []
                interval_seconds = calculate_interval_seconds(interval)
                max_seconds = (max_count - 1) * interval_seconds
                ranges = []
                current_start = start_time
                while current_start < end_time:
                    current_end = min(current_start + timedelta(seconds=max_seconds), end_time)
                    ranges.append((current_start, current_end))
                    current_start = current_end + timedelta(seconds=interval_seconds)
                return ranges
            
            # è®¡ç®—æ•°æ®æ¡æ•°
            data_count = calculate_data_count(actual_start_dt, actual_end_dt, interval)
            
            # è·å–äº¤æ˜“æ‰€æ•°æ®ï¼ˆåˆ†æ®µè·å–ï¼‰
            try:
                exchange_df = pd.DataFrame()
                
                if data_count > 1500:
                    # éœ€è¦åˆ†æ®µä¸‹è½½
                    if verbose:
                        print(f"  æ•°æ®æ¡æ•° {data_count} è¶…è¿‡1500æ¡ï¼Œå°†åˆ†æ®µä¸‹è½½...")
                    
                    time_ranges = split_time_range(actual_start_dt, actual_end_dt, interval, max_count=1500)
                    
                    for idx, (seg_start, seg_end) in enumerate(time_ranges, 1):
                        seg_start_ts = int(seg_start.timestamp() * 1000)
                        seg_end_ts = int(seg_end.timestamp() * 1000)
                        
                        if verbose:
                            print(f"  ä¸‹è½½ç¬¬ {idx}/{len(time_ranges)} æ®µ: {seg_start.strftime('%Y-%m-%d')} è‡³ {seg_end.strftime('%Y-%m-%d')}")
                        
                        time.sleep(0.2)  # é¿å…APIé™æµ
                        
                        api_data = kline_candlestick_data(
                            symbol=symbol,
                            interval=interval,
                            starttime=seg_start_ts,
                            endtime=seg_end_ts,
                            limit=1500
                        )
                        
                        if api_data:
                            seg_df = kline2df(api_data)
                            if not seg_df.empty:
                                if exchange_df.empty:
                                    exchange_df = seg_df
                                else:
                                    # åˆå¹¶æ•°æ®ï¼Œå»é‡
                                    exchange_df = pd.concat([exchange_df, seg_df], ignore_index=True)
                                    exchange_df = exchange_df.drop_duplicates(subset=['trade_date'], keep='first')
                                    exchange_df = exchange_df.sort_values('trade_date').reset_index(drop=True)
                else:
                    # å•æ¬¡ä¸‹è½½å³å¯
                    api_data = kline_candlestick_data(
                        symbol=symbol,
                        interval=interval,
                        starttime=start_timestamp,
                        endtime=end_timestamp,
                        limit=1500
                    )
                    
                    if api_data:
                        exchange_df = kline2df(api_data)
                
                if not exchange_df.empty:
                    # è½¬æ¢trade_dateä¸ºdatetime
                    if 'trade_date' in exchange_df.columns:
                        exchange_df['trade_date_dt'] = pd.to_datetime(exchange_df['trade_date'])
                    else:
                        exchange_df['trade_date_dt'] = pd.NaT
                    
                    # æ˜¾ç¤ºå®é™…æ€»è®°å½•æ•°ï¼ˆä¸è¿‡æ»¤ï¼‰
                    total_record_count = len(exchange_df)
                    
                    # è®¡ç®—æ—¥æœŸèŒƒå›´
                    date_start = None
                    date_end = None
                    if 'trade_date_dt' in exchange_df.columns and not exchange_df['trade_date_dt'].isna().all():
                        valid_dates = exchange_df['trade_date_dt'].dropna()
                        if not valid_dates.empty:
                            date_start = valid_dates.min().strftime('%Y-%m-%d')
                            date_end = valid_dates.max().strftime('%Y-%m-%d')
                    
                    symbol_detail['exchange_data'] = {
                        'record_count': total_record_count,  # æ˜¾ç¤ºæ€»è®°å½•æ•°
                        'date_range': {
                            'start': date_start,
                            'end': date_end
                        },
                        'duplicates': exchange_df.duplicated(subset=['trade_date']).sum() if 'trade_date' in exchange_df.columns else 0,
                        'null_counts': {
                            'open': exchange_df['open'].isna().sum() if 'open' in exchange_df.columns else 0,
                            'high': exchange_df['high'].isna().sum() if 'high' in exchange_df.columns else 0,
                            'low': exchange_df['low'].isna().sum() if 'low' in exchange_df.columns else 0,
                            'close': exchange_df['close'].isna().sum() if 'close' in exchange_df.columns else 0,
                            'volume': exchange_df['volume'].isna().sum() if 'volume' in exchange_df.columns else 0
                        },
                        'invalid_prices': (((exchange_df['open'] <= 0) if 'open' in exchange_df.columns else pd.Series([False])) | 
                                          ((exchange_df['high'] <= 0) if 'high' in exchange_df.columns else pd.Series([False])) | 
                                          ((exchange_df['low'] <= 0) if 'low' in exchange_df.columns else pd.Series([False])) | 
                                          ((exchange_df['close'] <= 0) if 'close' in exchange_df.columns else pd.Series([False]))).sum(),
                        'invalid_volumes': (exchange_df['volume'] < 0).sum() if 'volume' in exchange_df.columns else 0
                    }
                    
                    # 3. å¯¹æ¯”åˆ†æ
                    comparison = {}
                    
                    # å¯¹æ¯”è®°å½•æ•°
                    local_count = symbol_detail['local_data'].get('record_count', 0)
                    exchange_count = symbol_detail['exchange_data'].get('record_count', 0)
                    comparison['record_count_diff'] = local_count - exchange_count
                    
                    # å¯¹æ¯”é‡å¤æ•°æ®
                    local_duplicates = symbol_detail['local_data'].get('duplicates', 0)
                    exchange_duplicates = symbol_detail['exchange_data'].get('duplicates', 0)
                    comparison['duplicates_diff'] = local_duplicates - exchange_duplicates
                    
                    # å¯¹æ¯”ç©ºå€¼
                    local_nulls = sum(symbol_detail['local_data'].get('null_counts', {}).values())
                    exchange_nulls = sum(symbol_detail['exchange_data'].get('null_counts', {}).values())
                    comparison['nulls_diff'] = local_nulls - exchange_nulls
                    
                    # å¯¹æ¯”æ— æ•ˆä»·æ ¼
                    local_invalid_prices = symbol_detail['local_data'].get('invalid_prices', 0)
                    exchange_invalid_prices = symbol_detail['exchange_data'].get('invalid_prices', 0)
                    comparison['invalid_prices_diff'] = local_invalid_prices - exchange_invalid_prices
                    
                    # å¯¹æ¯”æ— æ•ˆæˆäº¤é‡
                    local_invalid_volumes = symbol_detail['local_data'].get('invalid_volumes', 0)
                    exchange_invalid_volumes = symbol_detail['exchange_data'].get('invalid_volumes', 0)
                    comparison['invalid_volumes_diff'] = local_invalid_volumes - exchange_invalid_volumes
                    
                    symbol_detail['comparison'] = comparison
                    
                    # 4. å¾—å‡ºç»“è®º
                    conclusion_parts = []
                    
                    # å¦‚æœäº¤æ˜“æ‰€æ•°æ®ä¹Ÿæœ‰é—®é¢˜
                    if exchange_duplicates > 0 or exchange_nulls > 0 or exchange_invalid_prices > 0 or exchange_invalid_volumes > 0:
                        conclusion_parts.append("äº¤æ˜“æ‰€APIæ•°æ®å­˜åœ¨é—®é¢˜")
                        recheck_results['exchange_api_issues'].append(symbol)
                    
                    # å¦‚æœæœ¬åœ°æ•°æ®é—®é¢˜æ›´ä¸¥é‡
                    if (local_duplicates > exchange_duplicates or 
                        local_nulls > exchange_nulls or 
                        local_invalid_prices > exchange_invalid_prices or 
                        local_invalid_volumes > exchange_invalid_volumes):
                        conclusion_parts.append("æœ¬åœ°æ•°æ®é—®é¢˜æ›´ä¸¥é‡")
                        recheck_results['local_data_issues'].append(symbol)
                    
                    # å¦‚æœä¸¤è¾¹éƒ½æœ‰é—®é¢˜
                    if (exchange_duplicates > 0 or exchange_nulls > 0 or exchange_invalid_prices > 0 or exchange_invalid_volumes > 0) and \
                       (local_duplicates > 0 or local_nulls > 0 or local_invalid_prices > 0 or local_invalid_volumes > 0):
                        recheck_results['both_issues'].append(symbol)
                    
                    # å¦‚æœæœ¬åœ°æ•°æ®é—®é¢˜å¯ä»¥é€šè¿‡é‡æ–°ä¸‹è½½ä¿®å¤
                    if (local_duplicates > exchange_duplicates or 
                        local_nulls > exchange_nulls or 
                        local_invalid_prices > exchange_invalid_prices or 
                        local_invalid_volumes > exchange_invalid_volumes) and \
                       (exchange_duplicates == 0 and exchange_nulls == 0 and exchange_invalid_prices == 0 and exchange_invalid_volumes == 0):
                        conclusion_parts.append("å»ºè®®é‡æ–°ä¸‹è½½ä¿®å¤")
                        recheck_results['fixed_by_redownload'].append(symbol)
                    
                    if conclusion_parts:
                        symbol_detail['conclusion'] = " | ".join(conclusion_parts)
                    else:
                        symbol_detail['conclusion'] = "æ•°æ®æ­£å¸¸"
                    
                    if verbose:
                        print(f"  æœ¬åœ°è®°å½•æ•°: {local_count}, äº¤æ˜“æ‰€è®°å½•æ•°: {exchange_count}")
                        print(f"  æœ¬åœ°é‡å¤: {local_duplicates}, äº¤æ˜“æ‰€é‡å¤: {exchange_duplicates}")
                        print(f"  æœ¬åœ°ç©ºå€¼: {local_nulls}, äº¤æ˜“æ‰€ç©ºå€¼: {exchange_nulls}")
                        print(f"  ç»“è®º: {symbol_detail['conclusion']}")
                    else:
                        symbol_detail['exchange_data'] = {
                            'record_count': 0,
                            'error': 'äº¤æ˜“æ‰€APIè¿”å›ç©ºæ•°æ®'
                        }
                        symbol_detail['conclusion'] = "äº¤æ˜“æ‰€APIè¿”å›ç©ºæ•°æ®"
                        recheck_results['exchange_api_issues'].append(symbol)
                        
                        if verbose:
                            print(f"  è­¦å‘Š: äº¤æ˜“æ‰€APIè¿”å›ç©ºæ•°æ®")
                else:
                    symbol_detail['exchange_data'] = {
                        'record_count': 0,
                        'error': 'äº¤æ˜“æ‰€APIè¿”å›None'
                    }
                    symbol_detail['conclusion'] = "äº¤æ˜“æ‰€APIè¿”å›None"
                    recheck_results['exchange_api_issues'].append(symbol)
                    
                    if verbose:
                        print(f"  è­¦å‘Š: äº¤æ˜“æ‰€APIè¿”å›None")
                        
            except Exception as e:
                symbol_detail['exchange_data'] = {
                    'error': f'è·å–äº¤æ˜“æ‰€æ•°æ®å¤±è´¥: {str(e)}'
                }
                symbol_detail['conclusion'] = f"è·å–äº¤æ˜“æ‰€æ•°æ®å¤±è´¥: {str(e)}"
                recheck_results['exchange_api_issues'].append(symbol)
                
                if verbose:
                    print(f"  é”™è¯¯: è·å–äº¤æ˜“æ‰€æ•°æ®å¤±è´¥: {str(e)}")
                    
        except Exception as e:
            symbol_detail['error'] = f'å¤æ£€è¿‡ç¨‹å‡ºé”™: {str(e)}'
            if verbose:
                print(f"  é”™è¯¯: {str(e)}")
        
        recheck_results['details'][symbol] = symbol_detail
    
    if verbose:
        print("\n" + "=" * 80)
        print("å¤æ£€æ€»ç»“:")
        print(f"  æ€»å¤æ£€æ•°: {recheck_results['total_rechecked']}")
        print(f"  äº¤æ˜“æ‰€APIé—®é¢˜: {len(recheck_results['exchange_api_issues'])}")
        print(f"  æœ¬åœ°æ•°æ®é—®é¢˜: {len(recheck_results['local_data_issues'])}")
        print(f"  ä¸¤è¾¹éƒ½æœ‰é—®é¢˜: {len(recheck_results['both_issues'])}")
        print(f"  å¯é€šè¿‡é‡æ–°ä¸‹è½½ä¿®å¤: {len(recheck_results['fixed_by_redownload'])}")
    
    # ç”ŸæˆTXTæŠ¥å‘Šæ–‡ä»¶
    if output_file:
        try:
            report_lines = []
            report_lines.append("=" * 80)
            report_lines.append("æ•°æ®å¤æ£€æŠ¥å‘Š")
            report_lines.append("=" * 80)
            report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append(f"Kçº¿é—´éš”: {interval}")
            if start_date:
                report_lines.append(f"å¼€å§‹æ—¥æœŸ: {start_date}")
            if end_date:
                report_lines.append(f"ç»“æŸæ—¥æœŸ: {end_date}")
            report_lines.append("")
            
            # æ€»ç»“ç»Ÿè®¡
            report_lines.append("-" * 80)
            report_lines.append("å¤æ£€æ€»ç»“")
            report_lines.append("-" * 80)
            report_lines.append(f"æ€»å¤æ£€æ•°: {recheck_results['total_rechecked']}")
            report_lines.append(f"äº¤æ˜“æ‰€APIé—®é¢˜: {len(recheck_results['exchange_api_issues'])}")
            report_lines.append(f"æœ¬åœ°æ•°æ®é—®é¢˜: {len(recheck_results['local_data_issues'])}")
            report_lines.append(f"ä¸¤è¾¹éƒ½æœ‰é—®é¢˜: {len(recheck_results['both_issues'])}")
            report_lines.append(f"å¯é€šè¿‡é‡æ–°ä¸‹è½½ä¿®å¤: {len(recheck_results['fixed_by_redownload'])}")
            report_lines.append("")
            
            # é—®é¢˜åˆ†ç±»
            if recheck_results['exchange_api_issues']:
                report_lines.append("-" * 80)
                report_lines.append("äº¤æ˜“æ‰€APIé—®é¢˜äº¤æ˜“å¯¹")
                report_lines.append("-" * 80)
                for symbol in recheck_results['exchange_api_issues']:
                    report_lines.append(f"  - {symbol}")
                report_lines.append("")
            
            if recheck_results['local_data_issues']:
                report_lines.append("-" * 80)
                report_lines.append("æœ¬åœ°æ•°æ®é—®é¢˜äº¤æ˜“å¯¹")
                report_lines.append("-" * 80)
                for symbol in recheck_results['local_data_issues']:
                    report_lines.append(f"  - {symbol}")
                report_lines.append("")
            
            if recheck_results['both_issues']:
                report_lines.append("-" * 80)
                report_lines.append("ä¸¤è¾¹éƒ½æœ‰é—®é¢˜çš„äº¤æ˜“å¯¹")
                report_lines.append("-" * 80)
                for symbol in recheck_results['both_issues']:
                    report_lines.append(f"  - {symbol}")
                report_lines.append("")
            
            if recheck_results['fixed_by_redownload']:
                report_lines.append("-" * 80)
                report_lines.append("å¯é€šè¿‡é‡æ–°ä¸‹è½½ä¿®å¤çš„äº¤æ˜“å¯¹")
                report_lines.append("-" * 80)
                for symbol in recheck_results['fixed_by_redownload']:
                    report_lines.append(f"  - {symbol}")
                report_lines.append("")
            
            # è¯¦ç»†å¯¹æ¯”ä¿¡æ¯
            report_lines.append("=" * 80)
            report_lines.append("è¯¦ç»†å¯¹æ¯”ä¿¡æ¯")
            report_lines.append("=" * 80)
            
            for symbol, detail in recheck_results['details'].items():
                report_lines.append("")
                report_lines.append(f"äº¤æ˜“å¯¹: {symbol}")
                report_lines.append("-" * 80)
                
                if detail.get('issues'):
                    report_lines.append(f"åŸå§‹é—®é¢˜: {', '.join(detail['issues'])}")
                
                # æœ¬åœ°æ•°æ®ä¿¡æ¯
                local_data = detail.get('local_data', {})
                report_lines.append("\næœ¬åœ°æ•°æ®:")
                if 'error' in local_data:
                    report_lines.append(f"  é”™è¯¯: {local_data['error']}")
                else:
                    report_lines.append(f"  è®°å½•æ•°: {local_data.get('record_count', 0)}")
                    if local_data.get('date_range', {}).get('start'):
                        report_lines.append(f"  æ—¥æœŸèŒƒå›´: {local_data['date_range']['start']} è‡³ {local_data['date_range']['end']}")
                    report_lines.append(f"  é‡å¤æ•°æ®: {local_data.get('duplicates', 0)}")
                    null_counts = local_data.get('null_counts', {})
                    total_nulls = sum(null_counts.values())
                    report_lines.append(f"  ç©ºå€¼æ€»æ•°: {total_nulls}")
                    if total_nulls > 0:
                        report_lines.append(f"    - open: {null_counts.get('open', 0)}")
                        report_lines.append(f"    - high: {null_counts.get('high', 0)}")
                        report_lines.append(f"    - low: {null_counts.get('low', 0)}")
                        report_lines.append(f"    - close: {null_counts.get('close', 0)}")
                        report_lines.append(f"    - volume: {null_counts.get('volume', 0)}")
                    report_lines.append(f"  æ— æ•ˆä»·æ ¼: {local_data.get('invalid_prices', 0)}")
                    report_lines.append(f"  æ— æ•ˆæˆäº¤é‡: {local_data.get('invalid_volumes', 0)}")
                
                # äº¤æ˜“æ‰€æ•°æ®ä¿¡æ¯
                exchange_data = detail.get('exchange_data', {})
                report_lines.append("\näº¤æ˜“æ‰€æ•°æ®:")
                if 'error' in exchange_data:
                    report_lines.append(f"  é”™è¯¯: {exchange_data['error']}")
                else:
                    report_lines.append(f"  è®°å½•æ•°: {exchange_data.get('record_count', 0)}")
                    if exchange_data.get('date_range', {}).get('start'):
                        report_lines.append(f"  æ—¥æœŸèŒƒå›´: {exchange_data['date_range']['start']} è‡³ {exchange_data['date_range']['end']}")
                    report_lines.append(f"  é‡å¤æ•°æ®: {exchange_data.get('duplicates', 0)}")
                    null_counts = exchange_data.get('null_counts', {})
                    total_nulls = sum(null_counts.values())
                    report_lines.append(f"  ç©ºå€¼æ€»æ•°: {total_nulls}")
                    if total_nulls > 0:
                        report_lines.append(f"    - open: {null_counts.get('open', 0)}")
                        report_lines.append(f"    - high: {null_counts.get('high', 0)}")
                        report_lines.append(f"    - low: {null_counts.get('low', 0)}")
                        report_lines.append(f"    - close: {null_counts.get('close', 0)}")
                        report_lines.append(f"    - volume: {null_counts.get('volume', 0)}")
                    report_lines.append(f"  æ— æ•ˆä»·æ ¼: {exchange_data.get('invalid_prices', 0)}")
                    report_lines.append(f"  æ— æ•ˆæˆäº¤é‡: {exchange_data.get('invalid_volumes', 0)}")
                
                # å¯¹æ¯”ä¿¡æ¯
                comparison = detail.get('comparison', {})
                if comparison:
                    report_lines.append("\nå¯¹æ¯”åˆ†æ:")
                    report_lines.append(f"  è®°å½•æ•°å·®å¼‚: {comparison.get('record_count_diff', 0)} (æœ¬åœ° - äº¤æ˜“æ‰€)")
                    report_lines.append(f"  é‡å¤æ•°æ®å·®å¼‚: {comparison.get('duplicates_diff', 0)}")
                    report_lines.append(f"  ç©ºå€¼å·®å¼‚: {comparison.get('nulls_diff', 0)}")
                    report_lines.append(f"  æ— æ•ˆä»·æ ¼å·®å¼‚: {comparison.get('invalid_prices_diff', 0)}")
                    report_lines.append(f"  æ— æ•ˆæˆäº¤é‡å·®å¼‚: {comparison.get('invalid_volumes_diff', 0)}")
                
                # ç»“è®º
                if detail.get('conclusion'):
                    report_lines.append(f"\nç»“è®º: {detail['conclusion']}")
            
            # å†™å…¥æ–‡ä»¶
            report_content = "\n".join(report_lines)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            if verbose:
                print(f"\nå¤æ£€æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            if verbose:
                print(f"\nè­¦å‘Š: ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # è½¬æ¢æ‰€æœ‰ numpy/pandas ç±»å‹ä¸º Python åŸç”Ÿç±»å‹ï¼Œä»¥ä¾¿ JSON åºåˆ—åŒ–
    def convert_to_python_types(obj):
        """é€’å½’è½¬æ¢ numpy/pandas ç±»å‹ä¸º Python åŸç”Ÿç±»å‹"""
        try:
            import numpy as np
        except ImportError:
            np = None
        
        if isinstance(obj, dict):
            return {k: convert_to_python_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_python_types(item) for item in obj]
        elif np is not None:
            if isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
        elif pd.isna(obj):
            return None
        elif hasattr(obj, 'item'):  # numpy scalar
            return obj.item()
        else:
            return obj
    
    return convert_to_python_types(recheck_results)


if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹:
    python data.py --symbol BTCUSDT --interval 1d
    python data.py --interval 1d --start-date 2021-01-01 --end-date 2025-12-31
    python data.py --interval 1h --check-duplicates --check-missing-dates --check-data-quality
    python data.py --interval 1h --auto-download  # æ£€æŸ¥å¹¶è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ•°æ®
    python data.py --interval 1d --generate-report report.html --report-format html  # ç”ŸæˆHTMLæŠ¥å‘Š
    python data.py --interval 1h --generate-script download.sh  # ç”Ÿæˆä¸‹è½½è„šæœ¬
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='æ£€æŸ¥Kçº¿æ•°æ®å®Œæ•´æ€§')
    parser.add_argument(
        '--symbol',
        type=str,
        default=None,
        help='äº¤æ˜“å¯¹ç¬¦å·ï¼ˆå¦‚BTCUSDTï¼‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æ£€æŸ¥æ‰€æœ‰äº¤æ˜“å¯¹'
    )
    parser.add_argument(
        '--interval',
        type=str,
        default='1d',
        help='Kçº¿é—´éš”ï¼ˆé»˜è®¤: 1dï¼‰'
    )
    parser.add_argument(
        '--start-date',
        type=str,
        default=None,
        help='å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼: YYYY-MM-DDï¼‰'
    )
    parser.add_argument(
        '--end-date',
        type=str,
        default=None,
        help='ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼: YYYY-MM-DDï¼‰'
    )
    parser.add_argument(
        '--check-duplicates',
        action='store_true',
        help='æ£€æŸ¥é‡å¤æ•°æ®'
    )
    parser.add_argument(
        '--check-missing-dates',
        action='store_true',
        help='æ£€æŸ¥ç¼ºå¤±æ—¥æœŸ'
    )
    parser.add_argument(
        '--check-data-quality',
        action='store_true',
        help='æ£€æŸ¥æ•°æ®è´¨é‡'
    )
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºæ€»ç»“'
    )
    parser.add_argument(
        '--auto-download',
        action='store_true',
        dest='auto_download',
        help='è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ•°æ®'
    )
    parser.add_argument(
        '--generate-script',
        type=str,
        default=None,
        dest='generate_script',
        help='ç”Ÿæˆä¸‹è½½è„šæœ¬å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶'
    )
    parser.add_argument(
        '--generate-report',
        type=str,
        default=None,
        dest='generate_report',
        help='ç”Ÿæˆå®Œæ•´æ€§æŠ¥å‘Šå¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶'
    )
    parser.add_argument(
        '--report-format',
        type=str,
        default='text',
        choices=['text', 'json', 'html', 'markdown'],
        dest='report_format',
        help='æŠ¥å‘Šæ ¼å¼ï¼ˆé»˜è®¤: textï¼‰'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ£€æŸ¥é¡¹ï¼Œé»˜è®¤å…¨éƒ¨æ£€æŸ¥
    check_duplicates = args.check_duplicates if (args.check_duplicates or args.check_missing_dates or args.check_data_quality) else True
    check_missing_dates = args.check_missing_dates if (args.check_missing_dates or args.check_data_quality) else True
    check_data_quality = args.check_data_quality if args.check_data_quality else True
    
    results = check_data_integrity(
        symbol=args.symbol,
        interval=args.interval,
        start_date=args.start_date,
        end_date=args.end_date,
        check_duplicates=check_duplicates,
        check_missing_dates=check_missing_dates,
        check_data_quality=check_data_quality,
        verbose=not args.quiet
    )
    
    # ç”Ÿæˆä¸‹è½½è„šæœ¬
    if args.generate_script:
        script_content = generate_download_script_from_check(
            check_results=results,
            interval=args.interval,
            output_file=args.generate_script,
            auto_execute=False
        )
        if not args.quiet:
            print(f"\nä¸‹è½½è„šæœ¬å·²ç”Ÿæˆ: {args.generate_script}")
    
    # è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ•°æ®
    if args.auto_download:
        download_stats = download_missing_data_from_check(
            check_results=results,
            interval=args.interval,
            verbose=not args.quiet
        )
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.generate_report:
        # ä»æ–‡ä»¶æ‰©å±•åæ¨æ–­æ ¼å¼
        report_file = args.generate_report
        if report_file.endswith('.html'):
            report_format = 'html'
        elif report_file.endswith('.json'):
            report_format = 'json'
        elif report_file.endswith('.md'):
            report_format = 'markdown'
        else:
            report_format = args.report_format
        
        report_content = generate_integrity_report(
            check_results=results,
            interval=args.interval,
            start_date=args.start_date,
            end_date=args.end_date,
            check_duplicates=check_duplicates,
            check_missing_dates=check_missing_dates,
            check_data_quality=check_data_quality,
            output_format=report_format,
            output_file=report_file
        )
        if not args.quiet:
            print(f"\næŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    # å¦‚æœæœ‰é—®é¢˜ï¼Œè¿”å›éé›¶é€€å‡ºç 
    if results['symbols_with_issues'] or results['summary']['empty_tables'] > 0:
        exit(1)
    else:
        exit(0)