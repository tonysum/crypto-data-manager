"""
å¸å®‰Uæœ¬ä½åˆçº¦Kçº¿æ•°æ®ä¸‹è½½è„šæœ¬

åŠŸèƒ½ï¼š
1. è·å–æ‰€æœ‰USDTäº¤æ˜“å¯¹
2. ğŸ”§ äº¤æ˜“å¯¹æ ¡éªŒï¼šä¸‹è½½å‰è‡ªåŠ¨æ ¡éªŒäº¤æ˜“å¯¹æ˜¯å¦åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“ï¼Œè·³è¿‡å·²ä¸‹æ¶æˆ–æš‚åœçš„äº¤æ˜“å¯¹
3. ä¸‹è½½æ¯ä¸ªäº¤æ˜“å¯¹çš„Kçº¿æ•°æ®
4. ä¿å­˜åˆ°æœ¬åœ°PostgreSQLæ•°æ®åº“ï¼Œè¡¨åæ ¼å¼ï¼šK{interval}{symbol}ï¼ˆä¾‹å¦‚ï¼šK1dBTCUSDT, K1hETHUSDTï¼‰
5. æ”¯æŒå¢é‡æ›´æ–°(é¿å…é‡å¤ä¸‹è½½)
   - æ—¥çº¿åŠä»¥ä¸Šï¼šæŒ‰æ—¥æœŸå»é‡ï¼Œä¸æ›´æ–°æœ€åä¸€å¤©
   - å°æ—¶çº¿åŠä»¥ä¸‹ï¼šæŒ‰æ—¶é—´ç‚¹å»é‡ï¼Œä¸æ›´æ–°æœ€åä¸€æ¡
6. æ™ºèƒ½è·³è¿‡ï¼šä¸‹è½½å‰æ£€æŸ¥æœ¬åœ°æ•°æ®æœ€åæ—¶é—´ï¼Œå¦‚æœ >= end_timeåˆ™è·³è¿‡è¯¥äº¤æ˜“å¯¹ï¼ˆé™¤éä½¿ç”¨--updateï¼‰
7. æ”¯æŒæŒ‡å®šå¼€å§‹å’Œç»“æŸæ—¶é—´ï¼Œç¡®ä¿ä¸åŒæ—¶é—´é—´éš”çš„æ•°æ®æ—¶é—´èŒƒå›´ä¸€è‡´
8. é»˜è®¤ä¸ä¸‹è½½å½“å¤©æ•°æ®ï¼ˆå› ä¸ºå½“å¤©æ•°æ®ä¸å®Œæ•´ï¼‰
9. è‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼šå½“æ•°æ®æ¡æ•°è¶…è¿‡1500æ¡æ—¶ï¼Œè‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼Œæ¯æ®µæœ€å¤š1500æ¡
10. è¯·æ±‚é¢‘ç‡æ§åˆ¶ï¼šæ¯æ¬¡APIè¯·æ±‚ä¹‹é—´è‡ªåŠ¨å»¶è¿Ÿï¼Œé¿å…è§¦å‘APIé¢‘ç‡é™åˆ¶
    - æ¯æ¬¡è¯·æ±‚å»¶è¿Ÿï¼šé»˜è®¤0.1ç§’ï¼ˆå¯é€šè¿‡--request-delayè°ƒæ•´ï¼‰
    - æ‰¹æ¬¡æš‚åœï¼šæ¯å¤„ç†æŒ‡å®šæ•°é‡çš„äº¤æ˜“å¯¹åæš‚åœï¼ˆé»˜è®¤30ä¸ªåæš‚åœ3ç§’ï¼‰

ä½¿ç”¨æ–¹æ³•ä¸¾ä¾‹ï¼š

1. ä¸‹è½½æ‰€æœ‰äº¤æ˜“å¯¹çš„æ—¥çº¿æ•°æ®ï¼ˆé»˜è®¤ï¼‰ï¼š
   python download_klines.py

2. ä¸‹è½½æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ—¥çº¿æ•°æ®ï¼š
   python download_klines.py --interval 1d --start-time 2025-01-01 --end-time 2025-12-31

3. ä¸‹è½½1å°æ—¶Kçº¿æ•°æ®ï¼ŒæŒ‡å®šæ—¶é—´èŒƒå›´ï¼š
   python download_klines.py --interval 1h --start-time 2025-01-01 --end-time 2025-12-31

4. ä¸‹è½½4å°æ—¶Kçº¿æ•°æ®ï¼ŒæŒ‡å®šæ—¶é—´èŒƒå›´ï¼ˆè‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼‰ï¼š
   python download_klines.py --interval 4h --start-time 2022-01-01 --end-time 2025-12-31

5. ä¸‹è½½5åˆ†é’ŸKçº¿æ•°æ®ï¼ŒæŒ‡å®šæ—¶é—´èŒƒå›´ï¼š
   python download_klines.py --interval 5m --start-time 2025-01-01 --end-time 2025-12-31

6. ä¸‹è½½æŒ‡å®šäº¤æ˜“å¯¹çš„æ•°æ®ï¼š
   python download_klines.py --interval 1d --start-time 2025-01-01 --end-time 2025-12-31 --symbols BTCUSDT ETHUSDT

7. ä¸‹è½½æœ€è¿‘30å¤©çš„æ•°æ®ï¼š
   python download_klines.py --interval 1d --days 30

8. åªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹ï¼š
   python download_klines.py --interval 1d --missing-only

9. æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®ï¼š
   python download_klines.py --interval 1d --update

10. ä½¿ç”¨ç²¾ç¡®æ—¶é—´ï¼ˆåŒ…å«æ—¶åˆ†ç§’ï¼‰ï¼Œè‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼š
    python download_klines.py --interval 1h --start-time "2025-01-01 00:00:00" --end-time "2025-12-31 23:59:59"

11. è‡ªå®šä¹‰è¯·æ±‚å»¶è¿Ÿå’Œæ‰¹æ¬¡è®¾ç½®ï¼š
    python download_klines.py --interval 1h --start-time 2024-01-01 --end-time 2025-12-31 --request-delay 0.2 --batch-size 20 --batch-delay 5.0

12. ç¦ç”¨è‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼ˆä½¿ç”¨åŸæœ‰å•æ¬¡ä¸‹è½½é€»è¾‘ï¼‰ï¼š
    python download_klines.py --interval 4h --start-time 2022-01-01 --end-time 2025-12-31 --no-auto-split

å‘½ä»¤è¡Œå‚æ•°ï¼š
  --interval: Kçº¿é—´éš” (1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w, 1M)
  --start-time: å¼€å§‹æ—¶é—´ (YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS)
  --end-time: ç»“æŸæ—¶é—´ (YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS)
  --days: å›æº¯å¤©æ•°ï¼ˆå¦‚æœæä¾›äº†--start-timeå’Œ--end-timeåˆ™å¿½ç•¥æ­¤å‚æ•°ï¼‰
  --limit: æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§æ¡æ•°ï¼ˆé»˜è®¤Noneï¼Œè‡ªåŠ¨ä½¿ç”¨1500ã€‚å¦‚æœåªæä¾›start-timeå’Œend-timeä¼šè‡ªåŠ¨è®¡ç®—ï¼‰
  --auto-split: å½“æ•°æ®æ¡æ•°è¶…è¿‡é™åˆ¶æ—¶è‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼ˆé»˜è®¤: Trueï¼‰
  --no-auto-split: ç¦ç”¨è‡ªåŠ¨åˆ†æ®µä¸‹è½½
  --request-delay: æ¯æ¬¡APIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…é¢‘ç‡é™åˆ¶ï¼ˆé»˜è®¤: 0.1ï¼‰
  --batch-size: æ¯å¤„ç†å¤šå°‘ä¸ªäº¤æ˜“å¯¹åæš‚åœï¼ˆé»˜è®¤: 30ï¼‰
  --batch-delay: æ¯æ‰¹å¤„ç†åçš„æš‚åœæ—¶é—´ï¼ˆç§’ï¼‰ï¼ˆé»˜è®¤: 3.0ï¼‰
  --update: æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®
  --missing-only: åªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹
  --symbols: æŒ‡å®šè¦ä¸‹è½½çš„äº¤æ˜“å¯¹åˆ—è¡¨

æ³¨æ„äº‹é¡¹ï¼š
- è¡¨åæ ¼å¼ï¼šK{interval}{symbol}ï¼Œä¾‹å¦‚æ—¥çº¿æ•°æ®å­˜å‚¨åœ¨ K1dBTCUSDT è¡¨ä¸­
- ğŸ”§ äº¤æ˜“å¯¹æ ¡éªŒï¼šä¸‹è½½å‰ä¼šè‡ªåŠ¨æ ¡éªŒäº¤æ˜“å¯¹æ˜¯å¦åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“ï¼ˆçŠ¶æ€ä¸ºTRADINGï¼‰
  * å¦‚æœäº¤æ˜“å¯¹å·²ä¸‹æ¶æˆ–æš‚åœäº¤æ˜“ï¼Œä¼šè‡ªåŠ¨è·³è¿‡å¹¶è®°å½•è­¦å‘Šæ—¥å¿—
  * äº¤æ˜“å¯¹åˆ—è¡¨ä¼šç¼“å­˜1å°æ—¶ï¼Œé¿å…é‡å¤æŸ¥è¯¢äº¤æ˜“æ‰€
  * å¦‚æœæ— æ³•è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆç½‘ç»œé—®é¢˜ç­‰ï¼‰ï¼Œä¼šè®°å½•è­¦å‘Šä½†å…è®¸ç»§ç»­ä¸‹è½½
- é»˜è®¤ä¸ä¸‹è½½å½“å¤©çš„æ•°æ®ï¼ˆå› ä¸ºå½“å¤©æ•°æ®ä¸å®Œæ•´ï¼‰
- å¢é‡æ›´æ–°è§„åˆ™ï¼š
  * æ—¥çº¿åŠä»¥ä¸Šï¼ˆ1d, 3d, 1w, 1Mï¼‰ï¼šæŒ‰æ—¥æœŸå»é‡ï¼Œä¸æ›´æ–°æœ€åä¸€å¤©
  * å°æ—¶çº¿åŠä»¥ä¸‹ï¼ˆ1h, 4h, 5mç­‰ï¼‰ï¼šæŒ‰æ—¶é—´ç‚¹å»é‡ï¼Œä¸æ›´æ–°æœ€åä¸€æ¡
- å¦‚æœæä¾›äº†--start-timeå’Œ--end-timeï¼Œä¼šè‡ªåŠ¨è®¡ç®—æ•°æ®æ¡æ•°
- å½“æ•°æ®æ¡æ•°è¶…è¿‡1500æ¡æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ†æ®µä¸‹è½½ï¼Œæ¯æ®µæœ€å¤š1500æ¡
- æ¯æ¬¡APIè¯·æ±‚ä¹‹é—´ä¼šè‡ªåŠ¨å»¶è¿Ÿï¼ˆé»˜è®¤0.1ç§’ï¼‰ï¼Œé¿å…è§¦å‘é¢‘ç‡é™åˆ¶
- æ¯å¤„ç†æŒ‡å®šæ•°é‡çš„äº¤æ˜“å¯¹åä¼šæš‚åœï¼ˆé»˜è®¤30ä¸ªåæš‚åœ3ç§’ï¼‰
- å¦‚æœæä¾›äº†--start-timeå’Œ--end-timeï¼Œä¼šä¼˜å…ˆä½¿ç”¨è¿™äº›å‚æ•°ï¼Œå¿½ç•¥--dayså‚æ•°

#3. ä»£ç è§£è¯»ï¼šä¸€æ­¥æ­¥çœ‹æ‡‚ç®¡ç†å‘˜å¦‚ä½•å·¥ä½œ
è®©æˆ‘ä»¬è·Ÿç€ä¸Šé¢çš„æµç¨‹å›¾ï¼Œçœ‹çœ‹ä»£ç æ˜¯å¦‚ä½•å®ç°çš„ï¼š

1. å‡†å¤‡å·¥ä½œ (æ–‡ä»¶å¼€å¤´) è„šæœ¬é¦–å…ˆå¯¼å…¥æ‰€æœ‰éœ€è¦çš„å·¥å…·ï¼Œæ¯”å¦‚ pandas (ç”¨äºæ•´ç†æ•°æ®)ã€ sqlalchemy (ç”¨äºå’Œæ•°æ®åº“æ²Ÿé€š) 
   ä»¥åŠé¡¹ç›®å†…å…¶ä»–æ¨¡å—å¦‚ binance_client (è´Ÿè´£å’Œå¸å®‰APIæ‰“äº¤é“)ã€‚åŒæ—¶ï¼Œå®šä¹‰äº†ä¸€äº›é‡è¦çš„è§„åˆ™ï¼Œæ¯”å¦‚APIè¯·æ±‚çš„é™åˆ¶ã€ç¼“å­˜æ—¶é—´ç­‰ã€‚
2. æŒ‡ä»¤è§£æ ( main å‡½æ•°) å½“æ‚¨è¿è¡Œè¿™ä¸ªè„šæœ¬æ—¶ï¼Œ main å‡½æ•°ï¼ˆé€šå¸¸åœ¨æ–‡ä»¶çš„æœ€ä¸‹æ–¹ï¼‰ä¼šé¦–å…ˆå¯åŠ¨ï¼Œè´Ÿè´£è§£ææ‚¨åœ¨å‘½ä»¤è¡Œè¾“å…¥çš„æŒ‡ä»¤ï¼Œ
   æ¯”å¦‚ --interval 1d æˆ– --symbols BTCUSDT ã€‚è¿™äº›æŒ‡ä»¤å‘Šè¯‰ç®¡ç†å‘˜å…·ä½“è¦ä¸‹è½½ä»€ä¹ˆã€‚
3. è·å–å¹¶éªŒè¯äº¤æ˜“å¯¹ ( get_valid_trading_symbols å’Œ validate_symbol ) åœ¨å¼€å§‹ä¸‹è½½å‰ï¼Œ
   è„šæœ¬ä¼šè°ƒç”¨ get_valid_trading_symbols ä»äº¤æ˜“æ‰€è·å–ä¸€ä»½æ‰€æœ‰â€œä»åœ¨å‘è¡Œâ€çš„äº¤æ˜“å¯¹åˆ—è¡¨ã€‚ä¸ºäº†æ•ˆç‡ï¼Œè¿™ä»½åˆ—è¡¨ä¼šè¢« ç¼“å­˜ä¸€ä¸ªå°æ—¶ ï¼Œ
   é¿å…æ¯æ¬¡éƒ½å»éº»çƒ¦äº¤æ˜“æ‰€ã€‚æ¥ç€ï¼Œåœ¨å¤„ç†æ¯ä¸ªäº¤æ˜“å¯¹æ—¶ï¼Œ validate_symbol ä¼šæ ¸å¯¹ä¸€ä¸‹ï¼Œç¡®ä¿å®ƒåœ¨è¿™ä»½æœ‰æ•ˆåˆ—è¡¨é‡Œã€‚
4. å¢é‡æ›´æ–° ( get_last_trade_date ) ä¸ºäº†ä¸é‡å¤ä¸‹è½½ï¼Œè„šæœ¬ä¼šé€šè¿‡ get_last_trade_date æŸ¥è¯¢æ•°æ®åº“ï¼Œçœ‹çœ‹è¿™ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®
   å·²ç»ä¸‹è½½åˆ°å“ªä¸ªæ—¶é—´ç‚¹äº†ã€‚è¿™æ ·å®ƒå°±èƒ½ç²¾ç¡®åœ°è®¡ç®—å‡ºä¸‹ä¸€æ¬¡åº”è¯¥ä»å“ªé‡Œå¼€å§‹ã€‚
5. è‡ªåŠ¨åˆ†æ®µ ( split_time_range ) å¸å®‰APIä¸å…è®¸ä¸€æ¬¡è¯·æ±‚å¤ªå¤šæ•°æ®ï¼ˆæ¯”å¦‚è¶…è¿‡1500æ¡ï¼‰ã€‚å¦‚æœè®¡ç®—å‘ç°éœ€è¦ä¸‹è½½çš„æ•°æ®é‡è¶…è¿‡äº†
   è¿™ä¸ªé™åˆ¶ï¼Œ split_time_range å‡½æ•°å°±ä¼šåƒåˆ‡è›‹ç³•ä¸€æ ·ï¼ŒæŠŠä¸€ä¸ªå¤§çš„æ—¶é—´èŒƒå›´åˆ‡æˆå¤šä¸ªå°æ®µï¼Œç¡®ä¿æ¯ä¸€æ®µçš„è¯·æ±‚éƒ½ä¸ä¼šè¶…é™ã€‚
6. ä¸‹è½½ä¸å­˜å‚¨ ( kline_candlestick_data å’Œ _insert_with_skip_duplicates ) ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè„šæœ¬ä¼šä¸ºæ¯ä¸€ä¸ªå°æ—¶é—´æ®µ
   è°ƒç”¨ kline_candlestick_data å‡½æ•°ï¼Œè¿™æ‰æ˜¯çœŸæ­£å»å¸å®‰APIè·å–æ•°æ®çš„æ­¥éª¤ã€‚æ‹¿åˆ°æ•°æ®åï¼Œä¼šè½¬æ¢æˆ pandas çš„ DataFrame æ ¼å¼ï¼Œ
   è¿™æ˜¯ä¸€ç§éå¸¸ä¾¿äºå¤„ç†çš„è¡¨æ ¼å½¢å¼ã€‚æœ€åï¼Œé€šè¿‡ to_sql æˆ–ç±»ä¼¼æ–¹æ³•ï¼ˆå¦‚æ­¤æ–‡ä»¶ä¸­çš„ _insert_with_skip_duplicates ï¼‰å­˜å…¥PostgreSQLæ•°æ®åº“ã€‚
   è¡¨åæ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼Œä¾‹å¦‚ K1dBTCUSDT ï¼Œæ¸…æ™°æ˜äº†ã€‚

#4 å®¹æ˜“å¿½ç•¥çš„â€œå‘â€ï¼šæ—¶åŒºä¸â€œæœªå®Œæˆâ€çš„æ•°æ®

ä¸€ä¸ªå¸¸è§çš„è¯¯è§£å’Œé™·é˜±æ˜¯å…³äº æ—¶é—´å’Œæ•°æ®çš„å®Œæ•´æ€§ ã€‚

- é™·é˜±æ˜¯ä»€ä¹ˆï¼Ÿ æ‚¨å¯èƒ½ä¼šæƒ³ï¼šâ€œä¸ºä»€ä¹ˆè„šæœ¬é»˜è®¤ä¸ä¸‹è½½å½“å¤©çš„æ•°æ®ï¼Ÿâ€ å‡è®¾ç°åœ¨æ˜¯1æœˆ24æ—¥ä¸­åˆ12ç‚¹ï¼Œæ‚¨æƒ³è·å– BTCUSDT çš„æ—¥çº¿ ( 1d ) æ•°æ®ã€‚
  æ‚¨å¯èƒ½ä¼šæœŸæœ›æ‹¿åˆ°1æœˆ24æ—¥è¿™æ ¹Kçº¿ã€‚ä½†é—®é¢˜æ˜¯ï¼Œè¿™æ ¹â€œå¤©â€Kçº¿è¦åˆ°åˆå¤œUTCæ—¶é—´24:00æ‰ç®—çœŸæ­£â€œæ”¶ç›˜â€ï¼Œå®ƒçš„æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æ”¶ç›˜ä»·åœ¨è¿™ä¸€å¤©å†…
  éƒ½è¿˜åœ¨ä¸æ–­å˜åŒ–ã€‚å¦‚æœæ‚¨åœ¨ä¸­åˆ12ç‚¹å°±æŠŠå®ƒä¸‹è½½å¹¶ä¿å­˜äº†ï¼Œæ‚¨å­˜å‚¨çš„å°±æ˜¯ä¸€ä¸ª ä¸å®Œæ•´ã€ä¸å‡†ç¡® çš„â€œåŠæˆå“â€ã€‚
- è„šæœ¬å¦‚ä½•é¿å…è¿™ä¸ªé™·é˜±ï¼Ÿ è¿™ä¸ªè„šæœ¬è®¾è®¡å¾—éå¸¸ä¸¥è°¨ï¼Œå®ƒé»˜è®¤åªä¸‹è½½å·²ç» å®Œå…¨èµ°å®Œ çš„æ—¶é—´å‘¨æœŸçš„æ•°æ®ã€‚å¯¹äºæ—¥çº¿ï¼Œå®ƒé€šå¸¸ä¼šä¸‹è½½åˆ°â€œæ˜¨å¤©â€ä¸ºæ­¢ã€‚
  è¿™æ ·å¯ä»¥ç¡®ä¿å­˜å…¥æ•°æ®åº“çš„æ¯ä¸€æ¡è®°å½•éƒ½æ˜¯æœ€ç»ˆçš„ã€ä¸ä¼šå†æ”¹å˜çš„ã€‚
- ä»£ç ä¸­çš„ä½“ç° æ‚¨ä¼šçœ‹åˆ°ä»£ç åœ¨è®¡ç®—èµ·æ­¢æ—¶é—´æ—¶ï¼Œå¸¸å¸¸ä½¿ç”¨ datetime.now() - timedelta(days=1) è¿™æ ·çš„é€»è¾‘ã€‚
  æ­¤å¤–ï¼Œ ensure_utc_timezone å‡½æ•°çš„å­˜åœ¨è‡³å…³é‡è¦ã€‚é‡‘èæ•°æ®APIå‡ ä¹æ€»æ˜¯ä»¥**UTCï¼ˆåè°ƒä¸–ç•Œæ—¶ï¼‰**ä¸ºæ ‡å‡†ã€‚å¦‚æœåœ¨å¤„ç†æ—¶é—´æ—¶ä¸ç»Ÿä¸€æ—¶åŒºï¼Œ
  å¾ˆå®¹æ˜“å› ä¸ºæ—¶å·®å¯¼è‡´è¯·æ±‚é”™è¯¯çš„æ—¶é—´èŒƒå›´ï¼ˆæ¯”å¦‚â€œå·®ä¸€å¤©â€é—®é¢˜ï¼‰ã€‚è¯¥è„šæœ¬å¼ºåˆ¶å°†æ‰€æœ‰æ—¶é—´å¯¹è±¡è½¬æ¢ä¸ºUTCï¼Œä»æ ¹æºä¸Šé¿å…äº†è¿™ç±»æ··ä¹±ã€‚

"""

import os
import sys
import logging
import time
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd      # pyright: ignore[reportMissingImports]
from datetime import datetime, timedelta, timezone
from typing import List, Optional
from sqlalchemy import text  # pyright: ignore[reportMissingImports]
from sqlalchemy.exc import IntegrityError  # pyright: ignore[reportMissingImports]

from binance_client import (
    in_exchange_trading_symbols,
    kline_candlestick_data,
    kline2df
)
from binance_sdk_derivatives_trading_usds_futures.rest_api.models import (
    KlineCandlestickDataIntervalEnum
)
from db import engine, create_table

# ğŸ”§ ç¼“å­˜äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆé¿å…é‡å¤æŸ¥è¯¢ï¼‰
_valid_trading_symbols_cache: Optional[List[str]] = None
_cache_timestamp: Optional[datetime] = None
CACHE_TTL_SECONDS = 3600
DEFAULT_REQUEST_DELAY = 0.3
DEFAULT_BATCH_SIZE = 30
DEFAULT_BATCH_DELAY = 3.0
BATCH_SIZE = 50  # PostgreSQL æ‰¹é‡æ’å…¥å¤§å°
API_DATA_LIMIT = 1500
DISK_SPACE_REQUIRED_GB = 1.0


def get_valid_trading_symbols(force_refresh: bool = False) -> List[str]:
    """
    è·å–äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼Œé»˜è®¤False
    
    Returns:
        æ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹åˆ—è¡¨
    """
    global _valid_trading_symbols_cache, _cache_timestamp
    
    now = datetime.now()
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if (
        not force_refresh
        and _valid_trading_symbols_cache is not None
        and _cache_timestamp is not None
        and (now - _cache_timestamp).total_seconds() < CACHE_TTL_SECONDS
    ):
        return _valid_trading_symbols_cache
    
    # ä»äº¤æ˜“æ‰€è·å–äº¤æ˜“å¯¹åˆ—è¡¨
    logging.info("æ­£åœ¨ä»äº¤æ˜“æ‰€è·å–æ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹åˆ—è¡¨...")
    try:
        valid_symbols = in_exchange_trading_symbols(status="TRADING")
        if valid_symbols:
            _valid_trading_symbols_cache = valid_symbols
            _cache_timestamp = now
            logging.info(f"è·å–åˆ° {len(valid_symbols)} ä¸ªæ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹")
            return valid_symbols
        else:
            logging.warning("æ— æ³•ä»äº¤æ˜“æ‰€è·å–äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
    except Exception as e:
        logging.error(f"è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨å¤±è´¥: {e}")
        # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›ç¼“å­˜ï¼ˆå¦‚æœæœ‰ï¼‰
        if _valid_trading_symbols_cache is not None:
            logging.warning("ä½¿ç”¨ç¼“å­˜çš„äº¤æ˜“å¯¹åˆ—è¡¨")
            return _valid_trading_symbols_cache
        return []


def validate_symbol(symbol: str, skip_validation: bool = False) -> bool:
    """
    æ ¡éªŒäº¤æ˜“å¯¹æ˜¯å¦åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        skip_validation: æ˜¯å¦è·³è¿‡æ ¡éªŒï¼ˆç”¨äºæµ‹è¯•æˆ–ç‰¹æ®Šæƒ…å†µï¼‰ï¼Œé»˜è®¤False
    
    Returns:
        bool: å¦‚æœäº¤æ˜“å¯¹æ­£å¸¸äº¤æ˜“è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if skip_validation:
        return True
    
    valid_symbols = get_valid_trading_symbols()
    
    if not valid_symbols:
        # å¦‚æœæ— æ³•è·å–äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œè®°å½•è­¦å‘Šä½†å…è®¸ç»§ç»­ï¼ˆé¿å…ç½‘ç»œé—®é¢˜å¯¼è‡´æ— æ³•ä¸‹è½½ï¼‰
        logging.warning(f"âš ï¸ æ— æ³•è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œè·³è¿‡ {symbol} çš„æ ¡éªŒï¼ˆå…è®¸ç»§ç»­ä¸‹è½½ï¼‰")
        return True
    
    if symbol not in valid_symbols:
        logging.warning(
            f"âš ï¸ äº¤æ˜“å¯¹ {symbol} ä¸åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“åˆ—è¡¨ä¸­ï¼Œè·³è¿‡ä¸‹è½½ã€‚"
            f"ï¼ˆå¯èƒ½å·²ä¸‹æ¶æˆ–æš‚åœäº¤æ˜“ï¼‰"
        )
        return False
    
    return True

# æ³¨æ„ï¼šæ—¥å¿—é…ç½®åœ¨ main.py ä¸­ç»Ÿä¸€é…ç½®ï¼Œè¿™é‡Œä¸å†é‡å¤é…ç½®
# è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½è¾“å‡ºåˆ°åŒä¸€ä¸ªåœ°æ–¹ï¼ˆç»ˆç«¯ï¼‰


def check_disk_space(required_gb: float = 1.0) -> bool:
    """
    æ£€æŸ¥ç£ç›˜å¯ç”¨ç©ºé—´ï¼ˆä»…ä¾›å‚è€ƒï¼ŒPostgreSQL æ•°æ®åº“å­˜å‚¨åœ¨æœåŠ¡å™¨ä¸Šï¼‰
    
    æ³¨æ„ï¼šç”±äºä½¿ç”¨ PostgreSQLï¼Œæ•°æ®åº“å®é™…å­˜å‚¨åœ¨æœåŠ¡å™¨ä¸Šï¼Œæœ¬åœ°ç£ç›˜ç©ºé—´æ£€æŸ¥ä»…ä¾›å‚è€ƒã€‚
    å®é™…åº”è¯¥æ£€æŸ¥ PostgreSQL æœåŠ¡å™¨æ‰€åœ¨ç£ç›˜çš„ç©ºé—´ã€‚
    
    Args:
        required_gb: éœ€è¦çš„æœ€å°å¯ç”¨ç©ºé—´ï¼ˆGBï¼‰ï¼Œé»˜è®¤ 1GBï¼ˆæ­¤å‚æ•°å·²ä¸å†ä½¿ç”¨ï¼‰
    
    Returns:
        bool: å§‹ç»ˆè¿”å› Trueï¼ˆä¸é˜»æ­¢ä¸‹è½½ï¼‰
    """
    try:
        # PostgreSQL æ•°æ®åº“å­˜å‚¨åœ¨æœåŠ¡å™¨ä¸Šï¼Œæœ¬åœ°ç£ç›˜æ£€æŸ¥ä»…ä¾›å‚è€ƒ
        # å®é™…åº”è¯¥æ£€æŸ¥ PostgreSQL æœåŠ¡å™¨æ‰€åœ¨ç£ç›˜çš„ç©ºé—´
        import shutil
        # è·å–å½“å‰å·¥ä½œç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µï¼ˆä½œä¸ºå‚è€ƒï¼‰
        stat = shutil.disk_usage(os.getcwd())
        free_gb = stat.free / (1024 ** 3)  # è½¬æ¢ä¸º GB
        total_gb = stat.total / (1024 ** 3)
        used_percent = (stat.used / stat.total) * 100
        
        # ä»…è®°å½•ä¿¡æ¯ï¼Œä¸å‘å‡ºè­¦å‘Šï¼ˆå› ä¸ºæ•°æ®åº“åœ¨æœåŠ¡å™¨ä¸Šï¼‰
        logging.debug(f"æœ¬åœ°ç£ç›˜ç©ºé—´ï¼ˆä»…ä¾›å‚è€ƒï¼‰: æ€»å®¹é‡ {total_gb:.2f}GB, å·²ç”¨ {used_percent:.1f}%, å¯ç”¨ {free_gb:.2f}GB")
        logging.debug("æ³¨æ„ï¼šPostgreSQL æ•°æ®åº“å­˜å‚¨åœ¨æœåŠ¡å™¨ä¸Šï¼Œæœ¬åœ°ç£ç›˜ç©ºé—´ä»…ä¾›å‚è€ƒ")
        
        # å§‹ç»ˆè¿”å› Trueï¼Œä¸é˜»æ­¢ä¸‹è½½
        return True
    except Exception as e:
        logging.debug(f"æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}ï¼Œç»§ç»­æ‰§è¡Œ...")
        return True  # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå…è®¸ç»§ç»­æ‰§è¡Œ


def get_local_symbols(interval: str = "1d") -> List[str]:
    """
    è·å–æœ¬åœ°æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„äº¤æ˜“å¯¹åˆ—è¡¨
    
    ä¼˜å…ˆä»äº¤æ˜“å¯¹è¡¨è·å–ï¼Œå¦‚æœæ²¡æœ‰äº¤æ˜“å¯¹è¡¨åˆ™ä»è¡¨åæ¨æ–­
    """
    try:
        # ä¼˜å…ˆä»äº¤æ˜“å¯¹è¡¨è·å–
        from symbols import get_trading_symbols
        trading_symbols = get_trading_symbols()
        
        # æ£€æŸ¥è¿™äº›äº¤æ˜“å¯¹æ˜¯å¦æœ‰å¯¹åº”intervalçš„æ•°æ®è¡¨
        prefix = f'K{interval}'
        valid_symbols = []
        
        with engine.connect() as conn:
            for symbol in trading_symbols:
                table_name = f"{prefix}{symbol}"
                result = conn.execute(
                    text("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_schema = 'public' 
                            AND table_name = :table_name
                        );
                    """),
                    {"table_name": table_name}
                )
                if result.fetchone()[0]:
                    valid_symbols.append(symbol)
        
        if valid_symbols:
            logging.info(f"ä»äº¤æ˜“å¯¹è¡¨è·å–åˆ° {len(valid_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼ˆinterval: {interval}ï¼‰")
            return valid_symbols
    except Exception as e:
        logging.warning(f"ä»äº¤æ˜“å¯¹è¡¨è·å–äº¤æ˜“å¯¹å¤±è´¥ï¼Œå›é€€åˆ°è¡¨åæ¨æ–­æ–¹å¼: {e}")
    
    # å›é€€åˆ°åŸæ¥çš„æ–¹å¼ï¼šä»è¡¨åæ¨æ–­
    prefix = f'K{interval}'
    stmt = """
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_name LIKE :prefix
    """
    with engine.connect() as conn:
        result = conn.execute(text(stmt), {"prefix": f"{prefix}%"})
        table_names = result.fetchall()
    # å»æ‰å‰ç¼€ 'K{interval}', ä¾‹å¦‚ 'K1d' -> ''
    prefix_len = len(prefix)
    local_symbols = [name[0][prefix_len:] for name in table_names]
    logging.info(f"ä»è¡¨åæ¨æ–­è·å–åˆ° {len(local_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼ˆinterval: {interval}ï¼‰")
    return local_symbols


def calculate_interval_seconds(interval: str) -> int:
    """
    è®¡ç®—Kçº¿é—´éš”å¯¹åº”çš„ç§’æ•°
    
    Args:
        interval: Kçº¿é—´éš”å­—ç¬¦ä¸²ï¼Œå¦‚ '1m', '1h', '1d' ç­‰
    
    Returns:
        int: å¯¹åº”çš„ç§’æ•°
    """
    interval_map = {
        '1m': 60,
        '3m': 180,
        '5m': 300,
        '15m': 900,
        '30m': 1800,
        '1h': 3600,
        '2h': 7200,
        '4h': 14400,
        '6h': 21600,
        '8h': 28800,
        '12h': 43200,
        '1d': 86400,
        '3d': 259200,
        '1w': 604800,
        '1M': 2592000,  # å‡è®¾1ä¸ªæœˆ=30å¤©
    }
    return interval_map.get(interval, 86400)


def ensure_utc_timezone(*args: datetime) -> tuple:
    """
    ç¡®ä¿datetimeå¯¹è±¡å…·æœ‰UTCæ—¶åŒºä¿¡æ¯

    Args:
        *args: éœ€è¦å¤„ç†çš„datetimeå¯¹è±¡

    Returns:
        tuple: å¤„ç†åçš„datetimeå¯¹è±¡åˆ—è¡¨ï¼ˆéƒ½å¸¦æœ‰UTCæ—¶åŒºï¼‰
    """
    result = []
    for dt in args:
        if dt.tzinfo is None:
            result.append(dt.replace(tzinfo=timezone.utc))
        else:
            result.append(dt)
    return tuple(result) if len(result) > 1 else result[0]


def calculate_data_count(start_time: datetime, end_time: datetime, interval: str) -> int:
    """
    è®¡ç®—æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®æ¡æ•°
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        interval: Kçº¿é—´éš”
    
    Returns:
        int: æ•°æ®æ¡æ•°
    """
    if not start_time or not end_time:
        return 0

    start_time, end_time = ensure_utc_timezone(start_time, end_time)

    interval_seconds = calculate_interval_seconds(interval)
    total_seconds = int((end_time - start_time).total_seconds())
    count = total_seconds // interval_seconds + 1
    return count


def split_time_range(start_time: datetime, end_time: datetime, interval: str, max_count: int = API_DATA_LIMIT) -> List[tuple]:
    """
    å°†æ—¶é—´èŒƒå›´åˆ†å‰²æˆå¤šä¸ªæ®µï¼Œæ¯æ®µä¸è¶…è¿‡max_countæ¡æ•°æ®
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        interval: Kçº¿é—´éš”
        max_count: æ¯æ®µæœ€å¤§æ•°æ®æ¡æ•°ï¼Œé»˜è®¤1500
    
    Returns:
        List[tuple]: [(start1, end1), (start2, end2), ...] æ—¶é—´èŒƒå›´åˆ—è¡¨
    """
    if not start_time or not end_time:
        return []

    start_time, end_time = ensure_utc_timezone(start_time, end_time)

    interval_seconds = calculate_interval_seconds(interval)
    max_seconds = (max_count - 1) * interval_seconds  # å‡1æ˜¯å› ä¸ºåŒ…å«èµ·å§‹å’Œç»“æŸæ—¶é—´
    
    ranges = []
    current_start = start_time
    
    while current_start < end_time:
        # è®¡ç®—å½“å‰æ®µçš„ç»“æŸæ—¶é—´
        current_end = current_start + timedelta(seconds=max_seconds)
        if current_end > end_time:
            current_end = end_time
        
        ranges.append((current_start, current_end))
        current_start = current_end + timedelta(seconds=interval_seconds)
    
    return ranges


def get_existing_dates(symbol: str, interval: str = "1d") -> set:
    """è·å–æŒ‡å®šäº¤æ˜“å¯¹åœ¨æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ—¥æœŸé›†åˆ"""
    table_name = f'K{interval}{symbol}'
    # PostgreSQL è¡¨åéœ€è¦ç”¨å¼•å·åŒ…è£¹ï¼ˆä¿æŒå¤§å°å†™ï¼‰
    safe_table_name = f'"{table_name}"'
    try:
        stmt = f'SELECT trade_date FROM {safe_table_name}'
        with engine.connect() as conn:
            result = conn.execute(text(stmt))
            dates = result.fetchall()
        return {date[0] for date in dates}
    except Exception as e:
        # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•æ£€æŸ¥è¡¨åå¤§å°å†™é—®é¢˜
        logging.warning(f"è·å– {symbol} å·²å­˜åœ¨æ—¥æœŸå¤±è´¥: {e}")
        try:
            with engine.connect() as conn:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤§å°å†™ä¸åŒ¹é…çš„è¡¨å
                result = conn.execute(
                    text("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND (table_name = :table_name OR LOWER(table_name) = LOWER(:table_name))
                    """),
                    {"table_name": table_name}
                )
                actual_table_name = result.fetchone()
                if actual_table_name:
                    actual_name = actual_table_name[0]
                    safe_actual_name = f'"{actual_name}"'
                    stmt_retry = f'SELECT trade_date FROM {safe_actual_name}'
                    result_retry = conn.execute(text(stmt_retry))
                    dates_retry = result_retry.fetchall()
                    logging.info(f"ä½¿ç”¨å®é™…è¡¨å {actual_name} æˆåŠŸè·å– {len(dates_retry)} ä¸ªæ—¥æœŸ")
                    return {date[0] for date in dates_retry}
        except Exception as e2:
            logging.debug(f"æ£€æŸ¥è¡¨åæ—¶å‡ºé”™: {e2}")
        return set()


def _insert_with_skip_duplicates(df: pd.DataFrame, table_name: str, engine) -> int:
    """
    é€æ¡æ’å…¥æ•°æ®ï¼Œè·³è¿‡é‡å¤çš„trade_date
    
    Args:
        df: è¦æ’å…¥çš„DataFrame
        table_name: è¡¨å
        engine: æ•°æ®åº“å¼•æ“
    
    Returns:
        int: æˆåŠŸæ’å…¥çš„è¡Œæ•°
    """
    saved_count = 0
    skipped_count = 0
    total_rows = len(df)
    
    # ğŸ”§ ä¿®å¤ï¼šè¡¨åç”¨åŒå¼•å·æ‹¬èµ·æ¥ï¼Œé¿å…åŒ…å«ç‰¹æ®Šå­—ç¬¦æ—¶SQLè¯­æ³•é”™è¯¯
    quoted_table_name = f'"{table_name}"'
    
    for idx, (_, row) in enumerate(df.iterrows(), 1):
        try:
            # å°†rowè½¬æ¢ä¸ºå­—å…¸
            row_dict = row.to_dict()
            
            # æ„å»ºINSERTè¯­å¥ï¼Œä½¿ç”¨å‘½åå‚æ•°ï¼ˆ:paramï¼‰
            # ğŸ”§ ä¿®å¤ï¼šåˆ—åä¹Ÿç”¨åŒå¼•å·æ‹¬èµ·æ¥ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦é—®é¢˜
            columns = ', '.join([f'"{col}"' for col in df.columns])
            placeholders = ', '.join([f':{col}' for col in df.columns])
            
            stmt = f"INSERT INTO {quoted_table_name} ({columns}) VALUES ({placeholders})"
            with engine.connect() as conn:
                # SQLAlchemyçš„executeæ–¹æ³•ä½¿ç”¨å­—å…¸ä½œä¸ºå‚æ•°
                conn.execute(text(stmt), row_dict)
                conn.commit()
            saved_count += 1
            
            # æ¯å¤„ç†100æ¡è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if idx % 100 == 0:
                logging.info(f"é€æ¡æ’å…¥è¿›åº¦: {idx}/{total_rows}, å·²ä¿å­˜: {saved_count}, è·³è¿‡: {skipped_count}")
        except Exception as e:
            # å¦‚æœæ˜¯UNIQUE constrainté”™è¯¯ï¼Œè·³è¿‡è¿™æ¡æ•°æ®
            error_msg = str(e)
            is_unique_error = any(keyword in error_msg for keyword in ["UniqueViolation", "duplicate key", "IntegrityError"]) or "unique" in error_msg.lower()
            
            if is_unique_error:
                skipped_count += 1
                continue
            else:
                trade_date = row_dict.get('trade_date', 'unknown') if 'row_dict' in locals() else 'unknown'
                logging.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}, trade_date: {trade_date}")
                logging.error(f"SQLè¯­å¥: {stmt}")
                raise
    
    logging.info(f"é€æ¡æ’å…¥å®Œæˆ: æ€»è®¡ {total_rows} æ¡ï¼ŒæˆåŠŸä¿å­˜ {saved_count} æ¡ï¼Œè·³è¿‡ {skipped_count} æ¡é‡å¤æ•°æ®")
    return saved_count


def get_last_trade_date(symbol: str, interval: str = "1d") -> Optional[str]:
    """
    è·å–æŒ‡å®šäº¤æ˜“å¯¹åœ¨æ•°æ®åº“ä¸­çš„æœ€åä¸€æ¡æ•°æ®çš„trade_date
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        interval: Kçº¿é—´éš”
    
    Returns:
        Optional[str]: æœ€åä¸€æ¡æ•°æ®çš„trade_dateï¼Œå¦‚æœè¡¨ä¸å­˜åœ¨æˆ–æ²¡æœ‰æ•°æ®åˆ™è¿”å›None
    """
    table_name = f'K{interval}{symbol}'
    # PostgreSQL è¡¨åéœ€è¦ç”¨å¼•å·åŒ…è£¹ï¼ˆä¿æŒå¤§å°å†™ï¼‰
    safe_table_name = f'"{table_name}"'
    try:
        stmt = f'SELECT trade_date FROM {safe_table_name} ORDER BY open_time DESC LIMIT 1'
        with engine.connect() as conn:
            result = conn.execute(text(stmt))
            row = result.fetchone()
            if row:
                return row[0]
        return None
    except Exception as e:
        # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•æ£€æŸ¥è¡¨åå¤§å°å†™é—®é¢˜
        try:
            with engine.connect() as conn:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤§å°å†™ä¸åŒ¹é…çš„è¡¨å
                result = conn.execute(
                    text("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND (table_name = :table_name OR LOWER(table_name) = LOWER(:table_name))
                    """),
                    {"table_name": table_name}
                )
                actual_table_name = result.fetchone()
                if actual_table_name:
                    actual_name = actual_table_name[0]
                    safe_actual_name = f'"{actual_name}"'
                    stmt_retry = f'SELECT trade_date FROM {safe_actual_name} ORDER BY trade_date DESC LIMIT 1'
                    result_retry = conn.execute(text(stmt_retry))
                    row_retry = result_retry.fetchone()
                    if row_retry:
                        logging.debug(f"ä½¿ç”¨å®é™…è¡¨å {actual_name} æˆåŠŸè·å–æœ€åäº¤æ˜“æ—¥æœŸ")
                        return row_retry[0]
        except Exception as e2:
            logging.debug(f"æ£€æŸ¥è¡¨åæ—¶å‡ºé”™: {e2}")
        # è¡¨ä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼Œè¿”å›None
        return None


def compare_trade_dates(last_date: str, end_time: datetime, interval: str) -> bool:
    """
    æ¯”è¾ƒæœ¬åœ°æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´ä¸end_time
    
    Args:
        last_date: æœ¬åœ°æœ€åä¸€æ¡æ•°æ®çš„trade_dateï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        end_time: è¦ä¸‹è½½çš„ç»“æŸæ—¶é—´
        interval: Kçº¿é—´éš”
    
    Returns:
        bool: å¦‚æœlast_date >= end_timeå¯¹åº”çš„æ—¥æœŸ/æ—¶é—´ï¼Œè¿”å›Trueï¼ˆè¡¨ç¤ºå·²æ˜¯æœ€æ–°æ•°æ®ï¼‰
    """
    try:
        if interval in ['1d', '3d', '1w', '1M']:
            # æ—¥çº¿åŠä»¥ä¸Šï¼Œæ¯”è¾ƒæ—¥æœŸ
            last_date_obj = datetime.strptime(last_date, '%Y-%m-%d').date()
            # ç¡®ä¿end_timeæœ‰æ—¶åŒºä¿¡æ¯ï¼Œç„¶åè½¬æ¢ä¸ºdate
            if end_time.tzinfo is None:
                end_date = end_time.date()
            else:
                end_date = end_time.astimezone(timezone.utc).date()
            result = last_date_obj >= end_date
            comparison_op = ">=" if result else "<"
            logging.info(f"æ—¥æœŸæ¯”è¾ƒ: æœ¬åœ°æœ€åæ—¥æœŸ={last_date_obj}, ç»“æŸæ—¥æœŸ={end_date}, ç»“æœ={result} (æœ¬åœ°{comparison_op}ç»“æŸ)")
            return result
        else:
            # å°æ—¶çº¿åŠä»¥ä¸‹ï¼Œæ¯”è¾ƒå®Œæ•´æ—¶é—´
            last_date_obj = datetime.strptime(last_date, '%Y-%m-%d %H:%M:%S')
            # ç¡®ä¿ä¸¤ä¸ªdatetimeå¯¹è±¡éƒ½æœ‰ç›¸åŒçš„æ—¶åŒºä¿¡æ¯
            if end_time.tzinfo is not None:
                # end_timeæœ‰æ—¶åŒºä¿¡æ¯ï¼Œå°†last_date_objä¹Ÿè½¬æ¢ä¸ºUTCæ—¶åŒº
                last_date_obj = last_date_obj.replace(tzinfo=timezone.utc)
            elif last_date_obj.tzinfo is not None:
                # last_date_objæœ‰æ—¶åŒºä¿¡æ¯ï¼Œå°†end_timeä¹Ÿè½¬æ¢ä¸ºUTCæ—¶åŒº
                end_time = end_time.replace(tzinfo=timezone.utc)
            
            result = last_date_obj >= end_time
            end_time_str = end_time.strftime('%Y-%m-%d %H:%M:%S')
            comparison_op = ">=" if result else "<"
            logging.info(f"æ—¶é—´æ¯”è¾ƒ: æœ¬åœ°æœ€åæ—¶é—´={last_date}, ç»“æŸæ—¶é—´={end_time_str}, ç»“æœ={result} (æœ¬åœ°{comparison_op}ç»“æŸ)")
            return result
    except Exception as e:
        logging.warning(f"æ¯”è¾ƒæ—¥æœŸå¤±è´¥: {e}, last_date={last_date}, end_time={end_time}, interval={interval}")
        return False


def _get_default_end_time(interval: str, reference_time: Optional[datetime] = None) -> datetime:
    """
    è·å–æŒ‡å®šKçº¿é—´éš”çš„é»˜è®¤ç»“æŸæ—¶é—´

    Args:
        interval: Kçº¿é—´éš”
        reference_time: å‚è€ƒæ—¶é—´ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´

    Returns:
        datetime: é»˜è®¤ç»“æŸæ—¶é—´
    """
    if reference_time is None:
        reference_time = datetime.now(timezone.utc)

    if interval in ['1d', '3d', '1w', '1M']:
        today = reference_time.replace(hour=0, minute=0, second=0, microsecond=0)
        return today - timedelta(seconds=1)
    else:
        interval_seconds = calculate_interval_seconds(interval)
        now_utc = reference_time if reference_time.tzinfo is not None else reference_time.replace(tzinfo=timezone.utc)
        current_timestamp = int(now_utc.timestamp())
        kline_index = current_timestamp // interval_seconds
        current_kline_start_timestamp = kline_index * interval_seconds
        latest_complete_kline_start_timestamp = current_kline_start_timestamp - interval_seconds
        return datetime.fromtimestamp(latest_complete_kline_start_timestamp, tz=timezone.utc)


def _get_latest_complete_kline_time(interval: str) -> datetime:
    """
    è·å–å½“å‰æ—¶é—´ä¹‹å‰æœ€æ–°å®Œæ•´Kçº¿çš„å¼€å§‹æ—¶é—´

    Args:
        interval: Kçº¿é—´éš”

    Returns:
        datetime: æœ€æ–°å®Œæ•´Kçº¿çš„å¼€å§‹æ—¶é—´ï¼ˆUTCæ—¶åŒºï¼‰
    """
    interval_seconds = calculate_interval_seconds(interval)
    now_utc = datetime.now(timezone.utc)
    current_timestamp = int(now_utc.timestamp())
    kline_index = current_timestamp // interval_seconds
    current_kline_start_timestamp = kline_index * interval_seconds
    latest_complete_kline_start_timestamp = current_kline_start_timestamp - interval_seconds
    return datetime.fromtimestamp(latest_complete_kline_start_timestamp, tz=timezone.utc)


def download_kline_data(
    symbol: str,
    interval: str = "1d",
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    limit: Optional[int] = API_DATA_LIMIT,
    update_existing: bool = False,
    auto_split: bool = True,
    request_delay: float = DEFAULT_REQUEST_DELAY,
    skip_symbol_validation: bool = False
) -> bool:
    """
    ä¸‹è½½æŒ‡å®šäº¤æ˜“å¯¹çš„Kçº¿æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
    
    æ³¨æ„ï¼šé»˜è®¤ä¸ä¼šä¸‹è½½å½“å¤©çš„æ•°æ®, å› ä¸ºå½“å¤©æ•°æ®ä¸å®Œæ•´(è¿˜åœ¨äº¤æ˜“ä¸­)ã€‚
    åªæœ‰åœ¨ç¬¬äºŒå¤©æ›´æ–°å‰ä¸€å¤©çš„æ•°æ®æ‰å‡†ç¡®ã€‚
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·, å¦‚ 'BTCUSDT'
        interval: Kçº¿é—´éš”, é»˜è®¤ '1d'(æ—¥çº¿)
        start_time: å¼€å§‹æ—¶é—´, é»˜è®¤None(ä»æœ€æ—©å¼€å§‹)
        end_time: ç»“æŸæ—¶é—´, é»˜è®¤None(åˆ°æ˜¨å¤©çš„ç»“æŸæ—¶é—´, ä¸åŒ…å«ä»Šå¤©)
        limit: æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§æ¡æ•°, é»˜è®¤1500ã€‚å¦‚æœä¸ºNoneä¸”æä¾›äº†start_timeå’Œend_timeï¼Œä¼šè‡ªåŠ¨è®¡ç®—
        update_existing: æ˜¯å¦æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®, é»˜è®¤False
        auto_split: å½“æ•°æ®æ¡æ•°è¶…è¿‡limitæ—¶æ˜¯å¦è‡ªåŠ¨åˆ†æ®µä¸‹è½½, é»˜è®¤True
        request_delay: æ¯æ¬¡APIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…é¢‘ç‡é™åˆ¶, é»˜è®¤0.3ç§’
        skip_symbol_validation: æ˜¯å¦è·³è¿‡äº¤æ˜“å¯¹æ ¡éªŒï¼ˆç”¨äºæµ‹è¯•æˆ–ç‰¹æ®Šæƒ…å†µï¼‰ï¼Œé»˜è®¤False
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸä¸‹è½½
    """
    logging.info(f"å¼€å§‹ä¸‹è½½ {symbol} çš„ {interval} Kçº¿æ•°æ®...")
    if start_time:
        logging.info(f"  å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    if end_time:
        logging.info(f"  ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ğŸ”§ æ ¡éªŒäº¤æ˜“å¯¹æ˜¯å¦åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“
    if not validate_symbol(symbol, skip_validation=skip_symbol_validation):
        logging.warning(f"{symbol} äº¤æ˜“å¯¹æ ¡éªŒå¤±è´¥ï¼Œè·³è¿‡ä¸‹è½½")
        return False
    
    table_name = f'K{interval}{symbol}'
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆä»…ä¾›å‚è€ƒï¼ŒPostgreSQL æ•°æ®åº“åœ¨æœåŠ¡å™¨ä¸Šï¼‰
    # æ­¤æ£€æŸ¥ä¸ä¼šé˜»æ­¢ä¸‹è½½ï¼Œä»…ç”¨äºè®°å½•å‚è€ƒä¿¡æ¯
    check_disk_space(required_gb=DISK_SPACE_REQUIRED_GB)
    
    try:
        # å¦‚æœæœªå¯ç”¨update_existingï¼Œå…ˆæ£€æŸ¥æœ¬åœ°æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´
        if not update_existing:
            check_end_time = end_time if end_time is not None else _get_default_end_time(interval)

            last_trade_date = get_last_trade_date(symbol, interval)
            if last_trade_date:
                if compare_trade_dates(last_trade_date, check_end_time, interval):
                    end_time_str = check_end_time.strftime('%Y-%m-%d' if interval in ['1d', '3d', '1w', '1M'] else '%Y-%m-%d %H:%M:%S')
                    logging.info(f"{symbol} æœ¬åœ°æ•°æ®æœ€åæ—¶é—´({last_trade_date}) >= ç»“æŸæ—¶é—´({end_time_str})ï¼Œè·³è¿‡ä¸‹è½½ï¼ˆä½¿ç”¨--updateå¯å¼ºåˆ¶æ›´æ–°ï¼‰")
                    return True
        
        # åˆ›å»ºè¡¨(å¦‚æœä¸å­˜åœ¨)
        create_table(table_name)
        
        # è·å–å·²å­˜åœ¨çš„æ—¥æœŸ
        existing_dates = get_existing_dates(symbol, interval) if not update_existing else set()
        
        # è½¬æ¢æ—¶é—´é—´éš”
        interval_enum = KlineCandlestickDataIntervalEnum[f"INTERVAL_{interval}"].value

        # è½¬æ¢æ—¶é—´æ ¼å¼(å¦‚æœéœ€è¦)
        if end_time is None:
            end_time = _get_default_end_time(interval)
            logging.info(f"{symbol} é»˜è®¤ç»“æŸæ—¶é—´è®¾ç½®ä¸ºæœ€æ–°å®Œæ•´Kçº¿æ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
        
        # å¦‚æœæä¾›äº†start_timeå’Œend_timeï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µä¸‹è½½
        max_limit = limit if limit is not None else API_DATA_LIMIT
        
        if start_time and end_time and auto_split:
            # è®¡ç®—é¢„è®¡æ•°æ®æ¡æ•°
            data_count = calculate_data_count(start_time, end_time, interval)
            logging.info(f"{symbol} é¢„è®¡æ•°æ®æ¡æ•°: {data_count}, é™åˆ¶: {max_limit}")
            
            if data_count > max_limit:
                # éœ€è¦åˆ†æ®µä¸‹è½½
                logging.info(f"{symbol} æ•°æ®æ¡æ•°({data_count})è¶…è¿‡é™åˆ¶({max_limit})ï¼Œå°†åˆ†æ®µä¸‹è½½")
                time_ranges = split_time_range(start_time, end_time, interval, max_limit)
                logging.info(f"{symbol} å°†åˆ†ä¸º {len(time_ranges)} æ®µä¸‹è½½")
                
                all_dfs = []
                for idx, (seg_start, seg_end) in enumerate(time_ranges, 1):
                    logging.info(f"{symbol} æ­£åœ¨ä¸‹è½½ç¬¬ {idx}/{len(time_ranges)} æ®µ: {seg_start.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {seg_end.strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    seg_start_ts = int(seg_start.timestamp() * 1000)
                    seg_end_ts = int(seg_end.timestamp() * 1000)
                    
                    # è¯·æ±‚å‰æš‚åœï¼Œé¿å…é¢‘ç‡é™åˆ¶
                    if request_delay > 0:
                        time.sleep(request_delay)
                    
                    try:
                        klines = kline_candlestick_data(
                            symbol=symbol,
                            interval=interval_enum,
                            starttime=seg_start_ts,
                            endtime=seg_end_ts,
                            limit=max_limit
                        )
                        
                        if klines:
                            seg_df = kline2df(klines)
                            if not seg_df.empty:
                                all_dfs.append(seg_df)
                                logging.info(f"{symbol} ç¬¬ {idx} æ®µä¸‹è½½æˆåŠŸï¼Œè·å¾— {len(seg_df)} æ¡æ•°æ®")
                            else:
                                logging.warning(f"{symbol} ç¬¬ {idx} æ®µè½¬æ¢åçš„DataFrameä¸ºç©º")
                        else:
                            logging.warning(f"{symbol} ç¬¬ {idx} æ®µæ²¡æœ‰è·å–åˆ°Kçº¿æ•°æ®")
                    except Exception as e:
                        error_msg = str(e)
                        logging.error(f"{symbol} ç¬¬ {idx} æ®µä¸‹è½½å¤±è´¥: {e}")
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯APIé¢‘ç‡é™åˆ¶é”™è¯¯
                        if 'Way too many requests' in error_msg or 'banned until' in error_msg:
                            # å°è¯•ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å°ç¦æ—¶é—´
                            import re
                            banned_match = re.search(r'banned until (\d+)', error_msg)
                            if banned_match:
                                banned_until = int(banned_match.group(1))
                                current_time = int(time.time() * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                                wait_time = max(0, (banned_until - current_time) / 1000)  # è½¬æ¢ä¸ºç§’
                                if wait_time > 0:
                                    logging.warning(f"{symbol} æ£€æµ‹åˆ°APIé¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                                    time.sleep(min(wait_time + 5, 300))  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                            else:
                                # å¦‚æœæ²¡æœ‰æå–åˆ°å°ç¦æ—¶é—´ï¼Œç­‰å¾…60ç§’
                                logging.warning(f"{symbol} æ£€æµ‹åˆ°APIé¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… 60 ç§’...")
                                time.sleep(60)
                        
                        continue
                
                if not all_dfs:
                    logging.warning(f"{symbol} æ‰€æœ‰åˆ†æ®µéƒ½æ²¡æœ‰è·å–åˆ°æ•°æ®")
                    return False
                
                # åˆå¹¶æ‰€æœ‰åˆ†æ®µçš„æ•°æ®
                df = pd.concat(all_dfs, ignore_index=True)
                
                # å°†trade_dateè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼(ç”¨äºæ•°æ®åº“å­˜å‚¨å’Œå»é‡)
                # æ ¹æ®Kçº¿é—´éš”é€‰æ‹©åˆé€‚çš„æ—¥æœŸæ ¼å¼
                if interval in ['1d', '3d', '1w', '1M']:
                    # æ—¥çº¿åŠä»¥ä¸Š, ä½¿ç”¨æ—¥æœŸæ ¼å¼
                    df['trade_date'] = df['trade_date'].dt.strftime('%Y-%m-%d')
                else:
                    # å°æ—¶çº¿åŠä»¥ä¸‹, ä½¿ç”¨å®Œæ•´æ—¶é—´æ ¼å¼
                    df['trade_date'] = df['trade_date'].dt.strftime('%Y-%m-%d %H:%M:%S')
                
                # å»é‡ï¼ˆæŒ‰trade_dateï¼‰
                df = df.drop_duplicates(subset=['trade_date'], keep='first')
                logging.info(f"{symbol} åˆ†æ®µä¸‹è½½å®Œæˆï¼Œåˆå¹¶åå…± {len(df)} æ¡æ•°æ®ï¼ˆå»é‡å‰: {sum(len(d) for d in all_dfs)} æ¡ï¼‰")
            else:
                # ä¸éœ€è¦åˆ†æ®µï¼Œç›´æ¥ä¸‹è½½
                start_time, end_time = ensure_utc_timezone(start_time, end_time)

                start_timestamp = int(start_time.timestamp() * 1000)
                end_timestamp = int(end_time.timestamp() * 1000)

                # è¯·æ±‚å‰æš‚åœ
                if request_delay > 0:
                    time.sleep(request_delay)
                
                logging.info(f"æ­£åœ¨ä¸‹è½½ {symbol} çš„Kçº¿æ•°æ®...")
                klines = kline_candlestick_data(
                    symbol=symbol,
                    interval=interval_enum,
                    starttime=start_timestamp,
                    endtime=end_timestamp,
                    limit=max_limit
                )
                
                if not klines:
                    logging.warning(f"{symbol} æ²¡æœ‰è·å–åˆ°Kçº¿æ•°æ®")
                    return False
                
                df = kline2df(klines)
        else:
            # åŸæœ‰é€»è¾‘ï¼šå•æ¬¡ä¸‹è½½ï¼ˆä¸è‡ªåŠ¨åˆ†æ®µæˆ–æ²¡æœ‰æä¾›æ—¶é—´èŒƒå›´ï¼‰
            start_timestamp = None
            end_timestamp = None
            if start_time:
                start_time = ensure_utc_timezone(start_time)
                start_timestamp = int(start_time.timestamp() * 1000)
            if end_time:
                end_time = ensure_utc_timezone(end_time)
                end_timestamp = int(end_time.timestamp() * 1000)

            # è¯·æ±‚å‰æš‚åœ
            if request_delay > 0:
                time.sleep(request_delay)
            
            # ä¸‹è½½Kçº¿æ•°æ®
            logging.info(f"æ­£åœ¨ä¸‹è½½ {symbol} çš„Kçº¿æ•°æ®...")
            klines = kline_candlestick_data(
                symbol=symbol,
                interval=interval_enum,
                starttime=start_timestamp,
                endtime=end_timestamp,
                limit=max_limit
            )
            
            if not klines:
                logging.warning(f"{symbol} æ²¡æœ‰è·å–åˆ°Kçº¿æ•°æ®")
                return False
            
            # è½¬æ¢ä¸ºDataFrame
            df = kline2df(klines)
        
        if df.empty:
            logging.warning(f"{symbol} è½¬æ¢åçš„DataFrameä¸ºç©º")
            return False
        
        # å°†trade_dateè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼(ç”¨äºæ•°æ®åº“å­˜å‚¨å’Œå»é‡)
        # æ³¨æ„ï¼šåˆ†æ®µä¸‹è½½æ—¶å·²ç»åœ¨åˆå¹¶å‰è½¬æ¢è¿‡äº†ï¼Œè¿™é‡Œéœ€è¦æ£€æŸ¥æ˜¯å¦å·²è½¬æ¢
        if df['trade_date'].dtype == 'object':
            # å·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œè·³è¿‡è½¬æ¢
            pass
        else:
            # æ ¹æ®Kçº¿é—´éš”é€‰æ‹©åˆé€‚çš„æ—¥æœŸæ ¼å¼
            if interval in ['1d', '3d', '1w', '1M']:
                # æ—¥çº¿åŠä»¥ä¸Š, ä½¿ç”¨æ—¥æœŸæ ¼å¼
                df['trade_date'] = df['trade_date'].dt.strftime('%Y-%m-%d')
            else:
                # å°æ—¶çº¿åŠä»¥ä¸‹, ä½¿ç”¨å®Œæ•´æ—¶é—´æ ¼å¼
                df['trade_date'] = df['trade_date'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # è¿‡æ»¤æ‰ä¸å®Œæ•´çš„æ•°æ®
        now_utc = datetime.now(timezone.utc)
        today_str = now_utc.strftime('%Y-%m-%d')
        before_filter = len(df)

        if interval in ['1d', '3d', '1w', '1M']:
            df = df[df['trade_date'] != today_str]
        else:
            latest_complete_time = _get_latest_complete_kline_time(interval)

            def is_complete_kline(trade_date_str: str) -> bool:
                try:
                    trade_date_obj = datetime.strptime(trade_date_str, '%Y-%m-%d %H:%M:%S')
                    trade_date_utc = trade_date_obj.replace(tzinfo=timezone.utc)
                    return trade_date_utc <= latest_complete_time
                except (ValueError, TypeError):
                    return True

            df = df[df['trade_date'].apply(is_complete_kline)]

            if before_filter > len(df):
                logging.info(f"{symbol} è¿‡æ»¤æ‰ {before_filter - len(df)} æ¡ä¸å®Œæ•´çš„Kçº¿æ•°æ®ï¼ˆæœ€æ–°å®Œæ•´Kçº¿æ—¶é—´: {latest_complete_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼‰")

        after_filter = len(df)
        if after_filter < before_filter:
            logging.info(f"{symbol} å…±è¿‡æ»¤æ‰ {before_filter - after_filter} æ¡ä¸å®Œæ•´æ•°æ®")
        
        # å†æ¬¡å»é‡ï¼ˆç¡®ä¿DataFrameå†…éƒ¨æ²¡æœ‰é‡å¤çš„trade_dateï¼‰
        before_dedup = len(df)
        df = df.drop_duplicates(subset=['trade_date'], keep='first')
        after_dedup = len(df)
        if after_dedup < before_dedup:
            logging.info(f"{symbol} DataFrameå†…éƒ¨å»é‡ï¼Œç§»é™¤ {before_dedup - after_dedup} æ¡é‡å¤æ•°æ®")
        
        # è¿‡æ»¤å·²å­˜åœ¨çš„æ•°æ®
        if existing_dates and not update_existing:
            before_count = len(df)
            df = df[~df['trade_date'].isin(existing_dates)]
            after_count = len(df)
            if after_count < before_count:
                logging.info(f"{symbol} è¿‡æ»¤æ‰ {before_count - after_count} æ¡å·²å­˜åœ¨çš„æ•°æ®")
        
        if df.empty:
            logging.info(f"{symbol} æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
            return True
        
        # ä¿å­˜åˆ°æ•°æ®åº“å‰ï¼Œå†æ¬¡è·å–æœ€æ–°çš„å·²å­˜åœ¨æ•°æ®ï¼ˆé˜²æ­¢å¹¶å‘æ’å…¥ï¼‰
        if not update_existing:
            current_existing_dates = get_existing_dates(symbol, interval)
            if current_existing_dates:
                before_final_check = len(df)
                df = df[~df['trade_date'].isin(current_existing_dates)]
                after_final_check = len(df)
                if after_final_check < before_final_check:
                    logging.info(f"{symbol} æœ€ç»ˆæ£€æŸ¥è¿‡æ»¤æ‰ {before_final_check - after_final_check} æ¡å·²å­˜åœ¨çš„æ•°æ®")
                if df.empty:
                    logging.info(f"{symbol} æœ€ç»ˆæ£€æŸ¥åæ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
                    return True
        
        BATCH_SIZE = 50  # PostgreSQL æ‰¹é‡æ’å…¥å¤§å°
        total_rows = len(df)
        saved_count = 0

        if total_rows <= BATCH_SIZE:
            try:
                df.to_sql(
                    name=table_name,
                    con=engine,
                    if_exists='append',
                    index=False,
                    method='multi'
                )
                saved_count = len(df)
            except Exception as e:
                # ğŸ”§ å¢å¼ºï¼šå¦‚æœæ˜¯å”¯ä¸€çº¦æŸå†²çªï¼Œé™çº§åˆ°é€æ¡æ’å…¥
                error_msg = str(e)
                is_unique_error = any(keyword in error_msg for keyword in ["UniqueViolation", "duplicate key", "IntegrityError"]) or "unique" in error_msg.lower()
                
                if is_unique_error:
                    logging.warning(f"âš ï¸ {symbol} æ‰¹é‡æ’å…¥å‘ç”Ÿå†²çªï¼Œå°è¯•é™çº§åˆ°é€æ¡æ’å…¥è‡ªæ„ˆæ¨¡å¼...")
                    saved_count = _insert_with_skip_duplicates(df, table_name, engine)
                else:
                    logging.error(f"{symbol} æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
                    raise
        else:
            for i in range(0, total_rows, BATCH_SIZE):
                batch_df = df.iloc[i:i+BATCH_SIZE]
                try:
                    batch_df.to_sql(
                        name=table_name,
                        con=engine,
                        if_exists='append',
                        index=False,
                        method='multi'
                    )
                    saved_count += len(batch_df)
                except Exception as e:
                    # ğŸ”§ å¢å¼ºï¼šæ‰¹é‡æ’å…¥å¤±è´¥æ—¶å°è¯•é€æ¡æ’å…¥è¯¥æ‰¹æ¬¡
                    error_msg = str(e)
                    is_unique_error = any(keyword in error_msg for keyword in ["UniqueViolation", "duplicate key", "IntegrityError"]) or "unique" in error_msg.lower()
                    
                    if is_unique_error:
                        logging.warning(f"âš ï¸ {symbol} ç¬¬ {i//BATCH_SIZE + 1} æ‰¹æ’å…¥å†²çªï¼Œå°è¯•é€æ¡æ’å…¥è‡ªæ„ˆ...")
                        saved_batch_count = _insert_with_skip_duplicates(batch_df, table_name, engine)
                        saved_count += saved_batch_count
                    else:
                        logging.error(f"{symbol} ç¬¬ {i//BATCH_SIZE + 1} æ‰¹æ’å…¥å¤±è´¥: {e}")
                        raise

                if (i + BATCH_SIZE) % (BATCH_SIZE * 10) == 0 or (i + BATCH_SIZE) >= total_rows:
                    logging.info(f"{symbol} å·²ä¿å­˜ {saved_count}/{total_rows} æ¡æ•°æ®")
        
        if saved_count < total_rows:
            logging.info(f"{symbol} æˆåŠŸä¿å­˜ {saved_count} æ¡Kçº¿æ•°æ®ï¼ˆå…± {total_rows} æ¡ï¼Œè·³è¿‡ {total_rows - saved_count} æ¡é‡å¤æ•°æ®ï¼‰")
        else:
            logging.info(f"{symbol} æˆåŠŸä¿å­˜ {saved_count} æ¡Kçº¿æ•°æ®")
        return True
        
    except Exception as e:
        logging.error(f"ä¸‹è½½ {symbol} Kçº¿æ•°æ®å¤±è´¥: {e}")
        return False


def download_all_symbols(
    interval: str = "1d",
    days_back: Optional[int] = None,
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    limit: Optional[int] = API_DATA_LIMIT,
    update_existing: bool = False,
    symbols: Optional[List[str]] = None,
    auto_split: bool = True,
    request_delay: float = DEFAULT_REQUEST_DELAY,
    batch_size: int = DEFAULT_BATCH_SIZE,
    batch_delay: float = DEFAULT_BATCH_DELAY
):
    """
    ä¸‹è½½æ‰€æœ‰äº¤æ˜“å¯¹çš„Kçº¿æ•°æ®
    
    Args:
        interval: Kçº¿é—´éš”, é»˜è®¤ '1d'
        days_back: å›æº¯å¤©æ•°, é»˜è®¤None(ä¸‹è½½æ‰€æœ‰æ•°æ®), å¦‚æœæä¾›äº†start_timeå’Œend_timeåˆ™å¿½ç•¥æ­¤å‚æ•°
        start_time: å¼€å§‹æ—¶é—´, é»˜è®¤None(æ ¹æ®days_backè®¡ç®—æˆ–ä¸‹è½½æ‰€æœ‰æ•°æ®)
        end_time: ç»“æŸæ—¶é—´, é»˜è®¤None(æ˜¨å¤©çš„ç»“æŸæ—¶é—´)
        limit: æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§æ¡æ•°, é»˜è®¤1500
        update_existing: æ˜¯å¦æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®, é»˜è®¤False
        symbols: æŒ‡å®šè¦ä¸‹è½½çš„äº¤æ˜“å¯¹åˆ—è¡¨, é»˜è®¤None(ä¸‹è½½æ‰€æœ‰)
    """
    logging.info("=" * 80)
    logging.info(f"å¼€å§‹ä¸‹è½½æ‰€æœ‰äº¤æ˜“å¯¹çš„Kçº¿æ•°æ®ï¼Œé—´éš”: {interval}")
    if start_time:
        logging.info(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    if end_time:
        logging.info(f"ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    if days_back:
        logging.info(f"å›æº¯å¤©æ•°: {days_back}")
    logging.info("=" * 80)
    
    # è·å–äº¤æ˜“å¯¹åˆ—è¡¨
    if symbols is None:
        logging.info("æ­£åœ¨è·å–æ‰€æœ‰äº¤æ˜“å¯¹...")
        all_symbols = in_exchange_trading_symbols()
        if not all_symbols:
            logging.error("æ— æ³•è·å–äº¤æ˜“å¯¹åˆ—è¡¨")
            return
        # ä»äº¤æ˜“æ‰€è·å–çš„äº¤æ˜“å¯¹åˆ—è¡¨å·²ç»æ˜¯æ­£å¸¸äº¤æ˜“çš„ï¼Œä¸éœ€è¦é¢å¤–æ ¡éªŒ
        logging.info(f"ä»äº¤æ˜“æ‰€è·å–åˆ° {len(all_symbols)} ä¸ªæ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹")
    else:
        # ğŸ”§ å¦‚æœç”¨æˆ·æä¾›äº†è‡ªå®šä¹‰äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œéœ€è¦æ ¡éªŒæ¯ä¸ªäº¤æ˜“å¯¹
        all_symbols = symbols
        logging.info(f"ç”¨æˆ·æŒ‡å®šäº† {len(all_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œå°†è¿›è¡Œæ ¡éªŒ...")
        
        # è¿‡æ»¤æ‰ä¸åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹
        valid_symbols = []
        invalid_symbols = []
        valid_trading_list = get_valid_trading_symbols()
        
        for symbol in all_symbols:
            if valid_trading_list and symbol not in valid_trading_list:
                invalid_symbols.append(symbol)
                logging.warning(f"âš ï¸ äº¤æ˜“å¯¹ {symbol} ä¸åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“åˆ—è¡¨ä¸­ï¼Œå°†è·³è¿‡")
            else:
                valid_symbols.append(symbol)
        
        if invalid_symbols:
            logging.warning(f"âš ï¸ å…± {len(invalid_symbols)} ä¸ªäº¤æ˜“å¯¹ä¸åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“åˆ—è¡¨ä¸­ï¼Œå·²è·³è¿‡")
            logging.info(f"âœ… å…± {len(valid_symbols)} ä¸ªæœ‰æ•ˆäº¤æ˜“å¯¹å°†è¿›è¡Œä¸‹è½½")
        
        all_symbols = valid_symbols
        
        if not all_symbols:
            logging.error("æ²¡æœ‰æœ‰æ•ˆçš„äº¤æ˜“å¯¹å¯ä»¥ä¸‹è½½")
            return
    
    logging.info(f"å…±æ‰¾åˆ° {len(all_symbols)} ä¸ªäº¤æ˜“å¯¹")
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    # å¦‚æœæä¾›äº†start_timeå’Œend_timeï¼Œä¼˜å…ˆä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤é€»è¾‘
    if end_time is None:
        now_utc = datetime.now(timezone.utc)
        if interval in ['1d', '3d', '1w', '1M']:
            # æ—¥çº¿åŠä»¥ä¸Š, é»˜è®¤ç»“æŸæ—¶é—´ä¸ºæ˜¨å¤©çš„ç»“æŸæ—¶é—´(ä¸åŒ…å«ä»Šå¤©)
            today = now_utc.replace(hour=0, minute=0, second=0, microsecond=0)
            end_time = today - timedelta(seconds=1)  # æ˜¨å¤©çš„23:59:59
        else:
            # å°æ—¶çº¿åŠä»¥ä¸‹, è®¾ç½®ä¸ºå½“å‰æ—¶é—´ä¹‹å‰çš„æœ€æ–°å®Œæ•´Kçº¿æ—¶é—´
            interval_seconds = calculate_interval_seconds(interval)
            current_timestamp = int(now_utc.timestamp())
            kline_index = current_timestamp // interval_seconds
            current_kline_start_timestamp = kline_index * interval_seconds
            latest_complete_kline_start_timestamp = current_kline_start_timestamp - interval_seconds
            end_time = datetime.fromtimestamp(latest_complete_kline_start_timestamp, tz=timezone.utc)
            logging.info(f"é»˜è®¤ç»“æŸæ—¶é—´è®¾ç½®ä¸ºæœ€æ–°å®Œæ•´Kçº¿æ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    
    if start_time is None:
        # å¦‚æœæ²¡æœ‰æä¾›start_timeï¼Œæ ¹æ®days_backè®¡ç®—
        if days_back:
            start_time = end_time - timedelta(days=days_back)
        # å¦‚æœdays_backä¹Ÿä¸ºNoneï¼Œåˆ™start_timeä¿æŒä¸ºNoneï¼ˆä¸‹è½½æ‰€æœ‰æ•°æ®ï¼‰
    
    # ä¸‹è½½æ¯ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®
    success_count = 0
    fail_count = 0
    
    for i, symbol in enumerate(all_symbols, 1):
        logging.info(f"[{i}/{len(all_symbols)}] å¤„ç†äº¤æ˜“å¯¹: {symbol}")
        if download_kline_data(
            symbol=symbol,
            interval=interval,
            start_time=start_time,
            end_time=end_time,
            limit=limit,
            update_existing=update_existing,
            auto_split=auto_split,
            request_delay=request_delay
        ):
            success_count += 1
        else:
            fail_count += 1
        
        # æ¯å¤„ç†æŒ‡å®šæ•°é‡çš„äº¤æ˜“å¯¹åæš‚åœï¼Œé¿å…è§¦å‘äº¤æ˜“æ‰€APIé™åˆ¶
        if i % batch_size == 0:
            logging.info(f"å·²å¤„ç† {i} ä¸ªäº¤æ˜“å¯¹, æš‚åœ {batch_delay} ç§’ä»¥é¿å…APIé™åˆ¶...")
            time.sleep(batch_delay)
    
    logging.info(f"ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")


def download_missing_symbols(
    interval: str = "1d",
    days_back: Optional[int] = None,
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    limit: Optional[int] = API_DATA_LIMIT,
    auto_split: bool = True,
    request_delay: float = DEFAULT_REQUEST_DELAY,
    batch_size: int = DEFAULT_BATCH_SIZE,
    batch_delay: float = DEFAULT_BATCH_DELAY
):
    """åªä¸‹è½½æœ¬åœ°æ•°æ®åº“ä¸­ç¼ºå¤±çš„äº¤æ˜“å¯¹æ•°æ®"""
    logging.info("=" * 80)
    logging.info(f"å¼€å§‹ä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹æ•°æ®ï¼Œé—´éš”: {interval}")
    if start_time:
        logging.info(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    if end_time:
        logging.info(f"ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    if days_back:
        logging.info(f"å›æº¯å¤©æ•°: {days_back}")
    logging.info("=" * 80)
    logging.info("æ­£åœ¨æ£€æŸ¥ç¼ºå¤±çš„äº¤æ˜“å¯¹...")
    
    # è·å–äº¤æ˜“æ‰€æ‰€æœ‰äº¤æ˜“å¯¹
    exchange_symbols = in_exchange_trading_symbols()
    if not exchange_symbols:
        logging.error("æ— æ³•è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨")
        return
    
    # è·å–æœ¬åœ°å·²æœ‰äº¤æ˜“å¯¹
    local_symbols = get_local_symbols(interval)
    
    # æ‰¾å‡ºç¼ºå¤±çš„äº¤æ˜“å¯¹
    missing_symbols = [s for s in exchange_symbols if s not in local_symbols]
    
    if not missing_symbols:
        logging.info("æ²¡æœ‰ç¼ºå¤±çš„äº¤æ˜“å¯¹")
        return
    
    logging.info(f"æ‰¾åˆ° {len(missing_symbols)} ä¸ªç¼ºå¤±çš„äº¤æ˜“å¯¹")
    
    # ä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹æ•°æ®
    download_all_symbols(
        interval=interval,
        days_back=days_back,
        start_time=start_time,
        end_time=end_time,
        limit=limit,
        update_existing=False,
        symbols=missing_symbols,
        auto_split=auto_split,
        request_delay=request_delay,
        batch_size=batch_size,
        batch_delay=batch_delay
    )


def _update_single_symbol(
    symbol: str, 
    i: int, 
    total: int, 
    interval: str, 
    end_time: datetime, 
    limit: Optional[int], 
    auto_split: bool, 
    request_delay: float
):
    """
    å¤„ç†å•ä¸ªäº¤æ˜“å¯¹çš„æ›´æ–°é€»è¾‘ï¼ˆç”¨äºå¤šçº¿ç¨‹å¹¶è¡Œï¼‰
    """
    try:
        logging.info(f"[{i}/{total}] å¤„ç†äº¤æ˜“å¯¹: {symbol}")
        
        # ğŸ”§ å…ˆæ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨äº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“
        is_valid = validate_symbol(symbol, skip_validation=False)
        if not is_valid:
            logging.info(f"â­ï¸  è·³è¿‡ {symbol}ï¼ˆå·²ä¸‹æ¶æˆ–æš‚åœäº¤æ˜“ï¼‰")
            return 'skipped', symbol
        
        # è·å–æœ€åæ›´æ–°æ—¥æœŸ
        last_trade_date = get_last_trade_date(symbol, interval)
        
        status = 'updated'
        if last_trade_date:
            # æœ‰æ•°æ®ï¼Œè®¡ç®—å¼€å§‹æ—¶é—´ï¼ˆæœ€åæ—¥æœŸçš„ä¸‹ä¸€ä¸ªKçº¿ï¼‰
            if interval in ['1d', '3d', '1w', '1M']:
                last_date_obj = datetime.strptime(last_trade_date, '%Y-%m-%d').date()
                if interval == '1d':
                    next_date = last_date_obj + timedelta(days=1)
                elif interval == '3d':
                    next_date = last_date_obj + timedelta(days=3)
                elif interval == '1w':
                    next_date = last_date_obj + timedelta(weeks=1)
                elif interval == '1M':
                    if last_date_obj.month == 12:
                        next_date = last_date_obj.replace(year=last_date_obj.year + 1, month=1)
                    else:
                        next_date = last_date_obj.replace(month=last_date_obj.month + 1)
                else:
                    next_date = last_date_obj + timedelta(days=1)
                start_time = datetime.combine(next_date, datetime.min.time()).replace(tzinfo=timezone.utc)
            else:
                last_datetime_obj = datetime.strptime(last_trade_date, '%Y-%m-%d %H:%M:%S')
                last_datetime_obj = ensure_utc_timezone(last_datetime_obj)
                interval_seconds = calculate_interval_seconds(interval)
                last_timestamp = int(last_datetime_obj.timestamp())
                next_timestamp = ((last_timestamp // interval_seconds) + 1) * interval_seconds
                start_time = datetime.fromtimestamp(next_timestamp, tz=timezone.utc)
            
            if compare_trade_dates(last_trade_date, end_time, interval):
                logging.info(f"{symbol} æ•°æ®å·²æ˜¯æœ€æ–° ({last_trade_date})")
                return 'no_data_needed', symbol
            
            logging.info(f"{symbol} æœ€åæ›´æ–°æ—¥æœŸ: {last_trade_date}, å¼€å§‹è¡¥å…¨æ•°æ®")
        else:
            start_time = end_time - timedelta(days=365)
            logging.info(f"{symbol} æ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œä» {start_time.strftime('%Y-%m-%d')} å¼€å§‹ä¸‹è½½")
            status = 'new'
        
        # ä¸‹è½½æ•°æ®
        success = download_kline_data(
            symbol=symbol,
            interval=interval,
            start_time=start_time,
            end_time=end_time,
            limit=limit,
            update_existing=False,
            auto_split=auto_split,
            request_delay=request_delay,
            skip_symbol_validation=True
        )
        
        if success:
            return status, symbol
        else:
            return 'failed', symbol
            
    except Exception as e:
        logging.error(f"å¤„ç† {symbol} å¤±è´¥: {e}")
        return 'failed', symbol


def auto_update_all_symbols(
    interval: str = "1d",
    limit: Optional[int] = API_DATA_LIMIT,
    auto_split: bool = True,
    request_delay: float = DEFAULT_REQUEST_DELAY,
    batch_size: int = DEFAULT_BATCH_SIZE,
    batch_delay: float = DEFAULT_BATCH_DELAY,
    max_workers: int = 1  # é»˜è®¤ 1 è¡¨ç¤ºä¿æŒåŸæœ‰å•çº¿ç¨‹è¡Œä¸º
):
    """
    è‡ªåŠ¨è¡¥å…¨æ‰€æœ‰äº¤æ˜“å¯¹çš„æ•°æ®ï¼šä»æœ€åæ›´æ–°æ—¥æœŸåˆ°ç°åœ¨
    
    åŠŸèƒ½ï¼š
    1. è·å–æŒ‡å®šintervalçš„æ‰€æœ‰äº¤æ˜“å¯¹
    2. å¯¹äºæ¯ä¸ªäº¤æ˜“å¯¹ï¼Œè·å–æœ€åæ›´æ–°æ—¥æœŸ
    3. ä»æœ€åæ›´æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©/ä¸‹ä¸€ä¸ªKçº¿å¼€å§‹ï¼Œè¡¥å…¨åˆ°å½“å‰æ—¶é—´
    4. å¯¹äºæ²¡æœ‰æ•°æ®çš„äº¤æ˜“å¯¹ï¼Œä»é»˜è®¤å¼€å§‹æ—¶é—´ä¸‹è½½
    
    Args:
        interval: Kçº¿é—´éš”
        limit: æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§æ¡æ•°
        auto_split: æ˜¯å¦è‡ªåŠ¨åˆ†æ®µä¸‹è½½
        request_delay: æ¯æ¬¡APIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        batch_size: æ¯å¤„ç†å¤šå°‘ä¸ªäº¤æ˜“å¯¹åæš‚åœ
        batch_delay: æ¯æ‰¹å¤„ç†åçš„æš‚åœæ—¶é—´ï¼ˆç§’ï¼‰
    """
    logging.info("=" * 80)
    logging.info(f"å¼€å§‹è‡ªåŠ¨è¡¥å…¨ {interval} æ•°æ®")
    logging.info("=" * 80)
    
    # å¯¼å…¥çº¿ç¨‹é”
    import threading
    stats_lock = threading.Lock()
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': 0,
        'updated': 0,
        'new': 0,
        'skipped': 0,
        'failed': 0,
        'no_data_needed': 0
    }
    
    # ä¿®æ”¹è¾…åŠ©å‡½æ•°ä»¥æ”¯æŒé”
    def safe_stats_increment(key, delta=1):
        with stats_lock:
            stats[key] += delta
    
    # åªè·å–äº¤æ˜“æ‰€çš„äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆå¿½ç•¥æœ¬åœ°å·²ä¸‹æ¶çš„äº¤æ˜“å¯¹ï¼‰
    # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨å¸¦ç¼“å­˜çš„ get_valid_trading_symbolsï¼Œé¿å…é‡å¤è¯·æ±‚ exchange_info
    try:
        exchange_symbols = get_valid_trading_symbols()
    except Exception as e:
        logging.error(f"è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨å¤±è´¥: {e}")
        logging.info("=" * 80)
        logging.info("è‡ªåŠ¨è¡¥å…¨å¤±è´¥ï¼šæ— æ³•è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨")
        logging.info("=" * 80)
        return stats
    
    if not exchange_symbols:
        logging.error("æ— æ³•è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆè¿”å›ç©ºåˆ—è¡¨ï¼‰")
        logging.info("=" * 80)
        logging.info("è‡ªåŠ¨è¡¥å…¨å¤±è´¥ï¼šæ— æ³•è·å–äº¤æ˜“æ‰€äº¤æ˜“å¯¹åˆ—è¡¨")
        logging.info("=" * 80)
        return stats
    
    all_symbols = exchange_symbols
    
    logging.info(f"å…±æ‰¾åˆ° {len(all_symbols)} ä¸ªäº¤æ˜“æ‰€æ­£å¸¸äº¤æ˜“çš„äº¤æ˜“å¯¹")
    
    if not all_symbols:
        logging.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•äº¤æ˜“å¯¹ï¼Œé€€å‡ºè‡ªåŠ¨è¡¥å…¨")
        logging.info("=" * 80)
        logging.info("è‡ªåŠ¨è¡¥å…¨å®Œæˆï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•äº¤æ˜“å¯¹")
        logging.info("=" * 80)
        return stats
    
    stats['total'] = len(all_symbols)
    
    # è®¡ç®—å½“å‰æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
    now = datetime.now(timezone.utc)
    if interval in ['1d', '3d', '1w', '1M']:
        # æ—¥çº¿åŠä»¥ä¸Šï¼Œä½¿ç”¨æ˜¨å¤©çš„ç»“æŸæ—¶é—´ï¼ˆä¸åŒ…å«ä»Šå¤©ï¼‰
        today = now.replace(hour=0, minute=0, second=0, microsecond=0)
        end_time = today - timedelta(seconds=1)  # æ˜¨å¤©çš„23:59:59
    else:
        # å°æ—¶çº¿åŠä»¥ä¸‹ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä¹‹å‰çš„æœ€æ–°å®Œæ•´Kçº¿æ—¶é—´
        interval_seconds = calculate_interval_seconds(interval)
        current_timestamp = int(now.timestamp())
        kline_index = current_timestamp // interval_seconds
        current_kline_start_timestamp = kline_index * interval_seconds
        latest_complete_kline_start_timestamp = current_kline_start_timestamp - interval_seconds
        end_time = datetime.fromtimestamp(latest_complete_kline_start_timestamp, tz=timezone.utc)
    
    logging.info(f"ç»“æŸæ—¶é—´è®¾ç½®ä¸º: {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    logging.info(f"å¼€å§‹å¤„ç† {len(all_symbols)} ä¸ªäº¤æ˜“å¯¹...")
    logging.info("")
    
    # å¤„ç†æ¯ä¸ªäº¤æ˜“å¯¹
    if max_workers <= 1:
        for i, symbol in enumerate(all_symbols, 1):
            status, _ = _update_single_symbol(symbol, i, len(all_symbols), interval, end_time, limit, auto_split, request_delay)
            if status in stats:
                stats[status] += 1
            
            # æ‰¹æ¬¡æš‚åœ (ä»…åœ¨å•çº¿ç¨‹æ¨¡å¼ä¸‹æœ‰æ•ˆ)
            if i % batch_size == 0:
                logging.info(f"å·²å¤„ç† {i} ä¸ªäº¤æ˜“å¯¹ï¼Œæš‚åœ {batch_delay} ç§’...")
                time.sleep(batch_delay)
    else:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        logging.info(f"ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹è¿›è¡Œå¹¶è¡Œæ›´æ–°...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(
                _update_single_symbol, 
                symbol, i, len(all_symbols), interval, end_time, limit, auto_split, request_delay
            ): symbol for i, symbol in enumerate(all_symbols, 1)}
            
            completed_count = 0
            for future in as_completed(futures):
                symbol = futures[future]
                completed_count += 1
                try:
                    status, _ = future.result()
                    if status in stats:
                        with stats_lock:
                            stats[status] += 1
                except Exception as e:
                    logging.error(f"å¹¶å‘å¤„ç† {symbol} æ—¶å‘ç”Ÿæœªæ•è·é”™è¯¯: {e}")
                    with stats_lock:
                        stats['failed'] += 1
                
                # æ¯10ä¸ªè¾“å‡ºä¸€æ¬¡è¿›åº¦
                if completed_count % 10 == 0:
                    with stats_lock:
                        logging.info(f"è¿›åº¦: {completed_count}/{len(all_symbols)} ({completed_count*100//len(all_symbols)}%) | æˆåŠŸ: {stats['updated']+stats['new']} | è·³è¿‡: {stats['skipped']} | æ— éœ€æ›´æ–°: {stats['no_data_needed']} | å¤±è´¥: {stats['failed']}")
    
    # è¾“å‡ºæœ€ç»ˆè¿›åº¦ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è¾“å‡ºè¿‡ï¼Œæˆ–è€…ä¸æ˜¯10çš„å€æ•°ï¼‰
    total_processed = stats['updated'] + stats['new'] + stats['no_data_needed'] + stats['skipped'] + stats['failed']
    if total_processed < len(all_symbols):
        logging.info(f"è¿›åº¦: {total_processed}/{len(all_symbols)} | æˆåŠŸ: {stats['updated']+stats['new']} | è·³è¿‡: {stats['skipped']} | æ— éœ€æ›´æ–°: {stats['no_data_needed']} | å¤±è´¥: {stats['failed']}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logging.info("")
    logging.info("=" * 80)
    logging.info("è‡ªåŠ¨è¡¥å…¨å®Œæˆï¼")
    logging.info("=" * 80)
    logging.info(f"æ€»äº¤æ˜“å¯¹æ•°: {stats['total']}")
    logging.info(f"âœ“ æ›´æ–°å·²æœ‰æ•°æ®: {stats['updated']}")
    logging.info(f"âœ“ æ–°å¢äº¤æ˜“å¯¹: {stats['new']}")
    logging.info(f"â—‹ æ— éœ€æ›´æ–°ï¼ˆæ•°æ®å·²æ˜¯æœ€æ–°ï¼‰: {stats['no_data_needed']}")
    logging.info(f"â­ï¸  è·³è¿‡ï¼ˆå·²ä¸‹æ¶æˆ–æš‚åœäº¤æ˜“ï¼‰: {stats['skipped']}")
    logging.info(f"âœ— å¤±è´¥: {stats['failed']}")
    logging.info(f"")
    logging.info(f"æ€»è®¡å¤„ç†: {stats['updated'] + stats['new'] + stats['no_data_needed'] + stats['skipped'] + stats['failed']} ä¸ªäº¤æ˜“å¯¹")
    logging.info("=" * 80)
    
    return stats


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¸‹è½½å¸å®‰Uæœ¬ä½åˆçº¦Kçº¿æ•°æ®')
    parser.add_argument(
        '--interval',
        type=str,
        default='1d',
        choices=['1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '1w', '1M'],
        help='Kçº¿é—´éš”(é»˜è®¤: 1d)'
    )
    parser.add_argument(
        '--days',
        type=int,
        default=None,
        help='å›æº¯å¤©æ•°(é»˜è®¤: None, ä¸‹è½½æ‰€æœ‰æ•°æ®), å¦‚æœæä¾›äº†--start-timeå’Œ--end-timeåˆ™å¿½ç•¥æ­¤å‚æ•°'
    )
    parser.add_argument(
        '--start-time',
        type=str,
        default=None,
        help='å¼€å§‹æ—¶é—´, æ ¼å¼: YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS (é»˜è®¤: None, æ ¹æ®--daysè®¡ç®—æˆ–ä¸‹è½½æ‰€æœ‰æ•°æ®)'
    )
    parser.add_argument(
        '--end-time',
        type=str,
        default=None,
        help='ç»“æŸæ—¶é—´, æ ¼å¼: YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS (é»˜è®¤: None, æ˜¨å¤©çš„ç»“æŸæ—¶é—´)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§æ¡æ•°(é»˜è®¤: None, è‡ªåŠ¨ä½¿ç”¨1500ã€‚å¦‚æœåªæä¾›start-timeå’Œend-timeä¼šè‡ªåŠ¨è®¡ç®—)'
    )
    parser.add_argument(
        '--auto-split',
        action='store_true',
        default=True,
        help='å½“æ•°æ®æ¡æ•°è¶…è¿‡é™åˆ¶æ—¶è‡ªåŠ¨åˆ†æ®µä¸‹è½½(é»˜è®¤: True)'
    )
    parser.add_argument(
        '--no-auto-split',
        action='store_false',
        dest='auto_split',
        help='ç¦ç”¨è‡ªåŠ¨åˆ†æ®µä¸‹è½½'
    )
    parser.add_argument(
        '--request-delay',
        type=float,
        default=0.1,
        help='æ¯æ¬¡APIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…é¢‘ç‡é™åˆ¶(é»˜è®¤: 0.1)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=30,
        help='æ¯å¤„ç†å¤šå°‘ä¸ªäº¤æ˜“å¯¹åæš‚åœ(é»˜è®¤: 30)'
    )
    parser.add_argument(
        '--batch-delay',
        type=float,
        default=3.0,
        help='æ¯æ‰¹å¤„ç†åçš„æš‚åœæ—¶é—´ï¼ˆç§’ï¼‰(é»˜è®¤: 3.0)'
    )
    parser.add_argument(
        '--update',
        action='store_true',
        help='æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®'
    )
    parser.add_argument(
        '--missing-only',
        action='store_true',
        help='åªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹'
    )
    parser.add_argument(
        '--symbols',
        type=str,
        nargs='+',
        help='æŒ‡å®šè¦ä¸‹è½½çš„äº¤æ˜“å¯¹åˆ—è¡¨, ä¾‹å¦‚: --symbols BTCUSDT ETHUSDT'
    )
    
    args = parser.parse_args()
    
    # è§£ææ—¶é—´å‚æ•°
    start_time = None
    end_time = None
    
    if args.start_time:
        try:
            # å°è¯•è§£ææ—¥æœŸæ—¶é—´æ ¼å¼
            if len(args.start_time) == 10:  # YYYY-MM-DD
                start_time = datetime.strptime(args.start_time, '%Y-%m-%d')
            else:  # YYYY-MM-DD HH:MM:SS
                start_time = datetime.strptime(args.start_time, '%Y-%m-%d %H:%M:%S')
        except ValueError as e:
            logging.error(f"å¼€å§‹æ—¶é—´æ ¼å¼é”™è¯¯: {args.start_time}, é”™è¯¯: {e}")
            logging.error("è¯·ä½¿ç”¨æ ¼å¼: YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS")
            sys.exit(1)
    
    if args.end_time:
        try:
            # å°è¯•è§£ææ—¥æœŸæ—¶é—´æ ¼å¼
            if len(args.end_time) == 10:  # YYYY-MM-DD
                end_time = datetime.strptime(args.end_time, '%Y-%m-%d')
                # å¦‚æœæ˜¯æ—¥æœŸæ ¼å¼ï¼Œè®¾ç½®ä¸ºå½“å¤©çš„23:59:59
                end_time = end_time.replace(hour=23, minute=59, second=59)
            else:  # YYYY-MM-DD HH:MM:SS
                end_time = datetime.strptime(args.end_time, '%Y-%m-%d %H:%M:%S')
        except ValueError as e:
            logging.error(f"ç»“æŸæ—¶é—´æ ¼å¼é”™è¯¯: {args.end_time}, é”™è¯¯: {e}")
            logging.error("è¯·ä½¿ç”¨æ ¼å¼: YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS")
            sys.exit(1)
    
    if args.missing_only:
        # åªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“å¯¹
        download_missing_symbols(
            interval=args.interval,
            days_back=args.days,
            start_time=start_time,
            end_time=end_time,
            limit=args.limit,
            auto_split=args.auto_split,
            request_delay=args.request_delay,
            batch_size=args.batch_size,
            batch_delay=args.batch_delay
        )
    else:
        # ä¸‹è½½æ‰€æœ‰æˆ–æŒ‡å®šçš„äº¤æ˜“å¯¹
        download_all_symbols(
            interval=args.interval,
            days_back=args.days,
            start_time=start_time,
            end_time=end_time,
            limit=args.limit,
            update_existing=args.update,
            symbols=args.symbols,
            auto_split=args.auto_split,
            request_delay=args.request_delay,
            batch_size=args.batch_size,
            batch_delay=args.batch_delay
        )

