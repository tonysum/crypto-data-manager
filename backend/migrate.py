#!/usr/bin/env python3
"""
SQLite åˆ° PostgreSQL æ•°æ®è¿ç§»è„šæœ¬

åŠŸèƒ½ï¼š
1. ä» SQLite æ•°æ®åº“è¯»å–æ‰€æœ‰è¡¨å’Œæ•°æ®
2. åœ¨ PostgreSQL ä¸­åˆ›å»ºå¯¹åº”çš„è¡¨ç»“æ„
3. è¿ç§»æ•°æ®åˆ° PostgreSQL
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆè®°å½•å·²è¿ç§»çš„è¡¨ï¼‰
5. æ”¯æŒæ‰¹é‡è¿ç§»å’Œè¿›åº¦æ˜¾ç¤º

ä½¿ç”¨æ–¹æ³•ï¼ˆæ¨èä½¿ç”¨ .env æ–‡ä»¶ï¼‰ï¼š
    1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œé…ç½® PostgreSQL è¿æ¥ä¿¡æ¯ï¼š
       PG_HOST=localhost
       PG_PORT=5432
       PG_DB=crypto_data
       PG_USER=crypto_user
       PG_PASSWORD=your_password
       SQLITE_PATH=data/crypto_data.db  # å¯é€‰
    
    2. ç›´æ¥è¿è¡Œï¼ˆä¼šè‡ªåŠ¨ä» .env è¯»å–é…ç½®ï¼‰ï¼š
       python migrate.py
    
    3. æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼ˆä¼šè¦†ç›– .env é…ç½®ï¼‰ï¼š
       python migrate.py --pg-host localhost --pg-password your_password
"""

import os
import sys
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

import sqlite3
import pandas as pd
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.pool import QueuePool
from dotenv import load_dotenv

# é…ç½®æ—¥å¿—ï¼ˆå…ˆé…ç½®ï¼Œä»¥ä¾¿åç»­æ—¥å¿—èƒ½æ­£å¸¸è¾“å‡ºï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ğŸ”§ åŠ è½½ .env æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ï¼Œå…¶æ¬¡ backend ç›®å½•ï¼‰
backend_dir = Path(__file__).parent
project_root = backend_dir.parent
env_path = project_root / '.env'
if not env_path.exists():
    env_path = backend_dir / '.env'

if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    logging.info(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
else:
    logging.warning(f"âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼ã€‚æŸ¥æ‰¾è·¯å¾„: {project_root / '.env'}")


class SQLiteToPostgreSQLMigrator:
    """SQLite åˆ° PostgreSQL æ•°æ®è¿ç§»å™¨"""
    
    def __init__(
        self,
        sqlite_path: str,
        pg_host: str = "localhost",
        pg_port: int = 5432,
        pg_db: str = "crypto_data",
        pg_user: str = "crypto_user",
        pg_password: str = "",
        batch_size: int = 10000
    ):
        """
        åˆå§‹åŒ–è¿ç§»å™¨
        
        Args:
            sqlite_path: SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            pg_host: PostgreSQL ä¸»æœºåœ°å€
            pg_port: PostgreSQL ç«¯å£
            pg_db: PostgreSQL æ•°æ®åº“å
            pg_user: PostgreSQL ç”¨æˆ·å
            pg_password: PostgreSQL å¯†ç 
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
        """
        self.sqlite_path = sqlite_path
        self.batch_size = batch_size
        self.migrated_tables = set()
        
        # è¿æ¥ SQLite
        if not os.path.exists(sqlite_path):
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            abs_path = os.path.abspath(sqlite_path)
            cwd = os.getcwd()
            raise FileNotFoundError(
                f"SQLite æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {sqlite_path}\n"
                f"  ç»å¯¹è·¯å¾„: {abs_path}\n"
                f"  å½“å‰å·¥ä½œç›®å½•: {cwd}\n"
                f"  è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ --sqlite-path å‚æ•°æŒ‡å®šå®Œæ•´è·¯å¾„"
            )
        
        self.sqlite_engine = create_engine(f'sqlite:///{sqlite_path}')
        logging.info(f"å·²è¿æ¥ SQLite æ•°æ®åº“: {sqlite_path}")
        
        # è¿æ¥ PostgreSQLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        pg_url = f"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}"
        max_retries = 3
        retry_delay = 5  # ç§’
        
        for attempt in range(max_retries):
            try:
                self.pg_engine = create_engine(
                    pg_url,
                    poolclass=QueuePool,
                    pool_size=5,
                    max_overflow=10,
                    pool_pre_ping=True,  # è‡ªåŠ¨æ£€æµ‹å¹¶é‡è¿æ–­å¼€çš„è¿æ¥
                    echo=False,
                    connect_args={
                        "connect_timeout": 10,  # è¿æ¥è¶…æ—¶10ç§’
                        "keepalives": 1,
                        "keepalives_idle": 30,
                        "keepalives_interval": 10,
                        "keepalives_count": 5
                    }
                )
                # æµ‹è¯•è¿æ¥
                with self.pg_engine.connect() as conn:
                    conn.execute(text("SELECT 1"))
                logging.info(f"âœ… å·²è¿æ¥ PostgreSQL æ•°æ®åº“: {pg_host}:{pg_port}/{pg_db}")
                break
            except Exception as e:
                if attempt < max_retries - 1:
                    logging.warning(f"âš ï¸  è¿æ¥ PostgreSQL å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    logging.info(f"    {retry_delay} ç§’åé‡è¯•...")
                    import time
                    time.sleep(retry_delay)
                else:
                    raise ConnectionError(
                        f"æ— æ³•è¿æ¥åˆ° PostgreSQL (å·²é‡è¯• {max_retries} æ¬¡): {e}\n"
                        f"  è¯·æ£€æŸ¥:\n"
                        f"  1. æ•°æ®åº“æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ\n"
                        f"  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                        f"  3. è¿æ¥å‚æ•°æ˜¯å¦æ­£ç¡® (ä¸»æœº: {pg_host}, ç«¯å£: {pg_port})"
                    )
    
    def get_sqlite_tables(self) -> List[str]:
        """è·å– SQLite ä¸­æ‰€æœ‰è¡¨å"""
        with self.sqlite_engine.connect() as conn:
            result = conn.execute(text(
                "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name"
            ))
            tables = [row[0] for row in result]
        return tables
    
    def get_postgresql_tables(self) -> List[str]:
        """è·å– PostgreSQL ä¸­æ‰€æœ‰è¡¨å"""
        with self.pg_engine.connect() as conn:
            result = conn.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                ORDER BY table_name
            """))
            tables = [row[0] for row in result]
        return tables
    
    def compare_table_counts(self, table_filter: Optional[str] = None) -> Dict:
        """
        å¯¹æ¯”SQLiteå’ŒPostgreSQLçš„è¡¨æ•°é‡
        
        Args:
            table_filter: è¡¨åè¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        # è·å–SQLiteè¡¨åˆ—è¡¨
        sqlite_tables = self.get_sqlite_tables()
        
        # åº”ç”¨è¿‡æ»¤
        if table_filter:
            if table_filter.startswith('K'):
                sqlite_tables = [t for t in sqlite_tables if t.startswith(table_filter)]
            else:
                sqlite_tables = [t for t in sqlite_tables if table_filter in t]
        
        # è·å–PostgreSQLè¡¨åˆ—è¡¨
        pg_tables = self.get_postgresql_tables()
        
        # åº”ç”¨ç›¸åŒçš„è¿‡æ»¤
        if table_filter:
            if table_filter.startswith('K'):
                pg_tables = [t for t in pg_tables if t.startswith(table_filter)]
            else:
                pg_tables = [t for t in pg_tables if table_filter in t]
        
        sqlite_count = len(sqlite_tables)
        pg_count = len(pg_tables)
        
        # æ‰¾å‡ºå·®å¼‚
        sqlite_set = set(sqlite_tables)
        pg_set = set(pg_tables)
        
        only_in_sqlite = sorted(sqlite_set - pg_set)
        only_in_pg = sorted(pg_set - sqlite_set)
        common = sorted(sqlite_set & pg_set)
        
        is_consistent = sqlite_count == pg_count and len(only_in_sqlite) == 0 and len(only_in_pg) == 0
        
        return {
            'sqlite_count': sqlite_count,
            'pg_count': pg_count,
            'is_consistent': is_consistent,
            'only_in_sqlite': only_in_sqlite,
            'only_in_pg': only_in_pg,
            'common_count': len(common),
            'sqlite_tables': sorted(sqlite_tables),
            'pg_tables': sorted(pg_tables)
        }
    
    def get_table_schema(self, table_name: str) -> Dict:
        """è·å– SQLite è¡¨çš„æ¶æ„ä¿¡æ¯"""
        with self.sqlite_engine.connect() as conn:
            # è·å–åˆ—ä¿¡æ¯
            result = conn.execute(text(f'PRAGMA table_info("{table_name}")'))
            columns = []
            for row in result:
                columns.append({
                    'name': row[1],
                    'type': row[2],
                    'not_null': row[3],
                    'default_value': row[4],
                    'primary_key': row[5]
                })
            
            # è·å–ä¸»é”®ä¿¡æ¯
            primary_keys = [col['name'] for col in columns if col['primary_key']]
            
            # è·å–è¡Œæ•°
            count_result = conn.execute(text(f'SELECT COUNT(*) FROM "{table_name}"'))
            row_count = count_result.scalar()
        
        return {
            'columns': columns,
            'primary_keys': primary_keys,
            'row_count': row_count
        }
    
    def sqlite_type_to_postgresql(self, sqlite_type: str, column_name: str = None) -> str:
        """
        å°† SQLite æ•°æ®ç±»å‹è½¬æ¢ä¸º PostgreSQL æ•°æ®ç±»å‹
        
        Args:
            sqlite_type: SQLite æ•°æ®ç±»å‹
            column_name: åˆ—åï¼ˆç”¨äºç‰¹æ®Šå¤„ç†ï¼Œå¦‚ has_added_positionï¼‰
        """
        sqlite_type_upper = sqlite_type.upper()
        
        # ğŸ”§ ç‰¹æ®Šå¤„ç†ï¼šæŸäº›åˆ—åæš—ç¤ºåº”è¯¥æ˜¯booleanç±»å‹
        if column_name and 'has_added_position' in column_name.lower():
            # å¦‚æœåˆ—ååŒ…å« has_added_positionï¼Œä¸”SQLiteä¸­æ˜¯INTEGERï¼Œæ£€æŸ¥PostgreSQLä¸­æ˜¯å¦å·²ç»æ˜¯boolean
            # è¿™é‡Œå…ˆè¿”å›INTEGERï¼Œå¦‚æœPostgreSQLä¸­å·²ç»æ˜¯booleanï¼Œä¼šåœ¨æ•°æ®è¿ç§»æ—¶è½¬æ¢
            pass  # ç»§ç»­æ­£å¸¸å¤„ç†
        
        # SQLite ç±»å‹æ˜ å°„åˆ° PostgreSQL
        type_mapping = {
            'INTEGER': 'BIGINT',
            'REAL': 'DOUBLE PRECISION',
            'TEXT': 'TEXT',
            'BLOB': 'BYTEA',
            'NUMERIC': 'NUMERIC',
            'BOOLEAN': 'BOOLEAN',
            'DATE': 'DATE',
            'DATETIME': 'TIMESTAMP',
            'TIMESTAMP': 'TIMESTAMP'
        }
        
        # å¤„ç†å¸¦é•¿åº¦çš„ç±»å‹ï¼ˆå¦‚ VARCHAR(255)ï¼‰
        if '(' in sqlite_type:
            base_type = sqlite_type_upper.split('(')[0].strip()
            length = sqlite_type.split('(')[1].split(')')[0]
            if base_type in ['VARCHAR', 'CHAR']:
                return f'VARCHAR({length})'
            elif base_type == 'DECIMAL':
                return f'NUMERIC({length})'
        
        # å¤„ç†å¸¸è§ç±»å‹
        for sqlite_key, pg_type in type_mapping.items():
            if sqlite_key in sqlite_type_upper:
                return pg_type
        
        # é»˜è®¤è¿”å› TEXT
        return 'TEXT'
    
    def create_postgresql_table(self, table_name: str, schema: Dict) -> bool:
        """åœ¨ PostgreSQL ä¸­åˆ›å»ºè¡¨"""
        # PostgreSQL è¡¨åéœ€è¦ç”¨å¼•å·åŒ…è£¹ï¼ˆä¿æŒå¤§å°å†™ï¼‰
        safe_table_name = f'"{table_name}"'
        
        try:
            with self.pg_engine.connect() as conn:
                # ä½¿ç”¨äº‹åŠ¡ç¡®ä¿æ“ä½œçš„åŸå­æ€§
                trans = conn.begin()
                try:
                    # ä½¿ç”¨ç³»ç»Ÿè¡¨æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆæ›´å®‰å…¨ï¼Œä¸ä¼šå¯¼è‡´äº‹åŠ¡å¤±è´¥ï¼‰
                    # PostgreSQL ä¸­ï¼Œå¦‚æœè¡¨åç”¨å¼•å·åˆ›å»ºï¼Œä¼šä¿æŒå¤§å°å†™ï¼›å¦åˆ™ä¼šè½¬æ¢ä¸ºå°å†™
                    # æ‰€ä»¥éœ€è¦æ£€æŸ¥ä¸¤ç§æƒ…å†µï¼šåŸå§‹å¤§å°å†™å’Œå°å†™
                    check_sql = text("""
                        SELECT EXISTS (
                            SELECT 1 
                            FROM information_schema.tables 
                            WHERE table_schema = 'public' 
                            AND (table_name = :table_name OR table_name = LOWER(:table_name))
                        )
                    """)
                    result = conn.execute(check_sql, {"table_name": table_name})
                    table_exists = result.scalar()
                    
                    if table_exists:
                        trans.commit()
                        logging.info(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                        return True
                    
                    # æ„å»º CREATE TABLE è¯­å¥
                    column_defs = []
                    for col in schema['columns']:
                        pg_type = self.sqlite_type_to_postgresql(col['type'], col['name'])
                        col_def = f'"{col["name"]}" {pg_type}'
                        
                        if col['not_null']:
                            col_def += ' NOT NULL'
                        
                        if col['default_value'] is not None:
                            default_val = col['default_value']
                            # å¤„ç†é»˜è®¤å€¼
                            if isinstance(default_val, str):
                                if default_val.upper() == 'CURRENT_TIMESTAMP':
                                    col_def += ' DEFAULT CURRENT_TIMESTAMP'
                                else:
                                    # è½¬ä¹‰å•å¼•å·
                                    escaped_val = default_val.replace("'", "''")
                                    col_def += f" DEFAULT '{escaped_val}'"
                            else:
                                col_def += f' DEFAULT {default_val}'
                        
                        column_defs.append(col_def)
                    
                    # æ·»åŠ ä¸»é”®çº¦æŸ
                    if schema['primary_keys']:
                        pk_cols = ', '.join([f'"{pk}"' for pk in schema['primary_keys']])
                        column_defs.append(f'PRIMARY KEY ({pk_cols})')
                    
                    create_sql = f"""
                        CREATE TABLE {safe_table_name} (
                            {', '.join(column_defs)}
                        );
                    """
                    
                    conn.execute(text(create_sql))
                    trans.commit()
                    logging.info(f"âœ… å·²åˆ›å»ºè¡¨: {table_name}")
                    return True
                    
                except Exception as e:
                    trans.rollback()
                    error_msg = str(e).lower()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
                    if ('connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 
                        'host is down' in error_msg or 'could not receive data' in error_msg or 
                        'operation timed out' in error_msg or 'server closed' in error_msg):
                        raise ConnectionError(f"åˆ›å»ºè¡¨æ—¶è¿æ¥å¤±è´¥: {e}")
                    # å…¶ä»–é”™è¯¯
                    logging.error(f"âŒ åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {e}")
                    import traceback
                    logging.debug(traceback.format_exc())
                    return False
        except ConnectionError:
            # é‡æ–°æŠ›å‡ºè¿æ¥é”™è¯¯
            raise
        except Exception as e:
            error_msg = str(e).lower()
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
            if ('connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 
                'host is down' in error_msg or 'could not receive data' in error_msg or 
                'operation timed out' in error_msg or 'server closed' in error_msg):
                raise ConnectionError(f"åˆ›å»ºè¡¨æ—¶è¿æ¥å¤±è´¥: {e}")
            # å…¶ä»–é”™è¯¯
            logging.error(f"âŒ åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {e}")
            import traceback
            logging.debug(traceback.format_exc())
            return False
    
    def migrate_table_data(self, table_name: str, schema: Dict) -> int:
        """è¿ç§»è¡¨æ•°æ®ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
        safe_table_name = f'"{table_name}"'
        row_count = schema['row_count']
        
        if row_count == 0:
            logging.info(f"è¡¨ {table_name} ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®è¿ç§»")
            return 0
        
        # ğŸ”§ æ£€æŸ¥å·²è¿ç§»çš„è¡Œæ•°ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        already_migrated = 0
        try:
            with self.pg_engine.connect() as conn:
                try:
                    count_result = conn.execute(text(f'SELECT COUNT(*) FROM {safe_table_name}'))
                    already_migrated = count_result.scalar()
                    if already_migrated > 0:
                        logging.info(f"ğŸ“Š è¡¨ {table_name} å·²å­˜åœ¨ {already_migrated:,} è¡Œæ•°æ®ï¼Œå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»...")
                except Exception as e:
                    # è¡¨ä¸å­˜åœ¨æˆ–æ— æ³•æŸ¥è¯¢ï¼Œä»å¤´å¼€å§‹
                    error_msg = str(e).lower()
                    # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œç«‹å³æŠ›å‡º
                    if 'connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 'host is down' in error_msg:
                        raise ConnectionError(f"æ£€æŸ¥å·²è¿ç§»æ•°æ®æ—¶è¿æ¥å¤±è´¥: {e}")
                    # å…¶ä»–é”™è¯¯ï¼ˆå¦‚è¡¨ä¸å­˜åœ¨ï¼‰ï¼Œä»å¤´å¼€å§‹
                    already_migrated = 0
        except ConnectionError:
            # é‡æ–°æŠ›å‡ºè¿æ¥é”™è¯¯
            raise
        except Exception as e:
            error_msg = str(e).lower()
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
            if 'connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 'host is down' in error_msg:
                raise ConnectionError(f"æ£€æŸ¥å·²è¿ç§»æ•°æ®æ—¶è¿æ¥å¤±è´¥: {e}")
            # å…¶ä»–é”™è¯¯ï¼Œä»å¤´å¼€å§‹
            logging.warning(f"âš ï¸  æ£€æŸ¥å·²è¿ç§»æ•°æ®æ—¶å‡ºé”™: {e}ï¼Œå°†ä»å¤´å¼€å§‹è¿ç§»")
            already_migrated = 0
        
        if already_migrated >= row_count:
            logging.info(f"âœ… è¡¨ {table_name} æ•°æ®å·²å®Œå…¨è¿ç§» ({already_migrated:,}/{row_count:,} è¡Œ)ï¼Œè·³è¿‡")
            return already_migrated
        
        logging.info(f"å¼€å§‹è¿ç§»è¡¨ {table_name}ï¼Œå…± {row_count:,} è¡Œ (å·²è¿ç§»: {already_migrated:,}, å‰©ä½™: {row_count - already_migrated:,})...")
        
        # åˆ†æ‰¹è¯»å–å’Œæ’å…¥ï¼ˆä»æ–­ç‚¹ç»§ç»­ï¼‰
        migrated_rows = already_migrated
        offset = already_migrated
        
        while offset < row_count:
            try:
                # ä» SQLite è¯»å–æ•°æ®
                with self.sqlite_engine.connect() as sqlite_conn:
                    query = f'SELECT * FROM "{table_name}" LIMIT {self.batch_size} OFFSET {offset}'
                    df = pd.read_sql(query, sqlite_conn)
                
                if df.empty:
                    break
                
                # ğŸ”§ æ•°æ®ç±»å‹è½¬æ¢ï¼šæ£€æŸ¥PostgreSQLè¡¨ç»“æ„ï¼Œè½¬æ¢éœ€è¦çš„æ•°æ®ç±»å‹
                with self.pg_engine.connect() as pg_conn:
                    # è·å–PostgreSQLè¡¨çš„åˆ—ä¿¡æ¯
                    column_info = pg_conn.execute(text("""
                        SELECT column_name, data_type 
                        FROM information_schema.columns 
                        WHERE table_schema = 'public' 
                        AND table_name = :table_name
                    """), {"table_name": table_name})
                    
                    pg_columns = {row[0]: row[1] for row in column_info}
                    
                    # è½¬æ¢booleanç±»å‹çš„åˆ—ï¼ˆSQLiteä¸­å¯èƒ½æ˜¯integerï¼Œéœ€è¦è½¬æ¢ä¸ºbooleanï¼‰
                    for col_name, pg_type in pg_columns.items():
                        if pg_type == 'boolean' and col_name in df.columns:
                            # å°†integer (0/1) è½¬æ¢ä¸ºboolean (False/True)
                            if df[col_name].dtype in ['int64', 'int32', 'int', 'Int64']:
                                # å°†0è½¬æ¢ä¸ºFalseï¼Œ1è½¬æ¢ä¸ºTrueï¼Œå…¶ä»–å€¼ä¿æŒåŸæ ·æˆ–è½¬æ¢ä¸ºNone
                                df[col_name] = df[col_name].apply(
                                    lambda x: bool(x) if pd.notna(x) else None
                                )
                            elif df[col_name].dtype in ['float64', 'float32', 'float']:
                                # å¤„ç†æµ®ç‚¹æ•°ç±»å‹ï¼ˆå¯èƒ½æ˜¯NaNï¼‰
                                df[col_name] = df[col_name].apply(
                                    lambda x: bool(int(x)) if pd.notna(x) else None
                                )
                            elif df[col_name].dtype == 'object':
                                # å¤„ç†å¯èƒ½ä¸ºNoneæˆ–å­—ç¬¦ä¸²çš„æƒ…å†µ
                                df[col_name] = df[col_name].apply(
                                    lambda x: bool(int(x)) if pd.notna(x) and str(x).isdigit() else (bool(x) if pd.notna(x) and x != '' else None)
                                )
                            logging.debug(f"å·²å°†åˆ— {col_name} ä» {df[col_name].dtype} è½¬æ¢ä¸º boolean")
                
                # å†™å…¥ PostgreSQL
                # ä½¿ç”¨ to_sql æ‰¹é‡æ’å…¥ï¼ˆä¼šè‡ªåŠ¨æäº¤ï¼‰
                df.to_sql(
                    name=table_name,
                    con=self.pg_engine,
                    if_exists='append',
                    index=False,
                    method='multi',
                    chunksize=1000
                )
                
                migrated_rows += len(df)
                offset += self.batch_size
                
                if migrated_rows % 10000 == 0 or offset >= row_count:
                    progress_pct = (migrated_rows / row_count * 100) if row_count > 0 else 0
                    logging.info(f"  è¿›åº¦: {migrated_rows:,}/{row_count:,} è¡Œ ({progress_pct:.1f}%)...")
            
            except Exception as e:
                error_msg = str(e).lower()
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºé‡å¤æ•°æ®ï¼ˆå¯èƒ½æ˜¯å¹¶å‘æ’å…¥æˆ–éƒ¨åˆ†æ•°æ®å·²å­˜åœ¨ï¼‰
                if 'duplicate key' in error_msg or 'unique constraint' in error_msg:
                    logging.warning(f"  âš ï¸  æ£€æµ‹åˆ°é‡å¤æ•°æ® (offset={offset})ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡...")
                    # è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹
                    offset += self.batch_size
                    continue
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
                elif ('connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 
                      'host is down' in error_msg or 'could not receive data' in error_msg or 
                      'operation timed out' in error_msg or 'server closed' in error_msg):
                    logging.error(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯ (offset={offset}): {e}")
                    logging.info(f"   å·²è¿ç§» {migrated_rows:,}/{row_count:,} è¡Œ")
                    logging.info(f"   è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥ï¼Œé‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
                    raise ConnectionError(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                else:
                    logging.error(f"âŒ è¿ç§»è¡¨ {table_name} æ•°æ®å¤±è´¥ (offset={offset}): {e}")
                    raise
        
        logging.info(f"âœ… è¡¨ {table_name} è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_rows:,} è¡Œ (æœ¬æ¬¡æ–°å¢: {migrated_rows - already_migrated:,} è¡Œ)")
        return migrated_rows
    
    def migrate_table(self, table_name: str, skip_existing: bool = True) -> bool:
        """è¿ç§»å•ä¸ªè¡¨ï¼ˆåŒ…æ‹¬ç»“æ„å’Œæ•°æ®ï¼‰"""
        # æ£€æŸ¥è¡¨æ˜¯å¦å·²è¿ç§»
        if skip_existing:
            safe_table_name = f'"{table_name}"'
            try:
                with self.pg_engine.connect() as conn:
                    try:
                        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                        check_sql = text("""
                            SELECT EXISTS (
                                SELECT 1 
                                FROM information_schema.tables 
                                WHERE table_schema = 'public' 
                                AND (table_name = :table_name OR table_name = LOWER(:table_name))
                            )
                        """)
                        table_exists = conn.execute(check_sql, {"table_name": table_name}).scalar()
                        
                        if table_exists:
                            # è·å–å·²è¿ç§»çš„è¡Œæ•°
                            count_result = conn.execute(text(f'SELECT COUNT(*) FROM {safe_table_name}'))
                            pg_row_count = count_result.scalar()
                            
                            # è·å–æºè¡¨çš„è¡Œæ•°
                            schema = self.get_table_schema(table_name)
                            sqlite_row_count = schema['row_count']
                            
                            if pg_row_count >= sqlite_row_count:
                                logging.info(f"â­ï¸  è¡¨ {table_name} å·²å®Œå…¨è¿ç§» ({pg_row_count:,}/{sqlite_row_count:,} è¡Œ)ï¼Œè·³è¿‡")
                                return True
                            else:
                                logging.info(f"ğŸ”„ è¡¨ {table_name} éƒ¨åˆ†è¿ç§» ({pg_row_count:,}/{sqlite_row_count:,} è¡Œ)ï¼Œå°†ç»§ç»­è¿ç§»å‰©ä½™æ•°æ®")
                    except Exception as e:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
                        error_msg = str(e).lower()
                        if ('connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 
                            'host is down' in error_msg or 'could not receive data' in error_msg or 
                            'operation timed out' in error_msg or 'server closed' in error_msg):
                            raise ConnectionError(f"æ£€æŸ¥è¡¨çŠ¶æ€æ—¶è¿æ¥å¤±è´¥: {e}")
                        # è¡¨ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å¤±è´¥ï¼Œç»§ç»­è¿ç§»
                        logging.debug(f"æ£€æŸ¥è¡¨ {table_name} æ—¶å‡ºé”™: {e}ï¼Œå°†åˆ›å»ºæ–°è¡¨")
            except ConnectionError:
                # é‡æ–°æŠ›å‡ºè¿æ¥é”™è¯¯
                raise
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
                error_msg = str(e).lower()
                if ('connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 
                    'host is down' in error_msg or 'could not receive data' in error_msg or 
                    'operation timed out' in error_msg or 'server closed' in error_msg):
                    raise ConnectionError(f"æ£€æŸ¥è¡¨çŠ¶æ€æ—¶è¿æ¥å¤±è´¥: {e}")
                # å…¶ä»–é”™è¯¯ï¼Œè®°å½•ä½†ç»§ç»­å°è¯•
                logging.warning(f"âš ï¸  æ£€æŸ¥è¡¨ {table_name} çŠ¶æ€æ—¶å‡ºé”™: {e}ï¼Œå°†ç»§ç»­å°è¯•è¿ç§»")
        
        try:
            # è·å–è¡¨ç»“æ„
            schema = self.get_table_schema(table_name)
            
            # åˆ›å»ºè¡¨
            if not self.create_postgresql_table(table_name, schema):
                return False
            
            # è¿ç§»æ•°æ®
            self.migrate_table_data(table_name, schema)
            
            self.migrated_tables.add(table_name)
            return True
        
        except Exception as e:
            logging.error(f"âŒ è¿ç§»è¡¨ {table_name} å¤±è´¥: {e}")
            return False
    
    def migrate_all(
        self, 
        table_filter: Optional[str] = None,
        table_names: Optional[List[str]] = None,
        skip_existing: bool = True
    ) -> Dict:
        """
        è¿ç§»æ‰€æœ‰è¡¨
        
        Args:
            table_filter: è¡¨åè¿‡æ»¤å­—ç¬¦ä¸²ï¼ˆæ”¯æŒå‰ç¼€åŒ¹é…æˆ–åŒ…å«åŒ¹é…ï¼‰
            table_names: æŒ‡å®šè¦è¿ç§»çš„è¡¨ååˆ—è¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼Œä¼˜å…ˆçº§é«˜äºtable_filterï¼‰
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„è¡¨
        """
        # è·å–æ‰€æœ‰è¡¨
        all_tables = self.get_sqlite_tables()
        
        # å¦‚æœæŒ‡å®šäº†è¡¨ååˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨ç²¾ç¡®åŒ¹é…
        if table_names:
            # ç²¾ç¡®åŒ¹é…æŒ‡å®šçš„è¡¨å
            filtered_tables = []
            for table_name in table_names:
                if table_name in all_tables:
                    filtered_tables.append(table_name)
                else:
                    logging.warning(f"âš ï¸  è¡¨ '{table_name}' ä¸å­˜åœ¨äºSQLiteæ•°æ®åº“ä¸­ï¼Œè·³è¿‡")
            all_tables = filtered_tables
            logging.info(f"æŒ‡å®šè¿ç§» {len(all_tables)} ä¸ªè¡¨: {', '.join(all_tables)}")
        # å¦åˆ™ä½¿ç”¨è¿‡æ»¤å­—ç¬¦ä¸²
        elif table_filter:
            if table_filter.startswith('K'):
                # Kçº¿è¡¨è¿‡æ»¤ï¼šåŒ¹é…ä»¥è¯¥å‰ç¼€å¼€å¤´çš„æ‰€æœ‰è¡¨
                all_tables = [t for t in all_tables if t.startswith(table_filter)]
                logging.info(f"ä½¿ç”¨å‰ç¼€è¿‡æ»¤ '{table_filter}'ï¼Œæ‰¾åˆ° {len(all_tables)} ä¸ªè¡¨")
            else:
                # åŒ…å«åŒ¹é…ï¼šåŒ¹é…åŒ…å«è¯¥å­—ç¬¦ä¸²çš„æ‰€æœ‰è¡¨
                all_tables = [t for t in all_tables if table_filter in t]
                logging.info(f"ä½¿ç”¨åŒ…å«è¿‡æ»¤ '{table_filter}'ï¼Œæ‰¾åˆ° {len(all_tables)} ä¸ªè¡¨")
        
        total_tables = len(all_tables)
        if total_tables == 0:
            logging.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦è¿ç§»çš„è¡¨")
            return {
                'total_tables': 0,
                'success_count': 0,
                'fail_count': 0,
                'total_rows': 0,
                'duration_seconds': 0
            }
        
        logging.info(f"æ‰¾åˆ° {total_tables} ä¸ªè¡¨éœ€è¦è¿ç§»")
        
        success_count = 0
        fail_count = 0
        total_rows = 0
        
        start_time = datetime.now()
        
        for i, table_name in enumerate(all_tables, 1):
            logging.info(f"\n[{i}/{total_tables}] å¤„ç†è¡¨: {table_name}")
            
            try:
                if self.migrate_table(table_name, skip_existing=skip_existing):
                    success_count += 1
                    schema = self.get_table_schema(table_name)
                    total_rows += schema['row_count']
                else:
                    fail_count += 1
            except ConnectionError as e:
                # è¿æ¥é”™è¯¯ï¼šåœæ­¢è¿ç§»ï¼Œæç¤ºç”¨æˆ·ä¿®å¤è¿æ¥åé‡æ–°è¿è¡Œ
                logging.error(f"âŒ å¤„ç†è¡¨ {table_name} æ—¶æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                logging.warning(f"\nâš ï¸  è¿ç§»å·²åœæ­¢åœ¨è¡¨: {table_name} ({i}/{total_tables})")
                logging.info(f"ğŸ’¡ å·²æˆåŠŸè¿ç§» {success_count} ä¸ªè¡¨ï¼Œå¤±è´¥ {fail_count} ä¸ªè¡¨")
                logging.info(f"ğŸ’¡ ä¿®å¤æ•°æ®åº“è¿æ¥åï¼Œé‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸»å‡½æ•°å¤„ç†
            except ConnectionError:
                # è¿æ¥é”™è¯¯ï¼šç«‹å³åœæ­¢è¿ç§»
                logging.error(f"âŒ å¤„ç†è¡¨ {table_name} æ—¶æ•°æ®åº“è¿æ¥å¤±è´¥")
                logging.warning(f"\nâš ï¸  è¿ç§»å·²åœæ­¢åœ¨è¡¨: {table_name} ({i}/{total_tables})")
                logging.info(f"ğŸ’¡ å·²æˆåŠŸè¿ç§» {success_count} ä¸ªè¡¨ï¼Œå¤±è´¥ {fail_count} ä¸ªè¡¨")
                logging.info(f"ğŸ’¡ ä¿®å¤æ•°æ®åº“è¿æ¥åï¼Œé‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸»å‡½æ•°å¤„ç†
            except Exception as e:
                error_msg = str(e).lower()
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥ç›¸å…³é”™è¯¯
                if ('connection' in error_msg or 'network' in error_msg or 'timeout' in error_msg or 
                    'host is down' in error_msg or 'could not receive data' in error_msg or 
                    'operation timed out' in error_msg or 'server closed' in error_msg):
                    logging.error(f"âŒ å¤„ç†è¡¨ {table_name} æ—¶å‘ç”Ÿè¿æ¥é”™è¯¯: {e}")
                    logging.warning(f"\nâš ï¸  è¿ç§»å·²åœæ­¢åœ¨è¡¨: {table_name} ({i}/{total_tables})")
                    logging.info(f"ğŸ’¡ å·²æˆåŠŸè¿ç§» {success_count} ä¸ªè¡¨ï¼Œå¤±è´¥ {fail_count} ä¸ªè¡¨")
                    logging.info(f"ğŸ’¡ ä¿®å¤æ•°æ®åº“è¿æ¥åï¼Œé‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
                    raise ConnectionError(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                else:
                    # å…¶ä»–é”™è¯¯ï¼šè®°å½•ä½†ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨
                    logging.error(f"âŒ å¤„ç†è¡¨ {table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    fail_count += 1
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨
                    continue
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            'total_tables': total_tables,
            'success_count': success_count,
            'fail_count': fail_count,
            'total_rows': total_rows,
            'duration_seconds': duration
        }


def main():
    """ä¸»å‡½æ•°"""
    # ğŸ”§ è·å–é¡¹ç›®è·¯å¾„å’Œ .env è·¯å¾„ï¼ˆä¸æ–‡ä»¶å¼€å¤´ä¿æŒä¸€è‡´ï¼‰
    backend_dir = Path(__file__).parent
    project_root = backend_dir.parent
    env_path = project_root / '.env'
    if not env_path.exists():
        env_path = backend_dir / '.env'
    
    # ğŸ”§ ä¼˜å…ˆä» .env æ–‡ä»¶è¯»å–é…ç½®ï¼ˆå·²åœ¨æ–‡ä»¶å¼€å¤´åŠ è½½ï¼‰
    pg_host = os.getenv('PG_HOST', '')
    pg_port = int(os.getenv('PG_PORT', ''))
    pg_db = os.getenv('PG_DB', '')
    pg_user = os.getenv('PG_USER', '')
    pg_password = os.getenv('PG_PASSWORD', '')
    sqlite_path_env = os.getenv('SQLITE_PATH', '')
    
    parser = argparse.ArgumentParser(
        description='SQLite åˆ° PostgreSQL æ•°æ®è¿ç§»å·¥å…·ï¼ˆä¼˜å…ˆä½¿ç”¨ .env æ–‡ä»¶é…ç½®ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
é…ç½®ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
  1. å‘½ä»¤è¡Œå‚æ•°
  2. .env æ–‡ä»¶ï¼ˆ{'å·²æ‰¾åˆ°: ' + str(env_path) if env_path.exists() else 'æœªæ‰¾åˆ°'}ï¼‰
  3. é»˜è®¤å€¼

å½“å‰é…ç½®ï¼ˆä» .env æˆ–é»˜è®¤å€¼ï¼‰ï¼š
  SQLite è·¯å¾„: {sqlite_path_env or 'data/crypto_data.db (é»˜è®¤)'}
  PostgreSQL ä¸»æœº: {pg_host}
  PostgreSQL ç«¯å£: {pg_port}
  PostgreSQL æ•°æ®åº“: {pg_db}
  PostgreSQL ç”¨æˆ·: {pg_user}
  PostgreSQL å¯†ç : {'å·²è®¾ç½®' if pg_password else 'æœªè®¾ç½®ï¼ˆéœ€è¦æä¾›ï¼‰'}
        """
    )
    
    parser.add_argument(
        '--sqlite-path',
        type=str,
        default=sqlite_path_env if sqlite_path_env else None,
        help=f'SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {sqlite_path_env or "data/crypto_data.db"} æˆ–ä» .env è¯»å–ï¼‰'
    )
    parser.add_argument(
        '--pg-host',
        type=str,
        default=pg_host,
        help=f'PostgreSQL ä¸»æœºåœ°å€ï¼ˆé»˜è®¤: {pg_host}ï¼Œä» .env æˆ–é»˜è®¤å€¼ï¼‰'
    )
    parser.add_argument(
        '--pg-port',
        type=int,
        default=pg_port,
        help=f'PostgreSQL ç«¯å£ï¼ˆé»˜è®¤: {pg_port}ï¼Œä» .env æˆ–é»˜è®¤å€¼ï¼‰'
    )
    parser.add_argument(
        '--pg-db',
        type=str,
        default=pg_db,
        help=f'PostgreSQL æ•°æ®åº“åï¼ˆé»˜è®¤: {pg_db}ï¼Œä» .env æˆ–é»˜è®¤å€¼ï¼‰'
    )
    parser.add_argument(
        '--pg-user',
        type=str,
        default=pg_user,
        help=f'PostgreSQL ç”¨æˆ·åï¼ˆé»˜è®¤: {pg_user}ï¼Œä» .env æˆ–é»˜è®¤å€¼ï¼‰'
    )
    parser.add_argument(
        '--pg-password',
        type=str,
        default=pg_password,
        help='PostgreSQL å¯†ç ï¼ˆé»˜è®¤: ä» .env è¯»å–ï¼Œå¦‚æœæœªè®¾ç½®åˆ™æç¤ºè¾“å…¥ï¼‰'
    )
    parser.add_argument(
        '--table-filter',
        type=str,
        default=None,
        help='è¡¨åè¿‡æ»¤ï¼ˆä¾‹å¦‚: K1d åªè¿ç§»æ—¥çº¿è¡¨ï¼ŒK5m åªè¿ç§»5åˆ†é’Ÿè¡¨ï¼Œbacktrade åŒ¹é…åŒ…å«backtradeçš„è¡¨ï¼‰'
    )
    parser.add_argument(
        '--tables',
        type=str,
        nargs='+',
        default=None,
        help='æŒ‡å®šè¦è¿ç§»çš„è¡¨ååˆ—è¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼Œå¤šä¸ªè¡¨åç”¨ç©ºæ ¼åˆ†éš”ï¼Œä¾‹å¦‚: --tables backtrade_records K1dBTCUSDTï¼‰'
    )
    parser.add_argument(
        '--table-file',
        type=str,
        default=None,
        help='ä»æ–‡ä»¶è¯»å–è¦è¿ç§»çš„è¡¨ååˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªè¡¨åï¼‰'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10000,
        help='æ‰¹é‡æ’å…¥å¤§å°ï¼ˆé»˜è®¤: 10000ï¼‰'
    )
    parser.add_argument(
        '--no-skip-existing',
        action='store_true',
        help='ä¸è·³è¿‡å·²å­˜åœ¨çš„è¡¨ï¼ˆä¼šé‡æ–°è¿ç§»ï¼‰'
    )
    parser.add_argument(
        '--compare-only',
        action='store_true',
        help='ä»…å¯¹æ¯”SQLiteå’ŒPostgreSQLçš„è¡¨æ•°é‡ï¼Œä¸æ‰§è¡Œè¿ç§»'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®š SQLite è·¯å¾„ï¼ˆç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
    if not args.sqlite_path:
        args.sqlite_path = str(project_root / "data" / "crypto_data.db")
    else:
        # å¦‚æœè·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
        sqlite_path_obj = Path(args.sqlite_path)
        if not sqlite_path_obj.is_absolute():
            # ç›¸å¯¹è·¯å¾„ï¼šå…ˆå°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
            abs_path = project_root / args.sqlite_path
            if not abs_path.exists():
                # å¦‚æœé¡¹ç›®æ ¹ç›®å½•ä¸‹ä¸å­˜åœ¨ï¼Œå°è¯•ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
                abs_path = Path(args.sqlite_path).resolve()
            args.sqlite_path = str(abs_path)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logging.info("=" * 80)
    logging.info("æ•°æ®è¿ç§»é…ç½®")
    logging.info("=" * 80)
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„å¹¶æ˜¾ç¤º
    sqlite_abs_path = os.path.abspath(args.sqlite_path)
    logging.info(f"SQLite æ•°æ®åº“: {args.sqlite_path}")
    logging.info(f"SQLite ç»å¯¹è·¯å¾„: {sqlite_abs_path}")
    logging.info(f"PostgreSQL ä¸»æœº: {args.pg_host}:{args.pg_port}")
    logging.info(f"PostgreSQL æ•°æ®åº“: {args.pg_db}")
    logging.info(f"PostgreSQL ç”¨æˆ·: {args.pg_user}")
    
    # æ£€æŸ¥é…ç½®æ¥æº
    config_source = []
    if env_path.exists():
        config_source.append(f".env æ–‡ä»¶ ({env_path})")
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†éé»˜è®¤å€¼ï¼ˆå¯èƒ½æ˜¯ä» .env æˆ–å‘½ä»¤è¡Œå‚æ•°ï¼‰
    if args.pg_host != 'localhost' or args.pg_port != 5432 or args.pg_db != 'crypto_data' or args.pg_user != 'crypto_user':
        if not env_path.exists():
            config_source.append("ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°")
    if not config_source:
        config_source.append("é»˜è®¤å€¼")
    
    logging.info(f"é…ç½®æ¥æº: {', '.join(config_source)}")
    
    # å¦‚æœæ²¡æœ‰æä¾›å¯†ç ï¼Œæç¤ºè¾“å…¥
    if not args.pg_password:
        import getpass
        logging.warning("âš ï¸  PostgreSQL å¯†ç æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        args.pg_password = getpass.getpass("è¯·è¾“å…¥ PostgreSQL å¯†ç : ")
    else:
        logging.info("âœ… PostgreSQL å¯†ç å·²ä» .env æ–‡ä»¶åŠ è½½")
    
    # åˆ›å»ºè¿ç§»å™¨
    try:
        migrator = SQLiteToPostgreSQLMigrator(
            sqlite_path=args.sqlite_path,
            pg_host=args.pg_host,
            pg_port=args.pg_port,
            pg_db=args.pg_db,
            pg_user=args.pg_user,
            pg_password=args.pg_password,
            batch_size=args.batch_size
        )
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–è¿ç§»å™¨å¤±è´¥: {e}")
        sys.exit(1)
    
    # å¤„ç†è¡¨ååˆ—è¡¨
    table_names = None
    if args.tables:
        # ä»å‘½ä»¤è¡Œå‚æ•°è·å–è¡¨ååˆ—è¡¨
        table_names = args.tables
        logging.info(f"ä»å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šäº† {len(table_names)} ä¸ªè¡¨: {', '.join(table_names)}")
    elif args.table_file:
        # ä»æ–‡ä»¶è¯»å–è¡¨ååˆ—è¡¨
        table_file_path = Path(args.table_file)
        if not table_file_path.is_absolute():
            # ç›¸å¯¹è·¯å¾„ï¼šå…ˆå°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
            table_file_path = project_root / args.table_file
            if not table_file_path.exists():
                # å¦‚æœé¡¹ç›®æ ¹ç›®å½•ä¸‹ä¸å­˜åœ¨ï¼Œå°è¯•ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
                table_file_path = Path(args.table_file).resolve()
        
        if not table_file_path.exists():
            logging.error(f"âŒ è¡¨ååˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {table_file_path}")
            sys.exit(1)
        
        try:
            with open(table_file_path, 'r', encoding='utf-8') as f:
                table_names = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
            logging.info(f"ä»æ–‡ä»¶ {table_file_path} è¯»å–äº† {len(table_names)} ä¸ªè¡¨å")
        except Exception as e:
            logging.error(f"âŒ è¯»å–è¡¨ååˆ—è¡¨æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    # å¦‚æœåŒæ—¶æŒ‡å®šäº† --tables/--table-file å’Œ --table-filterï¼Œæç¤ºç”¨æˆ·
    if (args.tables or args.table_file) and args.table_filter:
        logging.warning("âš ï¸  åŒæ—¶æŒ‡å®šäº† --tables/--table-file å’Œ --table-filterï¼Œå°†ä¼˜å…ˆä½¿ç”¨ --tables/--table-fileï¼ˆç²¾ç¡®åŒ¹é…ï¼‰")
    
    # å¦‚æœåªæ˜¯å¯¹æ¯”è¡¨æ•°é‡ï¼Œæ‰§è¡Œå¯¹æ¯”åé€€å‡º
    if args.compare_only:
        logging.info("=" * 80)
        logging.info("å¯¹æ¯”SQLiteå’ŒPostgreSQLçš„è¡¨æ•°é‡")
        logging.info("=" * 80)
        
        try:
            comparison = migrator.compare_table_counts(table_filter=args.table_filter)
            
            logging.info("\n" + "=" * 80)
            logging.info("å¯¹æ¯”ç»“æœ")
            logging.info("=" * 80)
            logging.info(f"SQLite è¡¨æ•°é‡: {comparison['sqlite_count']}")
            logging.info(f"PostgreSQL è¡¨æ•°é‡: {comparison['pg_count']}")
            logging.info(f"å…±åŒè¡¨æ•°é‡: {comparison['common_count']}")
            
            if comparison['is_consistent']:
                logging.info("âœ… è¡¨æ•°é‡ä¸€è‡´ï¼")
            else:
                logging.warning("âš ï¸  è¡¨æ•°é‡ä¸ä¸€è‡´ï¼")
                
                if comparison['only_in_sqlite']:
                    logging.info(f"\nä»…åœ¨SQLiteä¸­çš„è¡¨ ({len(comparison['only_in_sqlite'])} ä¸ª):")
                    for table in comparison['only_in_sqlite'][:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                        logging.info(f"  - {table}")
                    if len(comparison['only_in_sqlite']) > 20:
                        logging.info(f"  ... è¿˜æœ‰ {len(comparison['only_in_sqlite']) - 20} ä¸ªè¡¨")
                
                if comparison['only_in_pg']:
                    logging.info(f"\nä»…åœ¨PostgreSQLä¸­çš„è¡¨ ({len(comparison['only_in_pg'])} ä¸ª):")
                    for table in comparison['only_in_pg'][:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                        logging.info(f"  - {table}")
                    if len(comparison['only_in_pg']) > 20:
                        logging.info(f"  ... è¿˜æœ‰ {len(comparison['only_in_pg']) - 20} ä¸ªè¡¨")
            
            # è¿”å›é€‚å½“çš„é€€å‡ºç 
            sys.exit(0 if comparison['is_consistent'] else 1)
            
        except Exception as e:
            logging.error(f"\nâŒ å¯¹æ¯”è¡¨æ•°é‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logging.debug(traceback.format_exc())
            sys.exit(1)
    
    # æ‰§è¡Œè¿ç§»
    logging.info("=" * 80)
    logging.info("å¼€å§‹æ•°æ®è¿ç§»")
    logging.info("=" * 80)
    
    try:
        results = migrator.migrate_all(
            table_filter=args.table_filter,
            table_names=table_names,
            skip_existing=not args.no_skip_existing
        )
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logging.info("\n" + "=" * 80)
        logging.info("è¿ç§»å®Œæˆï¼")
        logging.info("=" * 80)
        logging.info(f"æ€»è¡¨æ•°: {results['total_tables']}")
        logging.info(f"æˆåŠŸ: {results['success_count']}")
        logging.info(f"å¤±è´¥: {results['fail_count']}")
        logging.info(f"æ€»è¡Œæ•°: {results['total_rows']:,}")
        logging.info(f"è€—æ—¶: {results['duration_seconds']:.2f} ç§’")
        
        if results['fail_count'] > 0:
            logging.warning(f"âš ï¸  æœ‰ {results['fail_count']} ä¸ªè¡¨è¿ç§»å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            sys.exit(1)
    
    except KeyboardInterrupt:
        logging.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­è¿ç§»")
        logging.info("ğŸ’¡ æç¤º: é‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»ï¼ˆå·²è¿ç§»çš„è¡¨å’Œæ•°æ®ä¼šè¢«è·³è¿‡ï¼‰")
        sys.exit(1)
    except ConnectionError as e:
        logging.error(f"\nâŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
        logging.info("\nğŸ’¡ å¤„ç†å»ºè®®:")
        logging.info("  1. æ£€æŸ¥æ•°æ®åº“æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ")
        logging.info("  2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        logging.info("  3. æ£€æŸ¥è¿æ¥å‚æ•°æ˜¯å¦æ­£ç¡®")
        logging.info("  4. ä¿®å¤è¿æ¥é—®é¢˜åï¼Œé‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
        sys.exit(1)
    except Exception as e:
        logging.error(f"\nâŒ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logging.debug(traceback.format_exc())
        error_msg = str(e).lower()
        if 'connection' in error_msg or 'network' in error_msg:
            logging.info("\nğŸ’¡ è¿™å¯èƒ½æ˜¯æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œä¿®å¤åé‡æ–°è¿è¡Œç¨‹åºå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
        sys.exit(1)


if __name__ == "__main__":
    main()
