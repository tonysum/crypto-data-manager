"""
PostgreSQL åˆ° PostgreSQL æ•°æ®è¿ç§»è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»æœ¬åœ° PostgreSQL æ•°æ®åº“å¯¼å‡ºæ•°æ®
2. å¯¼å…¥åˆ°äº‘æœåŠ¡å™¨ PostgreSQL æ•°æ®åº“
3. æ”¯æŒè¡¨è¿‡æ»¤ã€æ•°æ®éªŒè¯ã€å¢é‡è¿ç§»ç­‰

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä½¿ç”¨ pg_dump/pg_restoreï¼ˆæ¨èï¼Œé€Ÿåº¦å¿«ï¼‰ï¼š
   python migrate_pg_to_pg.py --method dump

2. ä½¿ç”¨ Python è„šæœ¬ï¼ˆæ›´çµæ´»ï¼Œæ”¯æŒè¿‡æ»¤ï¼‰ï¼š
   python migrate_pg_to_pg.py --method python

3. åªè¿ç§»ç‰¹å®šè¡¨ï¼š
   python migrate_pg_to_pg.py --tables K1dBTCUSDT K1dETHUSDT

4. åªè¿ç§»Kçº¿æ•°æ®è¡¨ï¼š
   python migrate_pg_to_pg.py --table-filter K1d

5. æ¯”è¾ƒä¸¤ä¸ªæ•°æ®åº“çš„è¡¨æ•°é‡ï¼š
   python migrate_pg_to_pg.py --compare-only
"""

import os
import sys
import logging
import argparse
import subprocess
import time
from pathlib import Path
from typing import List, Optional, Dict, Set
from datetime import datetime
from urllib.parse import quote_plus
import pandas as pd
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.pool import QueuePool
from sqlalchemy.exc import OperationalError, DisconnectionError
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ .env æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ï¼Œå…¶æ¬¡ backend ç›®å½•ï¼‰
backend_dir = Path(__file__).parent
env_path = project_root / '.env'
if not env_path.exists():
    env_path = backend_dir / '.env'
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    logging.info(f"âœ… å·²åŠ è½½ .env æ–‡ä»¶: {env_path}")
else:
    logging.warning(f"âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼ã€‚æŸ¥æ‰¾è·¯å¾„: {project_root / '.env'}")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


class PostgreSQLToPostgreSQLMigrator:
    """PostgreSQL åˆ° PostgreSQL æ•°æ®è¿ç§»å™¨"""
    
    def __init__(
        self,
        source_host: str,
        source_port: int,
        source_db: str,
        source_user: str,
        source_password: str,
        target_host: str,
        target_port: int,
        target_db: str,
        target_user: str,
        target_password: str,
        batch_size: int = 10000
    ):
        """
        åˆå§‹åŒ–è¿ç§»å™¨
        
        Args:
            source_host: æºæ•°æ®åº“ä¸»æœºåœ°å€
            source_port: æºæ•°æ®åº“ç«¯å£
            source_db: æºæ•°æ®åº“å
            source_user: æºæ•°æ®åº“ç”¨æˆ·å
            source_password: æºæ•°æ®åº“å¯†ç 
            target_host: ç›®æ ‡æ•°æ®åº“ä¸»æœºåœ°å€
            target_port: ç›®æ ‡æ•°æ®åº“ç«¯å£
            target_db: ç›®æ ‡æ•°æ®åº“å
            target_user: ç›®æ ‡æ•°æ®åº“ç”¨æˆ·å
            target_password: ç›®æ ‡æ•°æ®åº“å¯†ç 
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
        """
        self.batch_size = batch_size
        self.migrated_tables = set()
        
        # æ„å»ºè¿æ¥URLï¼ˆå¯¹å¯†ç è¿›è¡ŒURLç¼–ç ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
        if source_password:
            encoded_source_password = quote_plus(source_password)
            source_url = f"postgresql://{source_user}:{encoded_source_password}@{source_host}:{source_port}/{source_db}"
        else:
            source_url = f"postgresql://{source_user}@{source_host}:{source_port}/{source_db}"
        
        if target_password:
            encoded_target_password = quote_plus(target_password)
            target_url = f"postgresql://{target_user}:{encoded_target_password}@{target_host}:{target_port}/{target_db}"
        else:
            target_url = f"postgresql://{target_user}@{target_host}:{target_port}/{target_db}"
        
        # è¿æ¥æºæ•°æ®åº“
        logging.info(f"æ­£åœ¨è¿æ¥æºæ•°æ®åº“: {source_host}:{source_port}/{source_db}")
        self.source_engine = create_engine(
            source_url,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            connect_args={
                "connect_timeout": 10,
                "keepalives": 1,
                "keepalives_idle": 30,
                "keepalives_interval": 10,
                "keepalives_count": 5
            }
        )
        
        # æµ‹è¯•æºæ•°æ®åº“è¿æ¥
        try:
            with self.source_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logging.info(f"âœ… å·²è¿æ¥æºæ•°æ®åº“: {source_host}:{source_port}/{source_db}")
        except Exception as e:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°æºæ•°æ®åº“: {e}")
        
        # è¿æ¥ç›®æ ‡æ•°æ®åº“
        logging.info(f"æ­£åœ¨è¿æ¥ç›®æ ‡æ•°æ®åº“: {target_host}:{target_port}/{target_db}")
        self.target_engine = create_engine(
            target_url,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            connect_args={
                "connect_timeout": 10,
                "keepalives": 1,
                "keepalives_idle": 30,
                "keepalives_interval": 10,
                "keepalives_count": 5
            }
        )
        
        # æµ‹è¯•ç›®æ ‡æ•°æ®åº“è¿æ¥
        try:
            with self.target_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logging.info(f"âœ… å·²è¿æ¥ç›®æ ‡æ•°æ®åº“: {target_host}:{target_port}/{target_db}")
        except Exception as e:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°ç›®æ ‡æ•°æ®åº“: {e}")
        
        # ä¿å­˜è¿æ¥ä¿¡æ¯ï¼ˆç”¨äº pg_dumpï¼‰
        self.source_config = {
            'host': source_host,
            'port': source_port,
            'db': source_db,
            'user': source_user,
            'password': source_password
        }
        self.target_config = {
            'host': target_host,
            'port': target_port,
            'db': target_db,
            'user': target_user,
            'password': target_password
        }
    
    def get_source_tables(self) -> List[str]:
        """è·å–æºæ•°æ®åº“ä¸­æ‰€æœ‰è¡¨å"""
        with self.source_engine.connect() as conn:
            result = conn.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """))
            tables = [row[0] for row in result.fetchall()]
        return tables
    
    def get_target_tables(self) -> List[str]:
        """è·å–ç›®æ ‡æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨å"""
        with self.target_engine.connect() as conn:
            result = conn.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """))
            tables = [row[0] for row in result.fetchall()]
        return tables
    
    def get_table_row_count(self, engine, table_name: str) -> int:
        """è·å–è¡¨çš„è¡Œæ•°"""
        try:
            safe_table_name = f'"{table_name}"'
            with engine.connect() as conn:
                result = conn.execute(text(f'SELECT COUNT(*) FROM {safe_table_name}'))
                return result.fetchone()[0]
        except Exception as e:
            logging.warning(f"è·å–è¡¨ {table_name} è¡Œæ•°å¤±è´¥: {e}")
            return 0
    
    def compare_table_counts(self, table_filter: Optional[str] = None) -> Dict:
        """å¯¹æ¯”æºæ•°æ®åº“å’Œç›®æ ‡æ•°æ®åº“çš„è¡¨æ•°é‡"""
        source_tables = self.get_source_tables()
        target_tables = self.get_target_tables()
        
        if table_filter:
            if table_filter.startswith('K'):
                source_tables = [t for t in source_tables if t.startswith(table_filter)]
                target_tables = [t for t in target_tables if t.startswith(table_filter)]
            else:
                source_tables = [t for t in source_tables if table_filter in t]
                target_tables = [t for t in target_tables if table_filter in t]
        
        source_count = len(source_tables)
        target_count = len(target_tables)
        
        # æ‰¾å‡ºå·®å¼‚
        source_set = set(source_tables)
        target_set = set(target_tables)
        
        only_in_source = sorted(source_set - target_set)
        only_in_target = sorted(target_set - source_set)
        common = sorted(source_set & target_set)
        
        # å¯¹æ¯”å…±åŒè¡¨çš„è¡Œæ•°
        row_count_diff = {}
        for table in common:
            source_rows = self.get_table_row_count(self.source_engine, table)
            target_rows = self.get_table_row_count(self.target_engine, table)
            if source_rows != target_rows:
                row_count_diff[table] = {
                    'source': source_rows,
                    'target': target_rows,
                    'diff': source_rows - target_rows
                }
        
        is_consistent = (
            source_count == target_count and 
            len(only_in_source) == 0 and 
            len(only_in_target) == 0 and
            len(row_count_diff) == 0
        )
        
        return {
            'source_count': source_count,
            'target_count': target_count,
            'is_consistent': is_consistent,
            'only_in_source': only_in_source,
            'only_in_target': only_in_target,
            'common_count': len(common),
            'row_count_diff': row_count_diff,
            'source_tables': sorted(source_tables),
            'target_tables': sorted(target_tables)
        }
    
    def migrate_table_schema(self, table_name: str) -> bool:
        """è¿ç§»è¡¨ç»“æ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # æ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
            with self.target_engine.connect() as conn:
                result = conn.execute(
                    text("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_schema = 'public' 
                            AND table_name = :table_name
                        );
                    """),
                    {"table_name": table_name}
                )
                table_exists = result.fetchone()[0]
            
            if table_exists:
                logging.info(f"è¡¨ {table_name} å·²å­˜åœ¨äºç›®æ ‡æ•°æ®åº“")
                return True
            
            # ä»æºæ•°æ®åº“è·å–è¡¨ç»“æ„
            inspector = inspect(self.source_engine)
            columns = inspector.get_columns(table_name)
            primary_keys = inspector.get_pk_constraint(table_name)
            
            # æ„å»º CREATE TABLE è¯­å¥
            safe_table_name = f'"{table_name}"'
            column_defs = []
            
            for col in columns:
                col_name = col['name']
                col_type = str(col['type'])
                
                # è½¬æ¢æ•°æ®ç±»å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if 'VARCHAR' in col_type or 'TEXT' in col_type:
                    pg_type = 'TEXT'
                elif 'INTEGER' in col_type or 'INT' in col_type:
                    pg_type = 'INTEGER'
                elif 'BIGINT' in col_type:
                    pg_type = 'BIGINT'
                elif 'REAL' in col_type or 'FLOAT' in col_type or 'DOUBLE' in col_type:
                    pg_type = 'DOUBLE PRECISION'
                elif 'BOOLEAN' in col_type:
                    pg_type = 'BOOLEAN'
                elif 'TIMESTAMP' in col_type or 'DATETIME' in col_type:
                    pg_type = 'TIMESTAMP'
                else:
                    pg_type = col_type
                
                nullable = 'NULL' if col.get('nullable', True) else 'NOT NULL'
                column_defs.append(f'"{col_name}" {pg_type} {nullable}')
            
            # æ·»åŠ ä¸»é”®çº¦æŸ
            if primary_keys.get('constrained_columns'):
                pk_cols = ', '.join([f'"{col}"' for col in primary_keys['constrained_columns']])
                column_defs.append(f'PRIMARY KEY ({pk_cols})')
            
            create_sql = f'CREATE TABLE {safe_table_name} (\n    ' + ',\n    '.join(column_defs) + '\n);'
            
            # åœ¨ç›®æ ‡æ•°æ®åº“åˆ›å»ºè¡¨
            with self.target_engine.connect() as conn:
                conn.execute(text(create_sql))
                conn.commit()
            
            logging.info(f"âœ… å·²åˆ›å»ºè¡¨ç»“æ„: {table_name}")
            return True
            
        except Exception as e:
            logging.error(f"âŒ è¿ç§»è¡¨ç»“æ„å¤±è´¥ {table_name}: {e}")
            return False
    
    def migrate_table_data(
        self,
        table_name: str,
        skip_existing: bool = False
    ) -> Dict:
        """è¿ç§»è¡¨æ•°æ®"""
        safe_table_name = f'"{table_name}"'
        
        try:
            # æ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
            with self.target_engine.connect() as conn:
                result = conn.execute(
                    text("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_schema = 'public' 
                            AND table_name = :table_name
                        );
                    """),
                    {"table_name": table_name}
                )
                if not result.fetchone()[0]:
                    # å…ˆè¿ç§»è¡¨ç»“æ„
                    if not self.migrate_table_schema(table_name):
                        return {'success': False, 'rows': 0, 'error': 'æ— æ³•åˆ›å»ºè¡¨ç»“æ„'}
            
            # è·å–æºè¡¨è¡Œæ•°
            source_rows = self.get_table_row_count(self.source_engine, table_name)
            if source_rows == 0:
                logging.info(f"è¡¨ {table_name} æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡")
                return {'success': True, 'rows': 0}
            
            logging.info(f"å¼€å§‹è¿ç§»è¡¨ {table_name}ï¼Œå…± {source_rows} è¡Œ")
            
            # å¦‚æœè·³è¿‡å·²å­˜åœ¨çš„æ•°æ®ï¼Œå…ˆè·å–ç›®æ ‡è¡¨ä¸­å·²å­˜åœ¨çš„é”®
            existing_keys = set()
            if skip_existing:
                try:
                    # å‡è®¾ä¸»é”®æ˜¯ trade_dateï¼ˆKçº¿è¡¨ï¼‰æˆ– idï¼ˆå…¶ä»–è¡¨ï¼‰
                    with self.target_engine.connect() as conn:
                        result = conn.execute(text(f'SELECT trade_date FROM {safe_table_name}'))
                        existing_keys = {row[0] for row in result.fetchall()}
                    logging.info(f"ç›®æ ‡è¡¨ä¸­å·²æœ‰ {len(existing_keys)} æ¡è®°å½•")
                except:
                    # å¦‚æœæ²¡æœ‰ trade_date åˆ—ï¼Œå°è¯• id
                    try:
                        with self.target_engine.connect() as conn:
                            result = conn.execute(text(f'SELECT id FROM {safe_table_name}'))
                            existing_keys = {row[0] for row in result.fetchall()}
                    except:
                        pass
            
            # åˆ†æ‰¹è¯»å–å’Œæ’å…¥æ•°æ®
            migrated_rows = 0
            offset = 0
            max_retries = 3
            retry_delay = 2.0  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            
            while offset < source_rows:
                # ä»æºæ•°æ®åº“è¯»å–ä¸€æ‰¹æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
                limit = min(self.batch_size, source_rows - offset)
                df = None
                
                for retry in range(max_retries + 1):
                    try:
                        with self.source_engine.connect() as conn:
                            query = f'SELECT * FROM {safe_table_name} ORDER BY trade_date LIMIT {limit} OFFSET {offset}'
                            df = pd.read_sql(query, conn)
                        break  # æˆåŠŸè¯»å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    except (OperationalError, DisconnectionError) as e:
                        error_msg = str(e).lower()
                        is_network_error = (
                            'connection' in error_msg or 
                            'network' in error_msg or 
                            'timeout' in error_msg or
                            'could not translate host' in error_msg or
                            'could not receive data' in error_msg or
                            'server closed' in error_msg or
                            'connection refused' in error_msg
                        )
                        
                        if is_network_error and retry < max_retries:
                            wait_time = retry_delay * (2 ** retry)  # æŒ‡æ•°é€€é¿
                            logging.warning(f"  ç½‘ç»œé”™è¯¯ï¼ˆå°è¯• {retry + 1}/{max_retries + 1}ï¼‰: {str(e)[:100]}")
                            logging.info(f"  ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                            time.sleep(wait_time)
                            continue
                        else:
                            # ä¸æ˜¯ç½‘ç»œé”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                            raise
                
                if df is None or df.empty:
                    break
                
                # è¿‡æ»¤å·²å­˜åœ¨çš„æ•°æ®
                if skip_existing and existing_keys:
                    if 'trade_date' in df.columns:
                        df = df[~df['trade_date'].isin(existing_keys)]
                    elif 'id' in df.columns:
                        df = df[~df['id'].isin(existing_keys)]
                
                if not df.empty:
                    # æ’å…¥åˆ°ç›®æ ‡æ•°æ®åº“ï¼ˆå¸¦é‡è¯•ï¼‰
                    for retry in range(max_retries + 1):
                        try:
                            df.to_sql(
                                name=table_name,
                                con=self.target_engine,
                                if_exists='append',
                                index=False,
                                method='multi',
                                chunksize=min(1000, len(df))
                            )
                            migrated_rows += len(df)
                            
                            # æ›´æ–°å·²å­˜åœ¨çš„é”®é›†åˆ
                            if skip_existing:
                                if 'trade_date' in df.columns:
                                    existing_keys.update(df['trade_date'].tolist())
                                elif 'id' in df.columns:
                                    existing_keys.update(df['id'].tolist())
                            break  # æˆåŠŸæ’å…¥ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        except (OperationalError, DisconnectionError) as e:
                            error_msg = str(e).lower()
                            is_network_error = (
                                'connection' in error_msg or 
                                'network' in error_msg or 
                                'timeout' in error_msg or
                                'could not translate host' in error_msg or
                                'could not receive data' in error_msg or
                                'server closed' in error_msg or
                                'connection refused' in error_msg
                            )
                            
                            if is_network_error and retry < max_retries:
                                wait_time = retry_delay * (2 ** retry)  # æŒ‡æ•°é€€é¿
                                logging.warning(f"  ç½‘ç»œé”™è¯¯ï¼ˆå°è¯• {retry + 1}/{max_retries + 1}ï¼‰: {str(e)[:100]}")
                                logging.info(f"  ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                                time.sleep(wait_time)
                                continue
                            else:
                                # ä¸æ˜¯ç½‘ç»œé”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                raise
                
                offset += limit
                
                if offset % (self.batch_size * 10) == 0 or offset >= source_rows:
                    logging.info(f"  è¿›åº¦: {offset}/{source_rows} ({offset*100//source_rows}%)ï¼Œå·²è¿ç§»: {migrated_rows} è¡Œ")
            
            logging.info(f"âœ… è¡¨ {table_name} è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_rows} è¡Œ")
            return {'success': True, 'rows': migrated_rows}
            
        except (OperationalError, DisconnectionError) as e:
            error_msg = str(e).lower()
            is_network_error = (
                'connection' in error_msg or 
                'network' in error_msg or 
                'timeout' in error_msg or
                'could not translate host' in error_msg or
                'could not receive data' in error_msg or
                'server closed' in error_msg or
                'connection refused' in error_msg
            )
            
            logging.error(f"âŒ è¿ç§»è¡¨ {table_name} æ•°æ®å¤±è´¥: {e}")
            if is_network_error:
                logging.warning(f"âš ï¸  è¿™æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯ï¼ˆå·²è¿ç§» {migrated_rows} è¡Œï¼Œè¿›åº¦: {offset}/{source_rows}ï¼‰")
                logging.info(f"ğŸ’¡ ä¿®å¤ç½‘ç»œè¿æ¥åï¼Œä½¿ç”¨ --skip-existing å‚æ•°é‡æ–°è¿è¡Œå°†ä»æ–­ç‚¹ç»§ç»­è¿ç§»")
            import traceback
            logging.error(traceback.format_exc())
            return {'success': False, 'rows': migrated_rows, 'error': str(e), 'is_network_error': is_network_error}
        except Exception as e:
            logging.error(f"âŒ è¿ç§»è¡¨ {table_name} æ•°æ®å¤±è´¥: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return {'success': False, 'rows': migrated_rows, 'error': str(e)}
    
    def migrate_all(
        self,
        table_filter: Optional[str] = None,
        tables: Optional[List[str]] = None,
        table_file: Optional[str] = None,
        skip_existing: bool = False
    ) -> Dict:
        """è¿ç§»æ‰€æœ‰è¡¨"""
        # è·å–è¦è¿ç§»çš„è¡¨åˆ—è¡¨
        if tables:
            tables_to_migrate = tables
        elif table_file:
            with open(table_file, 'r') as f:
                tables_to_migrate = [line.strip() for line in f if line.strip()]
        else:
            tables_to_migrate = self.get_source_tables()
            if table_filter:
                if table_filter.startswith('K'):
                    tables_to_migrate = [t for t in tables_to_migrate if t.startswith(table_filter)]
                else:
                    tables_to_migrate = [t for t in tables_to_migrate if table_filter in t]
        
        logging.info(f"å‡†å¤‡è¿ç§» {len(tables_to_migrate)} ä¸ªè¡¨")
        
        stats = {
            'total': len(tables_to_migrate),
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'total_rows': 0
        }
        
        for i, table_name in enumerate(tables_to_migrate, 1):
            logging.info(f"\n[{i}/{len(tables_to_migrate)}] è¿ç§»è¡¨: {table_name}")
            
            result = self.migrate_table_data(table_name, skip_existing=skip_existing)
            
            if result['success']:
                stats['success'] += 1
                stats['total_rows'] += result['rows']
                if result['rows'] == 0:
                    stats['skipped'] += 1
            else:
                stats['failed'] += 1
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ï¼Œè®°å½•å¹¶æç¤º
                if result.get('is_network_error'):
                    logging.warning(f"âš ï¸  è¡¨ {table_name} å› ç½‘ç»œé”™è¯¯ä¸­æ–­ï¼Œå·²è¿ç§» {result['rows']} è¡Œ")
                    logging.info(f"ğŸ’¡ ä¿®å¤ç½‘ç»œåä½¿ç”¨ --skip-existing é‡æ–°è¿è¡Œå¯ç»§ç»­è¿ç§»æ­¤è¡¨")
        
        logging.info("\n" + "=" * 80)
        logging.info("è¿ç§»å®Œæˆï¼")
        logging.info("=" * 80)
        logging.info(f"æ€»è¡¨æ•°: {stats['total']}")
        logging.info(f"âœ“ æˆåŠŸ: {stats['success']}")
        logging.info(f"âœ— å¤±è´¥: {stats['failed']}")
        logging.info(f"â—‹ è·³è¿‡ï¼ˆæ— æ•°æ®ï¼‰: {stats['skipped']}")
        logging.info(f"æ€»è¿ç§»è¡Œæ•°: {stats['total_rows']}")
        logging.info("=" * 80)
        
        return stats
    
    def migrate_with_pg_dump(self, tables: Optional[List[str]] = None) -> bool:
        """ä½¿ç”¨ pg_dump å’Œ pg_restore è¿ç§»ï¼ˆæ¨èæ–¹æ³•ï¼Œé€Ÿåº¦å¿«ï¼‰"""
        try:
            import tempfile
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            dump_file = tempfile.NamedTemporaryFile(mode='w+b', suffix='.sql', delete=False)
            dump_path = dump_file.name
            dump_file.close()
            
            try:
                # æ„å»º pg_dump å‘½ä»¤
                dump_cmd = [
                    'pg_dump',
                    '-h', self.source_config['host'],
                    '-p', str(self.source_config['port']),
                    '-U', self.source_config['user'],
                    '-d', self.source_config['db'],
                    '-F', 'c',  # è‡ªå®šä¹‰æ ¼å¼ï¼ˆå‹ç¼©ï¼‰
                    '-f', dump_path
                ]
                
                # å¦‚æœæŒ‡å®šäº†è¡¨ï¼Œåªå¯¼å‡ºè¿™äº›è¡¨
                if tables:
                    for table in tables:
                        dump_cmd.extend(['-t', table])
                
                # è®¾ç½®å¯†ç ç¯å¢ƒå˜é‡
                env = os.environ.copy()
                if self.source_config['password']:
                    env['PGPASSWORD'] = self.source_config['password']
                
                logging.info(f"æ­£åœ¨å¯¼å‡ºæ•°æ®åˆ°: {dump_path}")
                logging.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(dump_cmd)}")
                
                result = subprocess.run(
                    dump_cmd,
                    env=env,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                logging.info("âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ")
                
                # æ„å»º pg_restore å‘½ä»¤
                restore_cmd = [
                    'pg_restore',
                    '-h', self.target_config['host'],
                    '-p', str(self.target_config['port']),
                    '-U', self.target_config['user'],
                    '-d', self.target_config['db'],
                    '--clean',  # æ¸…ç†ç›®æ ‡æ•°æ®åº“ä¸­çš„å¯¹è±¡
                    '--if-exists',  # å¦‚æœå¯¹è±¡ä¸å­˜åœ¨ä¹Ÿä¸æŠ¥é”™
                    dump_path
                ]
                
                # è®¾ç½®å¯†ç ç¯å¢ƒå˜é‡
                if self.target_config['password']:
                    env['PGPASSWORD'] = self.target_config['password']
                
                logging.info(f"æ­£åœ¨å¯¼å…¥æ•°æ®åˆ°ç›®æ ‡æ•°æ®åº“...")
                logging.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(restore_cmd)}")
                
                result = subprocess.run(
                    restore_cmd,
                    env=env,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                logging.info("âœ… æ•°æ®å¯¼å…¥æˆåŠŸ")
                return True
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(dump_path):
                    os.unlink(dump_path)
                    logging.info(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {dump_path}")
                    
        except subprocess.CalledProcessError as e:
            logging.error(f"âŒ pg_dump/pg_restore å¤±è´¥: {e}")
            if e.stderr:
                logging.error(f"é”™è¯¯è¾“å‡º: {e.stderr}")
            return False
        except Exception as e:
            logging.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return False


def main():
    # ä» .env æ–‡ä»¶è¯»å–æœ¬åœ°æ•°æ®åº“é…ç½®ï¼ˆæºæ•°æ®åº“ï¼‰
    source_host_default = os.getenv('PG_HOST', 'localhost')
    source_port_default = int(os.getenv('PG_PORT', '5432'))
    source_db_default = os.getenv('PG_DB', 'crypto_data')
    source_user_default = os.getenv('PG_USER', 'postgres')
    source_password_default = os.getenv('PG_PASSWORD', '')
    
    parser = argparse.ArgumentParser(description='PostgreSQL åˆ° PostgreSQL æ•°æ®è¿ç§»å·¥å…·')
    
    # æºæ•°æ®åº“é…ç½®ï¼ˆé»˜è®¤ä» .env æ–‡ä»¶è¯»å–ï¼‰
    parser.add_argument('--source-host', default=source_host_default, 
                       help=f'æºæ•°æ®åº“ä¸»æœºåœ°å€ï¼ˆé»˜è®¤: {source_host_default}ï¼Œä» .env è¯»å–ï¼‰')
    parser.add_argument('--source-port', type=int, default=source_port_default, 
                       help=f'æºæ•°æ®åº“ç«¯å£ï¼ˆé»˜è®¤: {source_port_default}ï¼Œä» .env è¯»å–ï¼‰')
    parser.add_argument('--source-db', default=source_db_default, 
                       help=f'æºæ•°æ®åº“åï¼ˆé»˜è®¤: {source_db_default}ï¼Œä» .env è¯»å–ï¼‰')
    parser.add_argument('--source-user', default=source_user_default, 
                       help=f'æºæ•°æ®åº“ç”¨æˆ·åï¼ˆé»˜è®¤: {source_user_default}ï¼Œä» .env è¯»å–ï¼‰')
    parser.add_argument('--source-password', default=source_password_default, 
                       help='æºæ•°æ®åº“å¯†ç ï¼ˆé»˜è®¤: ä» .env è¯»å–ï¼‰')
    
    # ç›®æ ‡æ•°æ®åº“é…ç½®
    parser.add_argument('--target-host', required=True, help='ç›®æ ‡æ•°æ®åº“ä¸»æœºåœ°å€ï¼ˆäº‘æœåŠ¡å™¨ï¼‰')
    parser.add_argument('--target-port', type=int, default=5432, help='ç›®æ ‡æ•°æ®åº“ç«¯å£')
    parser.add_argument('--target-db', default='crypto_data', help='ç›®æ ‡æ•°æ®åº“å')
    parser.add_argument('--target-user', default='postgres', help='ç›®æ ‡æ•°æ®åº“ç”¨æˆ·å')
    parser.add_argument('--target-password', required=True, help='ç›®æ ‡æ•°æ®åº“å¯†ç ')
    
    # è¿ç§»é€‰é¡¹
    parser.add_argument('--method', choices=['dump', 'python'], default='dump',
                       help='è¿ç§»æ–¹æ³•ï¼šdumpï¼ˆä½¿ç”¨pg_dumpï¼Œæ¨èï¼‰æˆ– pythonï¼ˆä½¿ç”¨Pythonè„šæœ¬ï¼‰')
    parser.add_argument('--table-filter', help='è¡¨åè¿‡æ»¤ï¼ˆå¦‚ K1d è¡¨ç¤ºåªè¿ç§»K1då¼€å¤´çš„è¡¨ï¼‰')
    parser.add_argument('--tables', nargs='+', help='æŒ‡å®šè¦è¿ç§»çš„è¡¨ååˆ—è¡¨')
    parser.add_argument('--table-file', help='ä»æ–‡ä»¶è¯»å–è¦è¿ç§»çš„è¡¨ååˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
    parser.add_argument('--skip-existing', action='store_true',
                       help='è·³è¿‡ç›®æ ‡æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ•°æ®ï¼ˆå¢é‡è¿ç§»ï¼‰')
    parser.add_argument('--compare-only', action='store_true',
                       help='åªå¯¹æ¯”ä¸¤ä¸ªæ•°æ®åº“çš„è¡¨æ•°é‡ï¼Œä¸æ‰§è¡Œè¿ç§»')
    parser.add_argument('--batch-size', type=int, default=10000,
                       help='æ‰¹é‡æ’å…¥å¤§å°ï¼ˆä»…ç”¨äºpythonæ–¹æ³•ï¼‰')
    
    args = parser.parse_args()
    
    # é…ç½®ä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ï¼ˆSOURCE_PG_*ï¼‰> .envæ–‡ä»¶ï¼ˆPG_*ï¼‰> é»˜è®¤å€¼
    # æºæ•°æ®åº“ï¼ˆæœ¬åœ°ï¼‰ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œæœ€å .env æ–‡ä»¶ä¸­çš„ PG_* é…ç½®
    source_host = os.getenv('SOURCE_PG_HOST') or args.source_host
    source_port = int(os.getenv('SOURCE_PG_PORT') or args.source_port)
    source_db = os.getenv('SOURCE_PG_DB') or args.source_db
    source_user = os.getenv('SOURCE_PG_USER') or args.source_user
    source_password = os.getenv('SOURCE_PG_PASSWORD') or args.source_password
    
    # ç›®æ ‡æ•°æ®åº“ï¼ˆäº‘æœåŠ¡å™¨ï¼‰ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡
    target_host = os.getenv('TARGET_PG_HOST') or args.target_host
    target_port = int(os.getenv('TARGET_PG_PORT') or args.target_port)
    target_db = os.getenv('TARGET_PG_DB') or args.target_db
    target_user = os.getenv('TARGET_PG_USER') or args.target_user
    target_password = os.getenv('TARGET_PG_PASSWORD') or args.target_password
    
    # æ˜¾ç¤ºé…ç½®æ¥æº
    # é‡æ–°è·å– env_pathï¼ˆåœ¨å‡½æ•°ä½œç”¨åŸŸå†…ï¼‰
    env_path_check = project_root / '.env'
    if not env_path_check.exists():
        env_path_check = backend_dir / '.env'
    
    logging.info("=" * 80)
    logging.info("æ•°æ®åº“é…ç½®")
    logging.info("=" * 80)
    logging.info(f"æºæ•°æ®åº“ï¼ˆæœ¬åœ°ï¼‰: {source_user}@{source_host}:{source_port}/{source_db}")
    if env_path_check.exists():
        logging.info(f"  âœ“ å·²ä» .env æ–‡ä»¶è¯»å–æœ¬åœ°æ•°æ®åº“é…ç½®: {env_path_check}")
    else:
        logging.info(f"  âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–å‘½ä»¤è¡Œå‚æ•°")
    logging.info(f"ç›®æ ‡æ•°æ®åº“ï¼ˆäº‘æœåŠ¡å™¨ï¼‰: {target_user}@{target_host}:{target_port}/{target_db}")
    logging.info("=" * 80)
    
    # åˆ›å»ºè¿ç§»å™¨
    migrator = PostgreSQLToPostgreSQLMigrator(
        source_host=source_host,
        source_port=source_port,
        source_db=source_db,
        source_user=source_user,
        source_password=source_password,
        target_host=target_host,
        target_port=target_port,
        target_db=target_db,
        target_user=target_user,
        target_password=target_password,
        batch_size=args.batch_size
    )
    
    # å¦‚æœåªæ˜¯å¯¹æ¯”
    if args.compare_only:
        logging.info("=" * 80)
        logging.info("å¯¹æ¯”æ•°æ®åº“è¡¨æ•°é‡")
        logging.info("=" * 80)
        comparison = migrator.compare_table_counts(args.table_filter)
        
        print(f"\næºæ•°æ®åº“è¡¨æ•°é‡: {comparison['source_count']}")
        print(f"ç›®æ ‡æ•°æ®åº“è¡¨æ•°é‡: {comparison['target_count']}")
        print(f"å…±åŒè¡¨æ•°é‡: {comparison['common_count']}")
        
        if comparison['only_in_source']:
            print(f"\nä»…åœ¨æºæ•°æ®åº“ä¸­çš„è¡¨ ({len(comparison['only_in_source'])}):")
            for table in comparison['only_in_source'][:10]:
                print(f"  - {table}")
            if len(comparison['only_in_source']) > 10:
                print(f"  ... è¿˜æœ‰ {len(comparison['only_in_source']) - 10} ä¸ªè¡¨")
        
        if comparison['only_in_target']:
            print(f"\nä»…åœ¨ç›®æ ‡æ•°æ®åº“ä¸­çš„è¡¨ ({len(comparison['only_in_target'])}):")
            for table in comparison['only_in_target'][:10]:
                print(f"  - {table}")
            if len(comparison['only_in_target']) > 10:
                print(f"  ... è¿˜æœ‰ {len(comparison['only_in_target']) - 10} ä¸ªè¡¨")
        
        return
    
    # æ‰§è¡Œè¿ç§»
    logging.info("=" * 80)
    logging.info("å¼€å§‹è¿ç§»æ•°æ®")
    logging.info("=" * 80)
    
    # ç¡®å®šè¦è¿ç§»çš„è¡¨
    tables_to_migrate = None
    if args.tables:
        tables_to_migrate = args.tables
        logging.info(f"æŒ‡å®šè¿ç§»è¡¨: {tables_to_migrate}")
    elif args.table_file:
        with open(args.table_file, 'r') as f:
            tables_to_migrate = [line.strip() for line in f if line.strip()]
        logging.info(f"ä»æ–‡ä»¶è¯»å– {len(tables_to_migrate)} ä¸ªè¡¨")
    elif args.table_filter:
        source_tables = migrator.get_source_tables()
        if args.table_filter.startswith('K'):
            tables_to_migrate = [t for t in source_tables if t.startswith(args.table_filter)]
        else:
            tables_to_migrate = [t for t in source_tables if args.table_filter in t]
        logging.info(f"è¿‡æ»¤åéœ€è¦è¿ç§»çš„è¡¨: {len(tables_to_migrate)} ä¸ª")
    
    # é€‰æ‹©è¿ç§»æ–¹æ³•
    if args.method == 'dump':
        # ä½¿ç”¨ pg_dump/pg_restoreï¼ˆæ¨èï¼‰
        logging.info("ä½¿ç”¨ pg_dump/pg_restore æ–¹æ³•è¿ç§»ï¼ˆæ¨èï¼‰")
        success = migrator.migrate_with_pg_dump(tables_to_migrate)
        if success:
            logging.info("=" * 80)
            logging.info("âœ… è¿ç§»å®Œæˆï¼")
            logging.info("=" * 80)
        else:
            logging.error("=" * 80)
            logging.error("âŒ è¿ç§»å¤±è´¥ï¼")
            logging.error("=" * 80)
            sys.exit(1)
    else:
        # ä½¿ç”¨ Python è„šæœ¬æ–¹æ³•
        logging.info("ä½¿ç”¨ Python è„šæœ¬æ–¹æ³•è¿ç§»")
        stats = migrator.migrate_all(
            table_filter=args.table_filter,
            tables=tables_to_migrate,
            table_file=args.table_file,
            skip_existing=args.skip_existing
        )
        
        logging.info("=" * 80)
        logging.info("è¿ç§»å®Œæˆï¼")
        logging.info("=" * 80)
        logging.info(f"æ€»è¡¨æ•°: {stats['total']}")
        logging.info(f"âœ“ æˆåŠŸ: {stats['success']}")
        logging.info(f"âœ— å¤±è´¥: {stats['failed']}")
        logging.info(f"â­ï¸  è·³è¿‡: {stats['skipped']}")
        logging.info("=" * 80)
        
        if stats['failed'] > 0:
            sys.exit(1)


if __name__ == "__main__":
    main()
       